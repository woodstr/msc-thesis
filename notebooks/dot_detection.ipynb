{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55ed89a",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Thank you Yucheng for the dataloader, model and image preparing code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b3528",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31efd61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b5568",
   "metadata": {},
   "source": [
    "## Image Preparing Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09d9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_rotate(image, angle_deg):\n",
    "    \"\"\"Rotate the image counterclockwise by `angle_deg` degrees.\"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w / 2.0, h / 2.0)\n",
    "    scale = 1.0\n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    a = np.sin(angle_rad) * scale\n",
    "    b = np.cos(angle_rad) * scale\n",
    "    new_w = int(h * abs(a) + w * abs(b))\n",
    "    new_h = int(w * abs(a) + h * abs(b))\n",
    "\n",
    "    M = cv2.getRotationMatrix2D(center, -angle_deg, scale)\n",
    "    M[0, 2] += (new_w - w) / 2\n",
    "    M[1, 2] += (new_h - h) / 2\n",
    "\n",
    "    rotated = cv2.warpAffine(image, M, (new_w, new_h), flags=cv2.INTER_LINEAR, borderValue=(255, 255, 255))\n",
    "    return rotated, M\n",
    "\n",
    "\n",
    "def split_and_save_patches(image, dots, output_dir, base_index, patch_size=256, t_size=False):\n",
    "    \"\"\"Split image and dot coordinates into NxN patches and save each as img_xxxxx.*\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    h, w = image.shape[:2]\n",
    "    patch_idx = 0\n",
    "\n",
    "    for y0 in range(0, h, patch_size):\n",
    "        for x0 in range(0, w, patch_size):\n",
    "            x1 = min(x0 + patch_size, w)\n",
    "            y1 = min(y0 + patch_size, h)\n",
    "            patch = image[y0:y1, x0:x1]\n",
    "\n",
    "            if patch.shape[0] != patch_size or patch.shape[1] != patch_size:\n",
    "                continue  # Skip incomplete patch\n",
    "\n",
    "            local_dots = []\n",
    "            for x, y in dots:\n",
    "                if x0 <= x < x1 and y0 <= y < y1:\n",
    "                    local_dots.append((x - x0, y - y0))\n",
    "\n",
    "            name = f\"img_{base_index + patch_idx:05d}\"\n",
    "            img_path = os.path.join(output_dir, name + \".png\")\n",
    "            csv_path = os.path.join(output_dir, name + \".csv\")\n",
    "\n",
    "            patch_gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imwrite(img_path, patch_gray)\n",
    "\n",
    "            with open(csv_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([t_size])\n",
    "                writer.writerow([\"x\", \"y\"])\n",
    "                writer.writerows(local_dots)\n",
    "\n",
    "            patch_idx += 1\n",
    "\n",
    "    return patch_idx\n",
    "\n",
    "\n",
    "def parse_and_save_DSBI(image_path, output_dir, base_index, patch_size=256):\n",
    "    \"\"\"Parse annotation and split into patches with adjusted coordinates.\"\"\"\n",
    "    base_path = os.path.splitext(image_path)[0]\n",
    "    recto_path = base_path + \"+recto.txt\"\n",
    "    verso_path = base_path + \"+verso.txt\"\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    all_coords = []\n",
    "    rotated_image = image\n",
    "    last_transform = None\n",
    "\n",
    "    for txt_path in [recto_path, verso_path]:\n",
    "        if not os.path.exists(txt_path):\n",
    "            continue\n",
    "\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if len(lines) < 3:\n",
    "            continue\n",
    "\n",
    "        angle = float(lines[0].strip())\n",
    "        verticals = list(map(int, lines[1].strip().split()))\n",
    "        horizontals = list(map(int, lines[2].strip().split()))\n",
    "        cell_lines = lines[3:]\n",
    "\n",
    "        rotated_image, transform = image_rotate(image, -angle)\n",
    "        last_transform = transform\n",
    "\n",
    "        for cell in cell_lines:\n",
    "            parts = list(map(int, cell.strip().split()))\n",
    "            r, c = parts[0] - 1, parts[1] - 1\n",
    "            dots = parts[2:]\n",
    "            for i, val in enumerate(dots):\n",
    "                if val == 1:\n",
    "                    if i < 3:\n",
    "                        y = horizontals[r * 3 + i]\n",
    "                        x = verticals[c * 2]\n",
    "                    else:\n",
    "                        y = horizontals[r * 3 + i - 3]\n",
    "                        x = verticals[c * 2 + 1]\n",
    "                    coord = np.array([[x, y]], dtype=np.float32)\n",
    "                    coord = np.array([coord])\n",
    "                    rotated_coord = cv2.transform(coord, transform)[0][0]\n",
    "                    all_coords.append(rotated_coord.tolist())\n",
    "\n",
    "        image = rotated_image\n",
    "\n",
    "    if last_transform is None:\n",
    "        return 0\n",
    "\n",
    "    return split_and_save_patches(image, all_coords, output_dir, base_index, patch_size)\n",
    "\n",
    "\n",
    "def collect_images(root_dir):\n",
    "    \"\"\"Recursively collect image files excluding those containing '+recto' or '+verso'.\"\"\"\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
    "    file_list = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.lower().endswith(valid_extensions):\n",
    "                if \"+recto\" not in fname and \"+verso\" not in fname:\n",
    "                    full_path = os.path.join(dirpath, fname)\n",
    "                    file_list.append(full_path)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b7bc8",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec676f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv → BN → ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling → concat → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # pad if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_c=64):\n",
    "        super().__init__()\n",
    "        self.in_conv = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c, base_c * 2)\n",
    "        self.down2 = Down(base_c * 2, base_c * 4)\n",
    "        self.down3 = Down(base_c * 4, base_c * 8)\n",
    "        self.down4 = Down(base_c * 8, base_c * 8)\n",
    "\n",
    "        self.up1 = Up(base_c * 16, base_c * 4)\n",
    "        self.up2 = Up(base_c * 8, base_c * 2)\n",
    "        self.up3 = Up(base_c * 4, base_c)\n",
    "        self.up4 = Up(base_c * 2, base_c)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_c, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return torch.sigmoid(self.out_conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aec4bb",
   "metadata": {},
   "source": [
    "# Preparing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768b575",
   "metadata": {},
   "source": [
    "## Final Saving Format\n",
    "Files are saved under data/dot_detection/prepared-patches.\n",
    "\n",
    "Images/labels matched by filename.\n",
    "\n",
    "Filename format is img_0000.png / img_0000.csv.\n",
    "\n",
    "Image size is 512/512.\n",
    "\n",
    ".csv format is:\n",
    "- 25 <-- template size, either a number or False\n",
    "- x,y <-- always there\n",
    "- 0.3494873046875,22.6949462890625 <-- x,y coord (float due to reduced pixels)\n",
    "- 22.349365234375,22.6181640625 <-- x,y coord (float due to reduced pixels)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ad80c",
   "metadata": {},
   "source": [
    "## DSBI\n",
    "### Format\n",
    "Filepathing:\n",
    "- data\n",
    "  - Fundamentals of Massage\n",
    "  - Massage\n",
    "  - Math\n",
    "  - Ordinary Printed Document\n",
    "  - Shaver Yang Fengting\n",
    "  - The Second Volume of Ninth Grade Chinese Book 1\n",
    "  - The Second Volume of Ninth Grade Chinese Book 2\n",
    "\n",
    "Different types of data are stored in different directories and processed later to be in the same. <br>\n",
    "Images and labels are stored in the same directories and matched by filename.\n",
    "\n",
    "example .txt data with explanation:\n",
    "- 0.80 <-- angle (which gets corrected for during preprocessing)\n",
    "- 47 67 94 114 142 162 189 209 236 256... <-- verticals\n",
    "- 47 67 87 125 145 165 203 223 243 281... <-- horizontals\n",
    "- 4 6 1 0 1 0 0 0 <-- what\n",
    "- 4 7 1 1 1 0 0 1 <-- huh\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b30b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115 images to process.\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+10.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+11.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+12.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+13.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+14.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+15.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+16.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+17.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+18.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+19.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+20.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Fundamentals of Massage\\FM+9.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+10.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+11.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+12.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+13.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+14.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+15.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+16.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+17.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+18.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+19.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+20.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Massage\\M+9.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+10.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+11.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+12.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+13.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+14.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+15.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+16.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+17.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+18.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+19.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+20.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+21.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+22.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+23.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+24.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+25.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+26.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+27.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+28.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+29.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+30.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+31.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+32.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Math\\math+9.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Ordinary Printed Document\\OPD+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\Shaver Yang Fengting\\SYF+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+10.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+11.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+12.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+13.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+14.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+15.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+16.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+17.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+18.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+19.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+20.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 1\\SVNGCB1+9.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+1.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+10.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+2.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+3.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+4.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+5.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+6.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+7.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+8.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\data\\The Second Volume of Ninth Grade Chinese Book 2\\SVNGCB2+9.jpg...\n",
      "Processing ../data/dot_detection/DSBI-master\\figures\\Braille dots arrangement within one Braille cell.jpg...\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"../data/dot_detection/DSBI-master\"\n",
    "output_directory = \"../data/dot_detection/prepared-patches\"\n",
    "patch_size = 512\n",
    "\n",
    "image_list = collect_images(root_directory)\n",
    "print(f\"Found {len(image_list)} images to process.\")\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "patch_counter = 0\n",
    "for path in sorted(image_list):\n",
    "    print(f\"Processing {path}...\")\n",
    "    n = parse_and_save_DSBI(path, output_directory, base_index=patch_counter, patch_size=patch_size)\n",
    "    patch_counter += n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2b76a",
   "metadata": {},
   "source": [
    "## MAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a318c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images to process.\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\1D1165212740006_jpeg.rf.d16389429a55a2c9ab5f6d02e8173fde.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\1D1165212740007_jpeg.rf.26033cb881d8bd1c105f6cc48b455b82.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\20231031_053432000_iOS_png.rf.4fd0ae5ea53655b08749a956753656aa.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\20231113_024141652_iOS_jpg.rf.6c4fd508cbe29751ac4cb2042de3aaf5.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\250115190140006_jpeg.rf.d3f59dc9b7bfc3e4fb7f7a25e8fff612.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\250115190140012_jpeg.rf.86d7815fc6dbb08a22f3043e5d47a3ac.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\G95-plunger-2_jpg.rf.cb1bf3703c0894c15072c90ec1c4b5b9.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\IMG_1058_JPG.rf.5f34941fe887524a8cca104339e19771.jpg...\n",
      "Processing ../data/dot_detection/MAN/roboflow\\images\\PortaDot-Trust-billede_jpg.rf.0ba49b85ff9e25bec9a58bcfe525e8e2.jpg...\n"
     ]
    }
   ],
   "source": [
    "def parse_and_save_MAN(image_path, output_dir):\n",
    "    \"\"\"Parse annotation and split into patches with adjusted coordinates.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    all_coords = []\n",
    "\n",
    "    # finding label txt\n",
    "    label_path = image_path.replace('images', 'labels')\n",
    "    label_path = label_path.replace('.jpg', '.txt')\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    data = lines[0].split(' ')\n",
    "    \n",
    "    # first 5 elements are bounding box related (skip)\n",
    "    data = data[5:]\n",
    "\n",
    "    # rest of elements are x, y, class. we look through them and only keep x, y, of classes 2\n",
    "    for i in range(0, len(data), 3):\n",
    "        x = float(data[i])\n",
    "        y = float(data[i + 1])\n",
    "        cls = int(data[i + 2])\n",
    "        if cls == 2:  # only keep class 2\n",
    "            # convert relative (0-1) coords to relative (0-max width/height)\n",
    "            x = x * image.shape[1]\n",
    "            y = y * image.shape[0]\n",
    "\n",
    "            all_coords.append([x, y])\n",
    "    \n",
    "    # based on xy coordinates, we find the center\n",
    "    dmc_center = np.mean(all_coords, axis=0)\n",
    "    dmc_center = np.array([dmc_center[0], dmc_center[1]])\n",
    "\n",
    "    # find coords of dot furtherst from center\n",
    "    max_dist = 0\n",
    "    for coord in all_coords:\n",
    "        dist = np.linalg.norm(coord - dmc_center)\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            # farthest_coord = coord # unused\n",
    "\n",
    "    # based on max dist, we use that val + some% padding to crop the image\n",
    "    padding = max_dist * 1.1\n",
    "    x1 = int(dmc_center[0] - padding)\n",
    "    y1 = int(dmc_center[1] - padding)\n",
    "    x2 = int(dmc_center[0] + padding)\n",
    "    y2 = int(dmc_center[1] + padding)\n",
    "    x1 = max(0, x1)\n",
    "    y1 = max(0, y1)\n",
    "    x2 = min(image.shape[1], x2)\n",
    "    y2 = min(image.shape[0], y2)\n",
    "    img_cropped = image[y1:y2, x1:x2]\n",
    "    all_coords = np.array(all_coords) - np.array([x1, y1])\n",
    "\n",
    "    # debug display crop with dots overlaid\n",
    "    # debug_img = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2RGB)\n",
    "    # debug_img = Image.fromarray(debug_img)\n",
    "    # draw = ImageDraw.Draw(debug_img)\n",
    "    # for coord in all_coords:\n",
    "    #     x, y = int(coord[0]), int(coord[1])\n",
    "    #     draw.ellipse((x - 5, y - 5, x + 5, y + 5), fill=(255, 0, 0), outline=(255, 0, 0))\n",
    "    # debug_img = np.array(debug_img)\n",
    "    # debug_img = Image.fromarray(debug_img)\n",
    "    # display(debug_img)\n",
    "\n",
    "    # return split_and_save_patches(img_cropped, all_coords, output_dir, base_index, patch_size)\n",
    "\n",
    "    # save cropped images and coordinates\n",
    "    name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    img_dir = output_dir + '/images/'\n",
    "    img_path = os.path.join(img_dir, name + \".png\")\n",
    "    csv_dir = output_dir + '/labels/'\n",
    "    csv_path = os.path.join(csv_dir, name + \".csv\")\n",
    "\n",
    "    cv2.imwrite(img_path, img_cropped)\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"x\", \"y\"])\n",
    "        for coord in all_coords:\n",
    "            writer.writerow(coord)\n",
    "\n",
    "\n",
    "root_directory = \"../data/dot_detection/MAN/roboflow\"\n",
    "\n",
    "image_list = collect_images(root_directory)\n",
    "print(f\"Found {len(image_list)} images to process.\")\n",
    "\n",
    "output_directory = \"../data/dot_detection/MAN/cropped\"\n",
    "\n",
    "for path in sorted(image_list):\n",
    "    print(f\"Processing {path}...\")\n",
    "    parse_and_save_MAN(path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe514576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images to process.\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\1D1165212740006_jpeg.rf.d16389429a55a2c9ab5f6d02e8173fde.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\1D1165212740007_jpeg.rf.26033cb881d8bd1c105f6cc48b455b82.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\20231031_053432000_iOS_png.rf.4fd0ae5ea53655b08749a956753656aa.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\20231113_024141652_iOS_jpg.rf.6c4fd508cbe29751ac4cb2042de3aaf5.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\250115190140006_jpeg.rf.d3f59dc9b7bfc3e4fb7f7a25e8fff612.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\250115190140012_jpeg.rf.86d7815fc6dbb08a22f3043e5d47a3ac.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\G95-plunger-2_jpg.rf.cb1bf3703c0894c15072c90ec1c4b5b9.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\IMG_1058_JPG.rf.5f34941fe887524a8cca104339e19771.png...\n",
      "Processing ../data/dot_detection/MAN/cropped/images\\PortaDot-Trust-billede_jpg.rf.0ba49b85ff9e25bec9a58bcfe525e8e2.png...\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"../data/dot_detection/MAN/cropped\"\n",
    "\n",
    "image_list = collect_images(root_dir + '/images')\n",
    "print(f\"Found {len(image_list)} images to process.\")\n",
    "\n",
    "output_dir = \"../data/dot_detection/MAN/prepared-patches\"\n",
    "\n",
    "patch_counter = 0\n",
    "for path in sorted(image_list):\n",
    "    print(f\"Processing {path}...\")\n",
    "\n",
    "    # read template image to get patch size\n",
    "    filename = os.path.basename(path)\n",
    "    template_path = root_dir + '/templates/' + filename\n",
    "    template = cv2.imread(template_path)\n",
    "    if template is None:\n",
    "        print(f\"Error: Template image not found for {filename}. Skipping.\")\n",
    "        continue\n",
    "    h, w = template.shape[:2]\n",
    "    if h != w:\n",
    "        print(f\"Error: Template image is not square for {filename}. Skipping.\")\n",
    "        continue\n",
    "    t_size = h\n",
    "\n",
    "    img = cv2.imread(path) # read image\n",
    "    coords = pd.read_csv(path.replace('images', 'labels').replace('.png', '.csv')).to_numpy().astype(np.float32)\n",
    "    # TODO: try with lower patch_size since we only get 1 patch per image with 512x512\n",
    "    n = split_and_save_patches(img, coords, output_dir, patch_counter, patch_size, t_size)\n",
    "    patch_counter += n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f985393",
   "metadata": {},
   "source": [
    "## Dataloader Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fd82773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrailleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, patch_size=224, transform_prob=0.1, train_val_ratio=0.8, is_train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.sigma = 3\n",
    "        self.transform_prob = transform_prob\n",
    "        self.is_train = is_train\n",
    "\n",
    "        all_paths = sorted(glob.glob(os.path.join(root_dir, \"*.png\")))\n",
    "        split_idx = int(len(all_paths) * train_val_ratio)\n",
    "        self.image_paths = all_paths[:split_idx] if is_train else all_paths[split_idx:]\n",
    "\n",
    "        # light augmentation for grayscale tensor images using torchvision v2\n",
    "        self.color_aug = T.Compose([\n",
    "            T.RandomApply([\n",
    "                T.ColorJitter(brightness=0.5, hue=0.3),\n",
    "                T.RandomInvert(),\n",
    "                T.RandomPosterize(bits=2),\n",
    "                T.RandomSolarize(threshold=192.0 / 255.0),\n",
    "                T.GaussianBlur(kernel_size=(3, 3))\n",
    "            ], p=self.transform_prob)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "\n",
    "        csv_path = os.path.splitext(img_path)[0] + \".csv\"\n",
    "        coords = pd.read_csv(csv_path, skiprows=1).to_numpy().astype(np.float32)\n",
    "        img_tensor = to_tensor(img)  # shape: [1, H, W]\n",
    "\n",
    "        # read first line of csv to get t_size\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            first_line = next(reader)\n",
    "            t_size = first_line[0]\n",
    "            if t_size == 'False':\n",
    "                t_size = False\n",
    "            else:\n",
    "                t_size = int(t_size)\n",
    "\n",
    "        if self.is_train:\n",
    "            if random.random() < 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[2])  # Horizontal flip\n",
    "                coords[:, 0] = w - coords[:, 0]\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[1])  # Vertical flip\n",
    "                coords[:, 1] = h - coords[:, 1]\n",
    "\n",
    "            img_tensor, coords = self.apply_random_zoom(img_tensor, coords, zoom_range=(0.8, 1.2))\n",
    "            img_tensor, coords = self.apply_random_crop(img_tensor, coords, crop_size=self.patch_size)\n",
    "\n",
    "            angle = random.uniform(-45, 45)\n",
    "            translate = (random.uniform(-0.1, 0.1) * w, random.uniform(-0.1, 0.1) * h)\n",
    "            scale = random.uniform(0.9, 1.1)\n",
    "            shear = random.uniform(-10, 10)\n",
    "            img_tensor, coords = self.apply_affine(img_tensor, coords, angle, translate, scale, shear)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                img_tensor, coords = self.apply_perspective(img_tensor, coords, distortion_scale=0.2)\n",
    "\n",
    "            img_tensor = self.color_aug(img_tensor)\n",
    "\n",
    "        heatmap = self.coords_to_heatmap(coords, img_tensor.shape[1:], sigma=self.sigma, t_size=t_size)\n",
    "        return img_tensor, heatmap\n",
    "\n",
    "    def apply_random_crop(self, img_tensor, coords, crop_size=256):\n",
    "        _, h, w = img_tensor.shape\n",
    "        if h <= crop_size or w <= crop_size:\n",
    "            return img_tensor, coords\n",
    "\n",
    "        x0 = random.randint(0, w - crop_size)\n",
    "        y0 = random.randint(0, h - crop_size)\n",
    "        x1, y1 = x0 + crop_size, y0 + crop_size\n",
    "\n",
    "        img_cropped = img_tensor[:, y0:y1, x0:x1]\n",
    "        coords_cropped = coords - np.array([x0, y0])\n",
    "\n",
    "        mask = (\n",
    "                (coords_cropped[:, 0] >= 0) & (coords_cropped[:, 0] < crop_size) &\n",
    "                (coords_cropped[:, 1] >= 0) & (coords_cropped[:, 1] < crop_size)\n",
    "        )\n",
    "        coords_cropped = coords_cropped[mask]\n",
    "\n",
    "        return img_cropped, coords_cropped\n",
    "\n",
    "    def apply_affine(self, img_tensor, coords, angle=0, translate=(0, 0), scale=1.0, shear=0.0):\n",
    "        img_np = (img_tensor.squeeze(0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        h, w = img_np.shape\n",
    "        center = (w / 2.0, h / 2.0)\n",
    "\n",
    "        M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "        shear_rad = np.deg2rad(shear)\n",
    "        shear_mat = np.array([[1, -np.tan(shear_rad)], [0, 1]])\n",
    "        M[:2, :2] = shear_mat @ M[:2, :2]\n",
    "        M[:, 2] += translate\n",
    "\n",
    "        img_warped = cv2.warpAffine(img_np, M, (w, h), borderValue=255)\n",
    "\n",
    "        coords_hom = np.hstack([coords, np.ones((coords.shape[0], 1))])\n",
    "        coords_warped = (M @ coords_hom.T).T\n",
    "\n",
    "        img_tensor = torch.from_numpy(img_warped).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        coords_int = coords_warped.astype(int)\n",
    "        valid_mask = []\n",
    "        for x, y in coords_int:\n",
    "            if 0 <= x < w and 0 <= y < h and img_tensor[0, y, x] < 0.99:\n",
    "                valid_mask.append(True)\n",
    "            else:\n",
    "                valid_mask.append(False)\n",
    "\n",
    "        coords_warped = coords_warped[valid_mask]\n",
    "        return img_tensor, coords_warped\n",
    "\n",
    "    def apply_perspective(self, img_tensor, coords, distortion_scale=0.2):\n",
    "        img_np = (img_tensor.squeeze(0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        h, w = img_np.shape\n",
    "\n",
    "        def random_shift(pt):\n",
    "            dx = np.random.uniform(-distortion_scale, distortion_scale) * w\n",
    "            dy = np.random.uniform(-distortion_scale, distortion_scale) * h\n",
    "            return [pt[0] + dx, pt[1] + dy]\n",
    "\n",
    "        src_pts = np.array([[0, 0], [w, 0], [w, h], [0, h]], dtype=np.float32)\n",
    "        dst_pts = np.array([random_shift(pt) for pt in src_pts], dtype=np.float32)\n",
    "\n",
    "        M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "        warped_np = cv2.warpPerspective(img_np, M, (w, h), borderValue=255)\n",
    "\n",
    "        coords_hom = np.hstack([coords, np.ones((coords.shape[0], 1))])\n",
    "        coords_proj = (M @ coords_hom.T).T\n",
    "        coords_proj /= coords_proj[:, [2]]\n",
    "        coords_warped = coords_proj[:, :2]\n",
    "\n",
    "        img_tensor = torch.from_numpy(warped_np).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        coords_int = coords_warped.astype(int)\n",
    "        valid_mask = []\n",
    "        for x, y in coords_int:\n",
    "            if 0 <= x < w and 0 <= y < h and img_tensor[0, y, x] < 0.99:\n",
    "                valid_mask.append(True)\n",
    "            else:\n",
    "                valid_mask.append(False)\n",
    "\n",
    "        coords_warped = coords_warped[valid_mask]\n",
    "        return img_tensor, coords_warped\n",
    "\n",
    "    def coords_to_heatmap(self, coords, img_size, sigma=2, t_size=False):\n",
    "        H, W = img_size\n",
    "        heatmap = torch.zeros((1, H, W), dtype=torch.float32)\n",
    "\n",
    "        if t_size:\n",
    "            sigma = sigma * (t_size / 15)\n",
    "        # else:\n",
    "        tmp_size = int(3 * sigma)\n",
    "\n",
    "        for x, y in coords:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            if x < 0 or y < 0 or x >= W or y >= H:\n",
    "                continue\n",
    "\n",
    "            x0 = max(0, x - tmp_size)\n",
    "            x1 = min(W, x + tmp_size + 1)\n",
    "            y0 = max(0, y - tmp_size)\n",
    "            y1 = min(H, y + tmp_size + 1)\n",
    "\n",
    "            yy, xx = torch.meshgrid(\n",
    "                torch.arange(y0, y1, dtype=torch.float32),\n",
    "                torch.arange(x0, x1, dtype=torch.float32),\n",
    "                indexing='ij'\n",
    "            )\n",
    "            g = torch.exp(-((xx - x) ** 2 + (yy - y) ** 2) / (2 * sigma ** 2))\n",
    "            g = g / g.max()\n",
    "\n",
    "            heatmap[0, y0:y1, x0:x1] = torch.maximum(heatmap[0, y0:y1, x0:x1], g)\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "    def apply_random_zoom(self, img_tensor, coords, zoom_range=(0.8, 1.2)):\n",
    "        c, h, w = img_tensor.shape\n",
    "        scale = random.uniform(*zoom_range)\n",
    "        new_h = int(h * scale)\n",
    "        new_w = int(w * scale)\n",
    "\n",
    "        img_np = (img_tensor.squeeze(0).numpy() * 255).astype(np.uint8)\n",
    "        img_resized = cv2.resize(img_np, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if scale > 1.0:\n",
    "            x0 = (new_w - w) // 2\n",
    "            y0 = (new_h - h) // 2\n",
    "            img_zoomed = img_resized[y0:y0 + h, x0:x0 + w]\n",
    "            offset = np.array([-x0, -y0])\n",
    "        else:\n",
    "            pad_left = (w - new_w) // 2\n",
    "            pad_top = (h - new_h) // 2\n",
    "            img_zoomed = np.full((h, w), 255, dtype=np.uint8)\n",
    "            img_zoomed[pad_top:pad_top + new_h, pad_left:pad_left + new_w] = img_resized\n",
    "            offset = np.array([pad_left, pad_top])\n",
    "\n",
    "        coords = coords * scale + offset\n",
    "\n",
    "        img_tensor = torch.from_numpy(img_zoomed).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        coords_int = coords.astype(int)\n",
    "        valid_mask = []\n",
    "        for x, y in coords_int:\n",
    "            if 0 <= x < w and 0 <= y < h and img_tensor[0, y, x] < 0.99:\n",
    "                valid_mask.append(True)\n",
    "            else:\n",
    "                valid_mask.append(False)\n",
    "\n",
    "        coords = coords[valid_mask]\n",
    "        return img_tensor, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1106d9",
   "metadata": {},
   "source": [
    "# Dataloader Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab38ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 debug images to ../data/dot_detection/debug_output\n"
     ]
    }
   ],
   "source": [
    "# Output directory for debug images\n",
    "output_dir = \"../data/dot_detection/debug_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BrailleDataset(root_dir=\"../data/dot_detection/prepared-patches\", transform_prob=0.2)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Iterate and save debug patches\n",
    "for idx, (img_tensor, heatmap_tensor) in enumerate(loader):\n",
    "    img = to_pil_image(img_tensor[0])  # Convert image tensor to PIL\n",
    "\n",
    "    # Convert heatmap to grayscale image\n",
    "    heatmap = heatmap_tensor[0, 0]  # Shape: H x W\n",
    "    heatmap_np = heatmap.clamp(0, 1).numpy()\n",
    "    heatmap_img = Image.fromarray((cm.inferno(heatmap_np)[:, :, :3] * 255).astype('uint8'))  # Apply colormap\n",
    "\n",
    "    # Resize heatmap to match image size (safety)\n",
    "    heatmap_img = heatmap_img.resize(img.size, resample=Image.BILINEAR)\n",
    "\n",
    "    # Option A: Combine side by side\n",
    "    combined = Image.new(\"RGB\", (img.width * 2, img.height))\n",
    "    combined.paste(img, (0, 0))\n",
    "    combined.paste(heatmap_img, (img.width, 0))\n",
    "\n",
    "    # Option B: Or overlay heatmap on image (if you prefer)\n",
    "    # overlay = Image.blend(img.convert(\"RGB\"), heatmap_img, alpha=0.5)\n",
    "\n",
    "    combined.save(os.path.join(output_dir, f\"debug_img_{idx:05d}.png\"))\n",
    "\n",
    "    if idx >= 49:\n",
    "        break  # Save first 50 only\n",
    "\n",
    "print(f\"Saved {idx + 1} debug images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cee9b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 8 debug images to ../data/dot_detection/MAN/debug_output\n"
     ]
    }
   ],
   "source": [
    "# Output directory for debug images\n",
    "output_dir = \"../data/dot_detection/MAN/debug_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BrailleDataset(root_dir=\"../data/dot_detection/MAN/prepared-patches\", transform_prob=0.2)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Iterate and save debug patches\n",
    "for idx, (img_tensor, heatmap_tensor) in enumerate(loader):\n",
    "    img = to_pil_image(img_tensor[0])  # Convert image tensor to PIL\n",
    "\n",
    "    # Convert heatmap to grayscale image\n",
    "    heatmap = heatmap_tensor[0, 0]  # Shape: H x W\n",
    "    heatmap_np = heatmap.clamp(0, 1).numpy()\n",
    "    heatmap_img = Image.fromarray((cm.inferno(heatmap_np)[:, :, :3] * 255).astype('uint8'))  # Apply colormap\n",
    "\n",
    "    # Resize heatmap to match image size (safety)\n",
    "    heatmap_img = heatmap_img.resize(img.size, resample=Image.BILINEAR)\n",
    "\n",
    "    # Option A: Combine side by side\n",
    "    combined = Image.new(\"RGB\", (img.width * 2, img.height))\n",
    "    combined.paste(img, (0, 0))\n",
    "    combined.paste(heatmap_img, (img.width, 0))\n",
    "\n",
    "    # Option B: Or overlay heatmap on image (if you prefer)\n",
    "    # overlay = Image.blend(img.convert(\"RGB\"), heatmap_img, alpha=0.5)\n",
    "\n",
    "    combined.save(os.path.join(output_dir, f\"debug_img_{idx:05d}.png\"))\n",
    "\n",
    "    if idx >= 49:\n",
    "        break  # Save first 50 only\n",
    "\n",
    "print(f\"Saved {idx + 1} debug images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6123d",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29bf6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 137/137 [00:41<00:00,  3.32it/s]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.1251, Val Loss = 0.0564\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.48it/s]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0378, Val Loss = 0.0299\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.50it/s]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0269, Val Loss = 0.0237\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.50it/s]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0233, Val Loss = 0.0229\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.52it/s]\n",
      "Epoch 5/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0218, Val Loss = 0.0224\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.49it/s]\n",
      "Epoch 6/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0211, Val Loss = 0.0223\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.50it/s]\n",
      "Epoch 7/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0209, Val Loss = 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.49it/s]\n",
      "Epoch 8/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0209, Val Loss = 0.0186\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.49it/s]\n",
      "Epoch 9/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0206, Val Loss = 0.0173\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.50it/s]\n",
      "Epoch 10/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0206, Val Loss = 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.52it/s]\n",
      "Epoch 11/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.0200, Val Loss = 0.0171\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.52it/s]\n",
      "Epoch 12/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.0189, Val Loss = 0.0156\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.50it/s]\n",
      "Epoch 13/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.0186, Val Loss = 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.48it/s]\n",
      "Epoch 14/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.0190, Val Loss = 0.0146\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.47it/s]\n",
      "Epoch 15/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.0186, Val Loss = 0.0145\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.48it/s]\n",
      "Epoch 16/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.0177, Val Loss = 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.51it/s]\n",
      "Epoch 17/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 0.0184, Val Loss = 0.0134\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.47it/s]\n",
      "Epoch 18/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 0.0178, Val Loss = 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.51it/s]\n",
      "Epoch 19/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 0.0181, Val Loss = 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.53it/s]\n",
      "Epoch 20/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 0.0177, Val Loss = 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.52it/s]\n",
      "Epoch 21/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss = 0.0176, Val Loss = 0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.51it/s]\n",
      "Epoch 22/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss = 0.0176, Val Loss = 0.0132\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.53it/s]\n",
      "Epoch 23/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss = 0.0178, Val Loss = 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.55it/s]\n",
      "Epoch 24/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss = 0.0174, Val Loss = 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.54it/s]\n",
      "Epoch 25/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss = 0.0171, Val Loss = 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.51it/s]\n",
      "Epoch 26/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss = 0.0174, Val Loss = 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.54it/s]\n",
      "Epoch 27/30 [Val]: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss = 0.0170, Val Loss = 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.59it/s]\n",
      "Epoch 28/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss = 0.0171, Val Loss = 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 137/137 [00:39<00:00,  3.51it/s]\n",
      "Epoch 29/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss = 0.0172, Val Loss = 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 137/137 [00:38<00:00,  3.58it/s]\n",
      "Epoch 30/30 [Val]: 100%|██████████| 35/35 [00:16<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss = 0.0171, Val Loss = 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "lr = 1e-3\n",
    "train_val_ratio = 0.8\n",
    "save_dir = \"../models/dot_detection/checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Load dataset ===\n",
    "train_dataset = BrailleDataset(root_dir=\"../data/dot_detection/prepared-patches\", is_train=True, train_val_ratio=train_val_ratio)\n",
    "val_dataset = BrailleDataset(root_dir=\"../data/dot_detection/prepared-patches\", is_train=False, train_val_ratio=train_val_ratio)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# === Model ===\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# == Training loop ===\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for img, heatmap in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        img = img.to(device)\n",
    "        heatmap = heatmap.to(device)\n",
    "\n",
    "        # debug display image + heatmap\n",
    "        # img_np = img[0].cpu().numpy().squeeze(0)  # Shape: H x W\n",
    "        # img_np = (img_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "        # img_pil = Image.fromarray(img_np, mode='L')\n",
    "        # display(img_pil)\n",
    "        # heatmap_np = heatmap[0, 0].cpu().numpy()\n",
    "        # heatmap_img = Image.fromarray((cm.inferno(heatmap_np)[:, :, :3] * 255).astype('uint8'))\n",
    "        # display(heatmap_img)\n",
    "\n",
    "        pred = model(img)\n",
    "        loss = criterion(pred, heatmap)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * img.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, heatmap in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n",
    "            img = img.to(device)\n",
    "            heatmap = heatmap.to(device)\n",
    "\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, heatmap)\n",
    "\n",
    "            val_loss += loss.item() * img.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        ckpt_path = os.path.join(save_dir, f\"unet_best.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved best model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35073",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3149bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "image_dir = \"../data/dot_detection/test-data\"\n",
    "output_dir = \"../data/dot_detection/results/bulk\"\n",
    "model_path = \"../models/dot_detection/checkpoints/unet_best.pth\"\n",
    "sigma = 3\n",
    "peak_threshold = 0.5\n",
    "input_size = 384  # NxN target input\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e992762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (in_conv): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load model ===\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe4a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Retinex reflectance extractor ===\n",
    "def compute_retinex_reflectance(img_pil, sigma=30):\n",
    "    img_np = np.array(img_pil).astype(np.float32) + 1.0  # Avoid log(0)\n",
    "    log_img = np.log(img_np)\n",
    "\n",
    "    blurred = cv2.GaussianBlur(img_np, (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    log_blur = np.log(blurred + 1.0)\n",
    "\n",
    "    reflectance = log_img - log_blur\n",
    "    reflectance = (reflectance - reflectance.min()) / (reflectance.max() - reflectance.min()) * 255.0\n",
    "    reflectance = reflectance.astype(np.uint8)\n",
    "    return Image.fromarray(reflectance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a9cda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NMS ===\n",
    "def extract_peaks_from_heatmap(heatmap, threshold=0.5, dist=3):\n",
    "    heatmap = heatmap.squeeze(0)\n",
    "    pooled = F.max_pool2d(heatmap.unsqueeze(0).unsqueeze(0), kernel_size=2 * dist + 1, stride=1, padding=dist)\n",
    "    peak_mask = (heatmap == pooled.squeeze()) & (heatmap > threshold)\n",
    "    coords = peak_mask.nonzero(as_tuple=False)  # (y, x)\n",
    "    coords = coords[:, [1, 0]].cpu().numpy()  # (x, y)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea87527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load all test images ===\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tif', '*.tiff']\n",
    "img_paths = []\n",
    "for ext in image_extensions:\n",
    "    img_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "img_paths = sorted(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5210b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12 prediction results with visualizations to ../data/dot_detection/results/bulk\n"
     ]
    }
   ],
   "source": [
    "# === Run inference ===\n",
    "for idx, img_path in enumerate(img_paths):\n",
    "    original_img = Image.open(img_path).convert(\"L\")\n",
    "    original_img_resized = original_img.resize((input_size, input_size), Image.BILINEAR)\n",
    "\n",
    "    # Compute reflectance map via Retinex\n",
    "    reflectance_img = compute_retinex_reflectance(original_img_resized, sigma=50)\n",
    "    img_tensor = to_tensor(reflectance_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)  # [1, 1, N, N]\n",
    "        pred = pred.squeeze(0).cpu()\n",
    "\n",
    "    # Extract dot coords in resized image\n",
    "    coords = extract_peaks_from_heatmap(pred, threshold=peak_threshold, dist=int(sigma * 1.5))\n",
    "\n",
    "    # Draw on reflectance image\n",
    "    reflectance_with_boxes = reflectance_img.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(reflectance_with_boxes)\n",
    "    for x, y in coords:\n",
    "        draw.rectangle([x - 3, y - 3, x + 3, y + 3], outline=\"green\", width=2)\n",
    "\n",
    "    # === Save side-by-side plot with 3 panels ===\n",
    "    base_name = os.path.basename(img_path)\n",
    "    heatmap_save_path = os.path.join(output_dir, f\"{os.path.splitext(base_name)[0]}_viz.png\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(original_img_resized, cmap=\"gray\")\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(reflectance_with_boxes)\n",
    "    axs[1].set_title(\"Reflectance + Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(pred.squeeze(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "    axs[2].set_title(\"Predicted Heatmap\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heatmap_save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Saved {len(img_paths)} prediction results with visualizations to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0719b",
   "metadata": {},
   "source": [
    "# Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfbd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0779, Val Loss = 0.1374\n",
      "Saved best model to ../models/dot_detection/checkpoints\\unet_finetuned_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0532, Val Loss = 0.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0442, Val Loss = 0.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0784, Val Loss = 0.1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Epoch 5/30 [Val]:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "lr = 1e-3 / 10  # Lower learning rate for fine-tuning\n",
    "train_val_ratio = 0.8\n",
    "save_dir = \"../models/dot_detection/checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Load dataset ===\n",
    "train_dataset = BrailleDataset(root_dir=\"../data/dot_detection/MAN/prepared-patches\", is_train=True, train_val_ratio=train_val_ratio)\n",
    "val_dataset = BrailleDataset(root_dir=\"../data/dot_detection/MAN/prepared-patches\", is_train=False, train_val_ratio=train_val_ratio)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# === Model ===\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"unet_best.pth\"), map_location=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# == Training loop ===\n",
    "best_val_loss = float('inf')\n",
    "debug_count = 0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for img, heatmap in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "        img = img.to(device)\n",
    "        heatmap = heatmap.to(device)\n",
    "\n",
    "        # debug display image + heatmap\n",
    "        if debug_count < 20:\n",
    "            img_np = img[0].cpu().numpy().squeeze(0)  # Shape: H x W\n",
    "            img_np = (img_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "            img_pil = Image.fromarray(img_np, mode='L')\n",
    "            # display(img_pil)\n",
    "            heatmap_np = heatmap[0, 0].cpu().numpy()\n",
    "            heatmap_img = Image.fromarray((cm.inferno(heatmap_np)[:, :, :3] * 255).astype('uint8'))\n",
    "            # display(heatmap_img)\n",
    "            debug_count += 1\n",
    "\n",
    "        pred = model(img)\n",
    "        loss = criterion(pred, heatmap)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * img.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, heatmap in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n",
    "            img = img.to(device)\n",
    "            heatmap = heatmap.to(device)\n",
    "\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, heatmap)\n",
    "\n",
    "            val_loss += loss.item() * img.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        ckpt_path = os.path.join(save_dir, f\"unet_finetuned_best.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved best model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d4fe3",
   "metadata": {},
   "source": [
    "# Testing Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e905f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "image_dir = \"../data/dot_detection/test-data\"\n",
    "output_dir = \"../data/dot_detection/results/finetuned\"\n",
    "model_path = \"../models/dot_detection/checkpoints/unet_finetuned_best.pth\"\n",
    "sigma = 3\n",
    "peak_threshold = 0.5\n",
    "input_size = 384  # NxN target input\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b463139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (in_conv): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load model ===\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d4ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load all test images ===\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tif', '*.tiff']\n",
    "img_paths = []\n",
    "for ext in image_extensions:\n",
    "    img_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "img_paths = sorted(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4d17457",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === Run inference ===\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_paths):\n\u001b[1;32m----> 3\u001b[0m     original_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     original_img_resized \u001b[38;5;241m=\u001b[39m original_img\u001b[38;5;241m.\u001b[39mresize((input_size, input_size), Image\u001b[38;5;241m.\u001b[39mBILINEAR)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Compute reflectance map via Retinex\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aidan\\anaconda3\\envs\\msc_thesis\\lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Run inference ===\n",
    "for idx, img_path in enumerate(img_paths):\n",
    "    original_img = Image.open(img_path).convert(\"L\")\n",
    "    original_img_resized = original_img.resize((input_size, input_size), Image.BILINEAR)\n",
    "\n",
    "    # Compute reflectance map via Retinex\n",
    "    reflectance_img = compute_retinex_reflectance(original_img_resized, sigma=50)\n",
    "    img_tensor = to_tensor(reflectance_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)  # [1, 1, N, N]\n",
    "        pred = pred.squeeze(0).cpu()\n",
    "\n",
    "    # Extract dot coords in resized image\n",
    "    coords = extract_peaks_from_heatmap(pred, threshold=peak_threshold, dist=int(sigma * 1.5))\n",
    "\n",
    "    # Draw on reflectance image\n",
    "    reflectance_with_boxes = reflectance_img.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(reflectance_with_boxes)\n",
    "    for x, y in coords:\n",
    "        draw.rectangle([x - 3, y - 3, x + 3, y + 3], outline=\"green\", width=2)\n",
    "\n",
    "    # === Save side-by-side plot with 3 panels ===\n",
    "    base_name = os.path.basename(img_path)\n",
    "    heatmap_save_path = os.path.join(output_dir, f\"{os.path.splitext(base_name)[0]}_viz.png\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(original_img_resized, cmap=\"gray\")\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(reflectance_with_boxes)\n",
    "    axs[1].set_title(\"Reflectance + Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(pred.squeeze(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "    axs[2].set_title(\"Predicted Heatmap\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heatmap_save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Saved {len(img_paths)} prediction results with visualizations to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
