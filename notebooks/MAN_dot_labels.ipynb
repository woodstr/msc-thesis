{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb7524d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0466e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pylibdmtx.pylibdmtx import decode\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169f9e2",
   "metadata": {},
   "source": [
    "## Template Matching Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_scale_retinex(image, sigma=30):\n",
    "    \"\"\"\n",
    "    Does single scale retinex on the input image.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        sigma: Gaussian kernel size (default is 30)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of reflectance and illumination images\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32) + 1.0\n",
    "    illumination = cv2.GaussianBlur(image, (0, 0), sigma)\n",
    "    illumination += 1.0\n",
    "    reflectance = np.log(image) - np.log(illumination)\n",
    "    reflectance_display = cv2.normalize(reflectance, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    reflectance_display = reflectance_display.astype(np.uint8)\n",
    "    illumination_display = cv2.normalize(illumination, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return reflectance_display, illumination_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, scores, overlap_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression on the bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "        scores: List of scores for each bounding box\n",
    "        overlap_thresh: Overlap threshold for suppression (default is 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after non-maximum suppression\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 0] + boxes[:, 2]\n",
    "    y2 = boxes[:, 1] + boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while len(idxs) > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        overlap = inter / (areas[i] + areas[idxs[1:]] - inter)\n",
    "        idxs = idxs[1:][overlap < overlap_thresh]\n",
    "    return boxes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_dot_template(image, min_area=20, max_area=300, patch_size=(24, 24), offset=5, size_tol=0.5):\n",
    "    \"\"\"\n",
    "    Extracts the dominant dot template from the image.\n",
    "    The function applies a series of image processing techniques to identify and extract the dot template.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        min_area: Minimum area of the dot to be considered (default is 20)\n",
    "        max_area: Maximum area of the dot to be considered (default is 300)\n",
    "        patch_size: Size of the patch to be extracted (default is (24, 24))\n",
    "        offset: Offset for bounding box around the detected dot (default is 5)\n",
    "        size_tol: Tolerance for size consistency (default is 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the extracted patch and contours of the detected dots.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no valid dot candidates are found or if no size-consistent patches are found.\n",
    "    \"\"\"\n",
    "    image_clean = cv2.bilateralFilter(image, d=15, sigmaColor=50, sigmaSpace=5)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image_clean = clahe.apply(image_clean)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "    tophat = cv2.morphologyEx(image_clean, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    _, binary_top = cv2.threshold(tophat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_top, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    candidates = []\n",
    "    sizes = []\n",
    "    img_w, img_h = image.shape\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            crop_x_start = x - offset\n",
    "            crop_x_end = x + w + offset\n",
    "            crop_y_start = y - offset\n",
    "            crop_y_end = y + h + offset\n",
    "\n",
    "            if crop_x_start < 0 or crop_x_end >= img_w or crop_y_start < 0 or crop_y_end >= img_h:\n",
    "                continue\n",
    "\n",
    "            patch = image[crop_y_start:crop_y_end, crop_x_start:crop_x_end]\n",
    "            candidates.append((patch, h, w))\n",
    "            sizes.append((h, w))\n",
    "\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No valid dot candidates found.\")\n",
    "\n",
    "    # Compute median size\n",
    "    heights = [s[0] for s in sizes]\n",
    "    widths = [s[1] for s in sizes]\n",
    "    median_area = np.median(heights) * np.median(widths)\n",
    "\n",
    "    # Keep only patches with similar size\n",
    "    patches_filtered = []\n",
    "    resized_for_matching = []\n",
    "    for (patch, h, w) in candidates:\n",
    "        # print(abs(h * w - median_area))\n",
    "        if abs(h * w - median_area) / median_area < size_tol:\n",
    "            patches_filtered.append(patch)\n",
    "            resized_for_matching.append(cv2.resize(patch, patch_size))\n",
    "\n",
    "    if not patches_filtered:\n",
    "        raise ValueError(\"No size-consistent patches found.\")\n",
    "\n",
    "    # Find patch closest to the median template\n",
    "    stack = np.stack(resized_for_matching, axis=0).astype(np.float32)\n",
    "    median_template = np.median(stack, axis=0)\n",
    "    diffs = [np.linalg.norm(p.astype(np.float32) - median_template) for p in resized_for_matching]\n",
    "    best_idx = np.argmin(diffs)\n",
    "\n",
    "    return patches_filtered[best_idx], contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64924894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours_from_patch(patch):\n",
    "    \"\"\"\n",
    "    Extracts contours from supplied patch image.\n",
    "    \"\"\"\n",
    "    _, binary_patch = cv2.threshold(patch, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_patch, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1303247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Displays the numpy image using PIL and notebook display functionality.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        size: Size to which the image should be resized (default is (300, 300))\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, size)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_yucheng_methods(nms_boxes, reflectance, dot_contours, img, illumination, dot_template):\n",
    "    \"\"\"\n",
    "    Displays the results of Yuchengs methods for dot detection and template matching.\n",
    "\n",
    "    Args:\n",
    "        nms_boxes: List of bounding boxes after non-maximum suppression\n",
    "        reflectance: Reflectance map (numpy array)\n",
    "        dot_contours: Contours of the detected dots\n",
    "        img: Original image (numpy array)\n",
    "        illumination: Estimated illumination (numpy array)\n",
    "        dot_template: Dot template (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # === Draw matching result ===\n",
    "    output = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h) in nms_boxes:\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # === Draw contours over reflectance ===\n",
    "    contour_vis = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.drawContours(contour_vis, dot_contours, -1, (0, 0, 255), 1)\n",
    "\n",
    "    # === Show results ===\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "    axs[0, 0].imshow(img, cmap='gray')\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(illumination, cmap='gray')\n",
    "    axs[0, 1].set_title(\"Estimated Illumination\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    axs[0, 2].imshow(reflectance, cmap='gray')\n",
    "    axs[0, 2].set_title(\"Reflectance Map (SSR)\")\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    axs[1, 0].imshow(cv2.cvtColor(contour_vis, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Dot Contours\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(dot_template, cmap='gray')\n",
    "    axs[1, 1].set_title(\"Dot template (median of patches)\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    axs[1, 2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 2].set_title(\"Template matching\")\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15098784",
   "metadata": {},
   "source": [
    "# YOLO Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train4 is original oriented YOLO no rotation lock\n",
    "# model = YOLO(f'../yolo/runs/obb/train4/weights/best.pt') # load best model from training\n",
    "# model.eval() # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c97fa",
   "metadata": {},
   "source": [
    "# Keeping Relevant YOLO Crops Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2cd6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_imgs = os.listdir('../data/dot_detection/MAN/raw/')\n",
    "# raw_imgs = [os.path.join('../data/dot_detection/MAN/raw/', img) for img in raw_imgs]\n",
    "\n",
    "# misses = 0\n",
    "# for img_path in raw_imgs:\n",
    "#     result = model(img_path, verbose=False)[0]\n",
    "\n",
    "#     if len(result) == 0:\n",
    "#         misses += 1\n",
    "#         print(f'Missed image: {img_path}')\n",
    "#         continue\n",
    "\n",
    "#     # get xywhr from result\n",
    "#     xywhr = result.obb.xywhr.cpu().numpy()\n",
    "\n",
    "#     if len(xywhr) == 0:\n",
    "#         misses += 1\n",
    "#         print(f'Missed image: {img_path}')\n",
    "#         continue\n",
    "\n",
    "#     # load image\n",
    "#     img_path = result.path\n",
    "#     img = cv2.imread(img_path)\n",
    "\n",
    "#     # display image\n",
    "#     # display_image(img, size=(300, 300))\n",
    "\n",
    "#     # get coords for cropping\n",
    "#     x, y, w, h, r = xywhr[0]\n",
    "#     x = int(x)\n",
    "#     y = int(y)\n",
    "#     w = int(w)\n",
    "#     h = int(h)\n",
    "#     r = int(r)\n",
    "#     x1 = int(x - w / 2)\n",
    "#     y1 = int(y - h / 2)\n",
    "#     x2 = int(x + w / 2)\n",
    "#     y2 = int(y + h / 2)\n",
    "\n",
    "#     # adding some padding to the cropped image\n",
    "#     pac_perc = 0.5\n",
    "#     pad = int(pac_perc * max(w, h))\n",
    "#     x1 -= pad\n",
    "#     y1 -= pad\n",
    "#     x2 += pad\n",
    "#     y2 += pad\n",
    "\n",
    "#     x1 = max(0, x1)\n",
    "#     y1 = max(0, y1)\n",
    "#     x2 = min(img.shape[1], x2)\n",
    "#     y2 = min(img.shape[0], y2)\n",
    "#     img = img[y1:y2, x1:x2]\n",
    "#     if img.size == 0:  # Check if the cropped image is empty\n",
    "#         misses += 1\n",
    "#         print(f'Missed image: {img_path}')\n",
    "#         continue\n",
    "\n",
    "#     # display cropped image\n",
    "#     display_image(img, size=(300, 300))\n",
    "\n",
    "# print(f'Misses: {misses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d52e1",
   "metadata": {},
   "source": [
    "# Mapping Dictionary for Originals and Manual Template Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through raw dir and map to same filenames in template dir\n",
    "mapping = dict()\n",
    "\n",
    "raw_dir = '../data/dot_detection/MAN/raw/'\n",
    "template_dir = '../data/dot_detection/MAN/templates/'\n",
    "\n",
    "# sanity check equal number of files in both directories\n",
    "if len(os.listdir(raw_dir)) != len(os.listdir(template_dir)):\n",
    "    print(\"MISSING TEMPLATES!!!\")\n",
    "else:\n",
    "    print(\"got enough templates :)\")\n",
    "\n",
    "mapping = dict()\n",
    "for raw_file in os.listdir(raw_dir):\n",
    "    raw_file_path = os.path.join(raw_dir, raw_file)\n",
    "    template_file = raw_file.split('.')[0] + '.jpg'\n",
    "    template_file_path = os.path.join(template_dir, template_file)\n",
    "    if os.path.exists(template_file_path):\n",
    "        mapping[raw_file] = template_file_path\n",
    "    else:\n",
    "        print(f\"Missing template for {raw_file}\")\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504bfac",
   "metadata": {},
   "source": [
    "# Testing Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load image (grayscale) ===\n",
    "img_to_test = \"../data/dot_detection/MAN/raw/1D1165212740006.jpeg\"\n",
    "template_to_test = \"../data/dot_detection/MAN/templates/1D1165212740006.jpg\"\n",
    "img = cv2.imread(img_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (320, 320))\n",
    "dot_template = cv2.imread(template_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "dot_template = cv2.resize(dot_template, (19, 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541face2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply Retinex ===\n",
    "reflectance, illumination = single_scale_retinex(img, sigma=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_contours = contours_from_patch(dot_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Template matching ===\n",
    "result = cv2.matchTemplate(reflectance, dot_template, cv2.TM_CCOEFF_NORMED)\n",
    "threshold = 0.7\n",
    "locations = zip(*np.where(result >= threshold)[::-1])\n",
    "scores = result[result >= threshold].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc44261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bounding boxes (x, y, w, h) for each match ===\n",
    "h, w = dot_template.shape\n",
    "boxes = [(int(x), int(y), w, h) for (x, y) in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply NMS ===\n",
    "nms_boxes = non_max_suppression_fast(boxes, scores, overlap_thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d276fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_yucheng_methods(nms_boxes, reflectance, dot_contours, img, illumination, dot_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef8fcb",
   "metadata": {},
   "source": [
    "# Template Matching for Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525d10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
