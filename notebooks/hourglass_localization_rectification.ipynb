{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python libraries\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Third party libraries\n",
    "from pylibdmtx.pylibdmtx import encode\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from retinex import msrcr\n",
    "import skimage\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Custom funcs\n",
    "sys.path.append('../scripts')\n",
    "from hourglass import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256 # Input image size for model (orig hourglasses 256x256)\n",
    "output_size = 64 # Output heatmap size for model (orig hourglasses 64x64)\n",
    "yolo_size = 640  # YOLOv11 input size (orig yolo 640x640)\n",
    "n_stacks = 8     # Number of stacks in hourglass model\n",
    "\n",
    "sigma = 1        # Gaussian distribution (spread) for heatmap generation\n",
    "\n",
    "n_train = 800    # Number of training samples\n",
    "n_val = 100      # Number of validation samples\n",
    "n_test = 100     # Number of test samples\n",
    "\n",
    "batch_size = 1   # Batch size for training\n",
    "\n",
    "max_complex_epoch = 150 # Number of epochs where complex data is used\n",
    "\n",
    "# Calculating weight for loss function\n",
    "# pixels_per_gauss = (5 + 7 + (9*5) + 7 + 5) * 4 # Circles are built from vertical pixels of 5, 7, 9, 9, 9, 9, 9, 7, 5 (and there are 4 of them)\n",
    "pixels_per_gauss = (3 + 5 + 5 + 5 + 3) * 4 # Circles are built from vertical pixels of 3, 5, 5, 5, 3\n",
    "pixels_total = output_size*output_size\n",
    "weight = (pixels_total - pixels_per_gauss) / pixels_per_gauss\n",
    "print(weight) # There are this many times more background pixels than gaussian pixels\n",
    "\n",
    "reset_synth = False # Set to True to regenerate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with 7x7 patch from https://medium.com/towards-data-science/human-pose-estimation-with-stacked-hourglass-network-and-tensorflow-c4e9f84fd3ce\n",
    "# pixels_per_gauss = 7*7\n",
    "# pixels_total = 64*64\n",
    "# weight = (pixels_total - pixels_per_gauss) / pixels_per_gauss\n",
    "# print(weight) # prints ~82.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_string():\n",
    "    '''\n",
    "    Generates a serial number to encode\n",
    "    \n",
    "    Serial numbers are:\n",
    "    - 11 characters long\n",
    "    - Index 0, 2, 4, 5, 6, 7, 8, 9, 10 are random digits\n",
    "    - Index 1 and 3 are uppercase letters\n",
    "    - Index 11, 12, 13, 14 are an incremental number starting from 0001\n",
    "\n",
    "    Example serial number: 4 L 4 N 0418028 0001\n",
    "    '''\n",
    "\n",
    "    to_encode = ''\n",
    "\n",
    "    # first 11 indexes\n",
    "    for j in range(11):\n",
    "        # 1 and 3 are uppercase\n",
    "        if j in [1, 3]:\n",
    "            to_encode += random.choice(string.ascii_uppercase)\n",
    "        else:\n",
    "            to_encode += str(random.randrange(0, 10))\n",
    "\n",
    "    # last 4 indexes\n",
    "    end = str(random.randrange(1, 99))\n",
    "    if len(end) == 1:\n",
    "        end = '0' + end\n",
    "    elif len(end) == 2:\n",
    "        end = '00' + end\n",
    "    else:\n",
    "        end = '000' + end\n",
    "    to_encode += end\n",
    "\n",
    "    return to_encode\n",
    "\n",
    "def encode_image(to_encode, dmc_size):\n",
    "    '''Creates a PIL image containing DMC encoding of given string'''\n",
    "\n",
    "    encoded = encode(to_encode.encode('utf8'))\n",
    "    img = Image.frombytes('RGB', (encoded.width, encoded.height), encoded.pixels)\n",
    "\n",
    "    # upscale image\n",
    "    img = img.resize((dmc_size, dmc_size), Image.NEAREST)\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_corner_coords(img, debug=False):\n",
    "    '''Returns the coordinates of corners of DMC'''\n",
    "    padding_perc = 0.1 # Percentage of image size to use as padding\n",
    "    padding = int(padding_perc * img.width) # Padding around DMC info zone in pixels (we only want inner modules!)\n",
    "    # padding = 26 # Padding around DMC info zone in pixels (we only want inner modules!)\n",
    "\n",
    "    raw_coords = [] # Raw pixel coords of dmc corners\n",
    "    label_info = [] # Line by line label info\n",
    "\n",
    "    # Get coords of each corner of DMC\n",
    "    top_left = (padding-1, padding-1)\n",
    "    top_right = (img.width-padding, padding-1)\n",
    "    bottom_left = (padding-1, img.height-padding)\n",
    "    bottom_right = (img.width-padding, img.height-padding)\n",
    "\n",
    "    # Paint corners red for viz/debug\n",
    "    if debug:\n",
    "        img = img.convert('RGB')\n",
    "        img.putpixel(top_left, (255, 0, 0))\n",
    "        img.putpixel(top_right, (255, 0, 0))\n",
    "        img.putpixel(bottom_left, (255, 0, 0))\n",
    "        img.putpixel(bottom_right, (255, 0, 0))\n",
    "\n",
    "    # Normalize pixel coords to 0-1\n",
    "    top_left_norm = [top_left[0] / img.width, top_left[1] / img.height]\n",
    "    top_right_norm = [top_right[0] / img.width, top_right[1] / img.height]\n",
    "    bottom_left_norm = [bottom_left[0] / img.width, bottom_left[1] / img.height]\n",
    "    bottom_right_norm = [bottom_right[0] / img.width, bottom_right[1] / img.height]\n",
    "\n",
    "    # Add raw coords\n",
    "    raw_coords.extend([top_left, top_right, bottom_left, bottom_right])\n",
    "\n",
    "    # Add normalized coords\n",
    "    label_info.extend([top_left_norm, top_right_norm, bottom_left_norm, bottom_right_norm])\n",
    "\n",
    "    return raw_coords, label_info, img\n",
    "\n",
    "def get_heatmaps_basic(img, raw_coords, heatmap_size, debug=False):\n",
    "    '''Returns \"heatmaps\" for each corner of DMC, except they are just single points'''\n",
    "    \n",
    "    # Create empty heatmaps\n",
    "    heatmaps = np.zeros((4, heatmap_size, heatmap_size))\n",
    "\n",
    "    # Create \"heatmaps\" for each corner\n",
    "    for i in range(4):\n",
    "        # Create \"heatmap\"\n",
    "        heatmap = np.zeros((heatmap_size, heatmap_size))\n",
    "        for y in range(img.height):\n",
    "            for x in range(img.width):\n",
    "                if x == raw_coords[i][0] and y == raw_coords[i][1]:\n",
    "                    heatmap[y, x] = 1\n",
    "        \n",
    "        # Scale to heatmap_size x heatmap_size\n",
    "        heatmap = cv2.resize(heatmap, (heatmap_size, heatmap_size))\n",
    "\n",
    "        # Add heatmap to heatmaps\n",
    "        heatmaps[i] = heatmap\n",
    "    \n",
    "    # Paint heatmaps on image for viz/debug\n",
    "    if debug:\n",
    "        # Scale heatmaps to image size\n",
    "        debug_heatmaps = []\n",
    "        for i in range(4):\n",
    "            debug_heatmaps.append(cv2.resize(heatmaps[i], (img.width, img.height)))\n",
    "        for i in range(4):\n",
    "            for y in range(img.height):\n",
    "                for x in range(img.width):\n",
    "                    if debug_heatmaps[i][y, x] > 0:\n",
    "                        img.putpixel((x, y), (int(debug_heatmaps[i][y, x] * 255), 0, 0))\n",
    "    \n",
    "    return heatmaps\n",
    "\n",
    "def get_texture_crop(textures_path, size, debug=False):\n",
    "    '''Gets a random texture image from the given path and returns a random crop'''\n",
    "\n",
    "    texture = random.choice(os.listdir(textures_path))\n",
    "\n",
    "    if debug:\n",
    "        print(texture)\n",
    "\n",
    "    texture = Image.open(os.path.join(textures_path, texture))\n",
    "\n",
    "    # get random crop\n",
    "    transform = v2.Compose([\n",
    "        v2.RandomCrop((size, size))\n",
    "    ])\n",
    "    texture = transform(texture)\n",
    "\n",
    "    return texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing texture crops (uncomment debug print)\n",
    "get_texture_crop('../data/textures/', size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functions\n",
    "test = gen_string()\n",
    "img = encode_image(test, dmc_size=yolo_size)\n",
    "raw_coords, label_info, img = get_corner_coords(img, debug=True)\n",
    "print(raw_coords)\n",
    "print(label_info)\n",
    "heatmaps = get_heatmaps_basic(img, raw_coords, heatmap_size=yolo_size, debug=True)\n",
    "print(heatmaps.shape)\n",
    "print(heatmaps[0].shape)\n",
    "print(heatmaps[0])\n",
    "\n",
    "# Display image\n",
    "display(img)\n",
    "\n",
    "# Display heatmaps\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    heatmap = heatmaps[i]\n",
    "    # scale up to input size\n",
    "    heatmap = cv2.resize(heatmap, (yolo_size, yolo_size))\n",
    "    plt.imshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_save(type, yolo_size):\n",
    "    '''Generates a random serial number, encodes it into a DMC image, and saves it to train/val/test folders'''\n",
    "\n",
    "    to_encode = gen_string()\n",
    "    img = encode_image(to_encode, dmc_size=yolo_size)\n",
    "\n",
    "    # Get corner values\n",
    "    raw_coords, label_info, img = get_corner_coords(img)\n",
    "\n",
    "    # Debug print\n",
    "    if (0, 0) in raw_coords:\n",
    "        print(raw_coords)\n",
    "        error\n",
    "\n",
    "    # Get heatmaps (basic - single point as we will create the actual heatmaps during augmentation)\n",
    "    heatmaps = get_heatmaps_basic(img, raw_coords, heatmap_size=yolo_size, debug=False)\n",
    "\n",
    "    # Convert heatmaps to tensor\n",
    "    heatmaps = torch.tensor(heatmaps).float()\n",
    "\n",
    "    # Normalize image\n",
    "    img = np.array(img)\n",
    "    img = img / 255\n",
    "    img = torch.tensor(img).float()\n",
    "    img = img.permute(2, 0, 1) # Change to having channel dim first\n",
    "\n",
    "    # Generate random texture crop (size should match input for model)\n",
    "    texture = get_texture_crop('../data/textures/', size=yolo_size)\n",
    "\n",
    "    # Normalize texture\n",
    "    texture = np.array(texture)\n",
    "    texture = texture / 255\n",
    "    texture = torch.tensor(texture).float()\n",
    "    texture = texture.permute(2, 0, 1) # Change to having channel dim first\n",
    "\n",
    "    # Combine image, texture, and heatmaps into single tensor\n",
    "    img = torch.cat((img, texture, heatmaps), dim=0)\n",
    "\n",
    "    # Save image texture heatmaps wombo combo\n",
    "    torch.save(img, f'../data/hourglass_localization_rectification/{type}/{to_encode}.pt')\n",
    "\n",
    "    return\n",
    "\n",
    "def delete_old():\n",
    "    '''Deletes all images and labels in train/val/test folders'''\n",
    "\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        for file in os.listdir(f'../data/hourglass_localization_rectification/{folder}'):\n",
    "            os.remove(f'../data/hourglass_localization_rectification/{folder}/{file}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old synth data\n",
    "if reset_synth:\n",
    "    delete_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating train/val/test datasets\n",
    "print(f'Generating {n_train} train images...')\n",
    "for i in range(n_train - len(os.listdir('../data/hourglass_localization_rectification/train'))):\n",
    "    gen_save('train', yolo_size=yolo_size)\n",
    "\n",
    "print(f'Generating {n_val} val images...')\n",
    "for i in range(n_val - len(os.listdir('../data/hourglass_localization_rectification/val'))):\n",
    "    gen_save('val', yolo_size=yolo_size)\n",
    "\n",
    "print(f'Generating {n_test} test images...')\n",
    "for i in range(n_test - len(os.listdir('../data/hourglass_localization_rectification/test'))):\n",
    "    gen_save('test', yolo_size=yolo_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file\n",
    "def load_test_file():\n",
    "    '''Loads a test file for inspection'''\n",
    "\n",
    "    test_file = '../data/hourglass_localization_rectification/train/' + os.listdir('../data/hourglass_localization_rectification/train')[0]\n",
    "    test_tensor = torch.load(test_file)\n",
    "\n",
    "    print(test_file)\n",
    "    return test_tensor\n",
    "load_test_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for presenting images / creating PIL images\n",
    "def tensors2PIL(tensor, size, debug=False):\n",
    "    # Move to CPU\n",
    "    tensor = tensor.cpu()\n",
    "\n",
    "    # Split into DMC and texture images\n",
    "    dmc = tensor[:3].numpy()\n",
    "    texture = tensor[3:6].numpy()\n",
    "    heatmap = tensor[6:].numpy()\n",
    "\n",
    "    # Multiply by 255 to convert to 0-255 range\n",
    "    dmc = dmc * 255\n",
    "    texture = texture * 255\n",
    "\n",
    "    # Ensure no values are above 255\n",
    "    dmc[dmc > 255] = 255\n",
    "    texture[texture > 255] = 255\n",
    "\n",
    "    # Convert to uint8\n",
    "    dmc = dmc.astype(np.uint8)\n",
    "    texture = texture.astype(np.uint8)\n",
    "\n",
    "    # Reshape dmc to 3D\n",
    "    dmc = dmc.reshape(3, size, size).transpose(1, 2, 0)\n",
    "\n",
    "    # Reshape texture to 3D\n",
    "    texture = texture.reshape(3, size, size).transpose(1, 2, 0)\n",
    "\n",
    "    # Reshape heatmaps to 2D\n",
    "    heatmaps = []\n",
    "    for i in range(4):\n",
    "        heatmaps.append(heatmap[i].reshape(size, size))\n",
    "\n",
    "    # Convert to PIL images\n",
    "    dmc = Image.fromarray(dmc).convert('L')\n",
    "    texture = Image.fromarray(texture).convert('RGB')\n",
    "    for idx, heatmap in enumerate(heatmaps):\n",
    "        heatmap_colored = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)  # Apply color map\n",
    "        dmc_tmp = cv2.cvtColor(np.array(dmc), cv2.COLOR_GRAY2RGB)\n",
    "        overlay = cv2.addWeighted(dmc_tmp, 0.5, heatmap_colored, 0.5, 0) # Blend images\n",
    "        heatmap_img = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "        heatmaps[idx] = Image.fromarray(heatmap_img)\n",
    "\n",
    "    if debug:\n",
    "        display(dmc)\n",
    "        display(texture)\n",
    "        for heatmap in heatmaps:\n",
    "            display(heatmap)\n",
    "\n",
    "    return dmc, texture, heatmaps\n",
    "\n",
    "test_tensor = load_test_file()\n",
    "dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_n_symbols(tensor_image):\n",
    "    '''Adds text and symbols randomly around the DMC image'''\n",
    "    # Load DMC image\n",
    "    dmc = tensor_image[:3]\n",
    "\n",
    "    # Pad DMC to input size to allow for text/symbols to be added\n",
    "    dmc = v2.functional.pad(dmc, (yolo_size, yolo_size, yolo_size, yolo_size), fill=1)\n",
    "\n",
    "    # Convert to PIL image\n",
    "    dmc = v2.functional.to_pil_image(dmc)\n",
    "    dmc = dmc.convert('RGB')\n",
    "\n",
    "    # Create 8 empty segments to add to DMC image\n",
    "    segments = [Image.new('RGB', (yolo_size, yolo_size), (255, 255, 255)) for _ in range(8)]\n",
    "\n",
    "    # For 8 segments of the image, add random text\n",
    "    for i in range(8):\n",
    "        # 50% chance to add anything\n",
    "        if random.random() > 0.5:\n",
    "            font = ImageFont.load_default(random.randint(yolo_size/8, yolo_size/2))\n",
    "            text = ''\n",
    "\n",
    "            # 50% chance to add text\n",
    "            if random.random() > 0.5:\n",
    "                # From 1 to 10 characters of random letters and numbers\n",
    "                for _ in range(random.randint(1, 10)):\n",
    "                    text += random.choice(string.ascii_letters + string.digits)\n",
    "\n",
    "                # If len text divisible by 2 or 3, 50% chance to throw newline characters in\n",
    "                if len(text) % 2 == 0:\n",
    "                    if random.random() > 0.5:\n",
    "                        text = text[:len(text)//2] + '\\n' + text[len(text)//2:]\n",
    "                    \n",
    "                elif len(text) % 3 == 0:\n",
    "                    if random.random() > 0.5:\n",
    "                        if len(text) < 9:\n",
    "                            text = text[:len(text)//3] + '\\n' + text[len(text)//3:]\n",
    "                        else:\n",
    "                            text = text[:len(text)//3] + '\\n' + text[len(text)//3:len(text)//3*2] + '\\n' + text[len(text)//3*2:]\n",
    "\n",
    "            # else add symbol\n",
    "            else:\n",
    "                # Generate random symbol\n",
    "                text = random.choice(['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', '|', ';', ':', '<', '>', ',', '.', '/', '?'])\n",
    "\n",
    "            # Draw text/symbol on segment in random x and y position\n",
    "            ImageDraw.Draw(segments[i]).text((random.randint(0, yolo_size-100), random.randint(0, yolo_size-50)), text, font=font, fill=(0, 0, 0))\n",
    "            # ImageDraw.Draw(segments[i]).text((0, 0), text, font=font, fill=(0, 0, 0))\n",
    "        \n",
    "        # Else segment remains white\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Draw segments on DMC image\n",
    "    dmc.paste(segments[0], (0, 0))                     # top left\n",
    "    dmc.paste(segments[1], (yolo_size, 0))             # top\n",
    "    dmc.paste(segments[2], (yolo_size*2, 0))           # top right\n",
    "    dmc.paste(segments[3], (0, yolo_size))             # left\n",
    "    dmc.paste(segments[4], (yolo_size*2, yolo_size))   # right\n",
    "    dmc.paste(segments[5], (0, yolo_size*2))           # bottom left\n",
    "    dmc.paste(segments[6], (yolo_size, yolo_size*2))   # bottom\n",
    "    dmc.paste(segments[7], (yolo_size*2, yolo_size*2)) # bottom right\n",
    "\n",
    "    # Resize to input size\n",
    "    dmc = dmc.resize((yolo_size, yolo_size), Image.NEAREST)\n",
    "\n",
    "    # Convert to tensor\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    "    dmc = transform(dmc)\n",
    "\n",
    "    # Load, pad, resize heatmaps too\n",
    "    heatmaps = tensor_image[6:]\n",
    "    heatmaps = v2.functional.pad(heatmaps, (yolo_size, yolo_size, yolo_size, yolo_size), fill=0)\n",
    "    heatmaps = v2.functional.resize(heatmaps, (yolo_size, yolo_size))\n",
    "\n",
    "    # For each heatmap keep the brightest pixel only and make it 1\n",
    "    for i in range(4):\n",
    "        heatmap = heatmaps[i]\n",
    "        heatmap[heatmap != heatmap.max()] = 0\n",
    "        heatmap[heatmap == heatmap.max()] = 1\n",
    "        heatmaps[i] = heatmap\n",
    "\n",
    "    return dmc, heatmaps\n",
    "\n",
    "test_tensor = load_test_file()\n",
    "test_tensor[:3], test_tensor[6:] = add_text_n_symbols(test_tensor)\n",
    "dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flood_fill(tensor_image, x, y):\n",
    "    '''Finds and fills all connected black pixels in image starting from x, y'''\n",
    "    rows, cols = tensor_image.shape[-2], tensor_image.shape[-1]\n",
    "    visited = set()\n",
    "    stack = [(x, y)]\n",
    "\n",
    "    connected_pixels = []\n",
    "\n",
    "    while stack:\n",
    "        cx, cy = stack.pop()\n",
    "        if (cx, cy) in visited:\n",
    "            continue\n",
    "        visited.add((cx, cy))\n",
    "\n",
    "        # Ensure it's within bounds and is black\n",
    "        if 0 <= cx < rows and 0 <= cy < cols and tensor_image[cx, cy] == 0:\n",
    "            connected_pixels.append((cx, cy))\n",
    "\n",
    "            # Add neighbors (if not at the edge of the image)\n",
    "            if cx > 0:\n",
    "                stack.append((cx-1, cy))\n",
    "            if cx < rows-1:\n",
    "                stack.append((cx+1, cy))\n",
    "            if cy > 0:\n",
    "                stack.append((cx, cy-1))\n",
    "            if cy < cols-1:\n",
    "                stack.append((cx, cy+1))\n",
    "\n",
    "    for idx in connected_pixels:\n",
    "        tensor_image[idx[0], idx[1]] = 1\n",
    "\n",
    "    return tensor_image\n",
    "\n",
    "def shape_transform(tensor, training_mode=False, epoch=None, max_complex_epoch=max_complex_epoch, debug=False):\n",
    "    '''Applies random shape transformations to image'''\n",
    "    generation_error = False\n",
    "\n",
    "    # Split into DMC and heatmaps\n",
    "    dmc = tensor[:3]\n",
    "    heatmaps = tensor[6:]\n",
    "\n",
    "    # Temporarily downsize dmc to 1 channel to allow for next steps (IMPORTANT: THIS WILL SUCK IF SHAPE TRANSFORM SOMEHOW GETS USED AFTER COLOR TRANSFORM)\n",
    "    dmc = dmc[0]\n",
    "\n",
    "    # Stacking tensors for easier processing\n",
    "    batched = torch.stack((dmc, heatmaps[0], heatmaps[1], heatmaps[2], heatmaps[3]), dim=0)\n",
    "\n",
    "    # First pad batched tensor to ensure corners are not near the edge\n",
    "    # batched = v2.functional.pad(batched, padding=100, fill=0, padding_mode='constant')\n",
    "\n",
    "    # DEBUG\n",
    "    heatmaps = batched[1:]\n",
    "    for i in range(4):\n",
    "        brightest = heatmaps[i].max()\n",
    "        torch_mask = torch.where(heatmaps[i] == brightest, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        if generation_error == False and torch_mask.nonzero()[0].tolist() == [0, 0]:\n",
    "            print('generation_error - before transforms')\n",
    "            generation_error = True\n",
    "        heatmaps[i] = torch_mask\n",
    "\n",
    "    # Defining max scale and translations\n",
    "    max_scale = (0.8, 1.8)\n",
    "    max_translate = (0.2, 0.2)\n",
    "\n",
    "    if training_mode:\n",
    "        # Calculate scale and translate amount based on epoch\n",
    "        if epoch < 10:\n",
    "            scale = (1, 1)\n",
    "            translate = (0, 0)\n",
    "        # Increase scale and translate amount linearly from 10 to max_epoch\n",
    "        elif epoch < max_complex_epoch:\n",
    "            scale = 1 + (max_scale[1] - 1) * ((epoch - 10) / (max_complex_epoch - 10))\n",
    "            scale = (1-scale + 1, scale)\n",
    "            translate = max_translate[1] * ((epoch - 10) / (max_complex_epoch - 10))\n",
    "            translate = (translate, translate)\n",
    "        # After max_epoch, keep at max values\n",
    "        else:\n",
    "            scale = max_scale\n",
    "            translate = max_translate\n",
    "    else:\n",
    "        # If not in training mode, just use max values\n",
    "        scale = max_scale\n",
    "        translate = max_translate\n",
    "\n",
    "    if debug:\n",
    "        print(scale)\n",
    "        print(translate)\n",
    "\n",
    "    # Applying transformations to all tensors\n",
    "    transforms = v2.Compose([\n",
    "        v2.RandomRotation(180, # Random rotation (-180 to 180 degrees, so full rotation)\n",
    "                          interpolation=Image.BILINEAR,\n",
    "                          expand=True),\n",
    "        v2.RandomPerspective(distortion_scale=0.5,\n",
    "                             p=0.5,\n",
    "                             interpolation=Image.BILINEAR,\n",
    "                             ),\n",
    "        v2.RandomAffine(degrees=0, # No rotation\n",
    "                        shear=(-20, 20, -20, 20), # Random shear on x and y axis (squish)\n",
    "                        interpolation=Image.BILINEAR,\n",
    "                        scale=scale, # Random scaling (zoom in/out)\n",
    "                        translate=translate, # Random translation (move dmc around a bit)\n",
    "                        ),\n",
    "    ])\n",
    "    batched = transforms(batched)\n",
    "\n",
    "    # Resize all to original size\n",
    "    dmc_width, dmc_height = dmc.shape[-1], dmc.shape[-2]\n",
    "    batched = v2.functional.resize(batched, (dmc_height, dmc_width), interpolation=Image.BILINEAR)\n",
    "\n",
    "    # Binarize dmc\n",
    "    dmc = batched[0]\n",
    "    dmc = torch.where(dmc > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
    "\n",
    "    # DEBUG\n",
    "    heatmaps = batched[1:]\n",
    "    for i in range(4):\n",
    "        brightest = heatmaps[i].max()\n",
    "        torch_mask = torch.where(heatmaps[i] == brightest, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        if generation_error == False and torch_mask.nonzero()[0].tolist() == [0, 0]:\n",
    "            # print('generation_error - after transforms')\n",
    "            generation_error = True\n",
    "        heatmaps[i] = torch_mask\n",
    "\n",
    "    # Fill outer black areas of dmc with white\n",
    "    dmc = flood_fill(dmc, 0, 0)\n",
    "    dmc = flood_fill(dmc, 0, dmc.shape[-1]-1)\n",
    "    dmc = flood_fill(dmc, dmc.shape[-2]-1, 0)\n",
    "    dmc = flood_fill(dmc, dmc.shape[-2]-1, dmc.shape[-1]-1)\n",
    "\n",
    "    # Split back into DMC and heatmaps\n",
    "    dmc = dmc.unsqueeze(0)\n",
    "    heatmaps = batched[1:]\n",
    "\n",
    "    # Binarize heatmaps so that brightest pixel is 1 and rest are 0\n",
    "    for i in range(4):\n",
    "        brightest = heatmaps[i].max()\n",
    "        torch_mask = torch.where(heatmaps[i] == brightest, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        if generation_error == False and torch_mask.nonzero()[0].tolist() == [0, 0]:\n",
    "            print('generation_error - after paintbucket')\n",
    "        heatmaps[i] = torch_mask\n",
    "\n",
    "    # Add back color channels to DMC\n",
    "    dmc = torch.cat((dmc, dmc, dmc), dim=0)\n",
    "\n",
    "    return dmc, heatmaps, generation_error\n",
    "\n",
    "test_tensor = load_test_file()\n",
    "test_tensor[:3], test_tensor[6:] = add_text_n_symbols(test_tensor)\n",
    "test_tensor[:3], test_tensor[6:], generation_error = shape_transform(test_tensor, training_mode=True, epoch=150, max_complex_epoch=max_complex_epoch, debug=True)\n",
    "dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(tensor):\n",
    "    '''Combines DMC and texture tensors into one tensor'''\n",
    "    dmc = tensor[:3].unsqueeze(0)\n",
    "    texture = tensor[3:6].unsqueeze(0)\n",
    "\n",
    "    # Increment black pixel intensity of DMC randomly\n",
    "    increment = random.uniform(0.1, 0.5)\n",
    "    dmc = torch.where(dmc < 0.5, dmc + increment, dmc)\n",
    "\n",
    "    # Multiply DMC onto each texture channel\n",
    "    texture = torch.mul(texture, dmc)\n",
    "\n",
    "    # Remove batch dimension\n",
    "    texture = texture.squeeze(0)\n",
    "\n",
    "    return texture\n",
    "\n",
    "test_tensor = load_test_file()\n",
    "test_tensor[:3] = combine_images(test_tensor)\n",
    "combined_dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_transform(tensor):\n",
    "    '''Applies random color transformations to image'''\n",
    "\n",
    "    # Unsqueeze tensor to add batch dimension\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Random kernel size for gaussian blur\n",
    "    kernel_size = random.choice([3, 5])\n",
    "    sharpness = random.uniform(0.5, 1.5)\n",
    "\n",
    "    transforms = v2.Compose([\n",
    "        # v2.ColorJitter(brightness = (0.5, 1.5),\n",
    "        #                contrast   = (0.5, 1.5),\n",
    "        #                saturation = (0.5, 1.5),\n",
    "        #                hue        = (-0.5, 0.5),\n",
    "        #                ),\n",
    "        # v2.RandomChannelPermutation(),\n",
    "        v2.RandomPhotometricDistort(brightness = (0.5, 1.5),\n",
    "                                    contrast   = (0.5, 1.5),\n",
    "                                    saturation = (0.5, 1.5),\n",
    "                                    hue        = (-0.5, 0.5),\n",
    "                                    ),\n",
    "        v2.GaussianBlur(kernel_size=kernel_size, sigma=(0.1, 25)), # chance to blur a lot or a little - mostly an ok amount\n",
    "        # v2.GaussianNoise(), # not implemented for PIL images\n",
    "        # v2.RandomInvert(0.2), # lower chance of inversion # apply to code not background\n",
    "        # v2.RandomPosterize(8),\n",
    "        # v2.RandomSolarize(0.5, 0.5),\n",
    "        v2.RandomAdjustSharpness(sharpness, 0.5),\n",
    "        # v2.RandomAutocontrast(),\n",
    "        # v2.RandomEqualize(0.2), # lower chance of equalization\n",
    "    ])\n",
    "\n",
    "    # Dummy transform for testing\n",
    "    # transforms = v2.Compose([\n",
    "    #     v2.Pad(padding=0)\n",
    "    # ])\n",
    "\n",
    "    tensor = transforms(tensor)\n",
    "\n",
    "    # Remove batch dimension\n",
    "    tensor = tensor.squeeze(0)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Testing color_transform\n",
    "test_tensor = load_test_file()\n",
    "test_tensor[:3] = color_transform(test_tensor[:3])\n",
    "test_tensor[3:6] = color_transform(test_tensor[3:6])\n",
    "dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all transforms\n",
    "test_tensor = load_test_file()\n",
    "\n",
    "print('DMC TEXTURE PAIR')\n",
    "# dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)\n",
    "\n",
    "print('TEXT AND SYMBOLS')\n",
    "test_tensor[:3], test_tensor[6:] = add_text_n_symbols(test_tensor)\n",
    "# dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)\n",
    "\n",
    "print('SHAPE TRANSFORM')\n",
    "test_tensor[:3], test_tensor[6:], generation_error = shape_transform(test_tensor, training_mode=False, debug=True)\n",
    "# dmc, texture, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)\n",
    "\n",
    "if generation_error:\n",
    "    print('GENERATION ERROR')\n",
    "\n",
    "print('COMBINE IMAGES')\n",
    "dmc = combine_images(test_tensor)\n",
    "# dmc_img, texture_img, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)\n",
    "\n",
    "print('COLOR TRANSFORM')\n",
    "dmc = color_transform(dmc)\n",
    "\n",
    "# Overwrite for the sake of seeing\n",
    "test_tensor[:3] = dmc\n",
    "print('FINAL SIZES')\n",
    "print(dmc.shape)\n",
    "print(test_tensor[6:].shape)\n",
    "dmc_img, texture_img, heatmaps = tensors2PIL(test_tensor, size=yolo_size, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for resizing heatmaps\n",
    "def resize_heatmaps(heatmaps, size):\n",
    "    '''Resizes heatmaps to given size'''\n",
    "    new_heatmaps = np.zeros((4, size, size))\n",
    "    for i in range(4):\n",
    "        # Find proportionally where the max value is in the heatmap\n",
    "        y, x = np.unravel_index(np.argmax(heatmaps[i].cpu().numpy()), heatmaps[i].shape)\n",
    "        y = int(y * size / heatmaps[i].shape[0])\n",
    "        x = int(x * size / heatmaps[i].shape[1])\n",
    "\n",
    "        # Create new heatmap\n",
    "        heatmap = np.zeros((size, size))\n",
    "        heatmap[y, x] = 1\n",
    "\n",
    "        new_heatmaps[i] = heatmap\n",
    "\n",
    "    # Convert to tensor\n",
    "    new_heatmaps = torch.tensor(new_heatmaps).float()\n",
    "\n",
    "    return new_heatmaps\n",
    "\n",
    "# Actually creating the heatmaps from the pixel heatmaps\n",
    "def create_heatmap(heatmaps, size, sigma):\n",
    "    '''Creates heatmaps from pixel heatmaps'''\n",
    "    heatmap = np.zeros((size, size))\n",
    "    for pixel_heatmap in heatmaps:\n",
    "        # Center of keypoint\n",
    "        y, x = np.unravel_index(np.argmax(pixel_heatmap.cpu().numpy()), pixel_heatmap.shape)\n",
    "\n",
    "        # Fill heatmap with gaussian distribution (keeping only 95% of the distribution)\n",
    "        threshold = 0.05\n",
    "        for y2 in range(size):\n",
    "            for x2 in range(size):\n",
    "                value = np.exp(-((x2 - x)**2 + (y2 - y)**2) / (2 * sigma**2))  # Sigma = std of the Gaussian distribution\n",
    "                if value >= threshold:\n",
    "                    heatmap[y2, x2] = value\n",
    "\n",
    "    # Convert to tensor\n",
    "    heatmap = torch.tensor(heatmap).float()\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest YOLO model from \"trainXX\" folder\n",
    "model_to_train = [d.replace('train', '') for d in os.listdir('../yolo/runs/obb') if d.startswith('train')]\n",
    "if model_to_train == ['']:\n",
    "    model_to_train = ''\n",
    "else:\n",
    "    model_to_train.remove('') # remove empty string\n",
    "    model_to_train = sorted(model_to_train, key=int)[-1] # get the latest one\n",
    "print(f'Training YOLO from train{model_to_train}')\n",
    "\n",
    "YOLO_model = YOLO(f'../yolo/runs/obb/train{model_to_train}/weights/best.pt')\n",
    "YOLO_model.eval() # Set to eval mode\n",
    "\n",
    "def save_yolo_failure(image, heatmap):\n",
    "    '''Saves image and heatmap to yolo failures folder for further training'''\n",
    "    # Convert image to PIL\n",
    "    image = image.squeeze(0).permute(1, 2, 0).numpy()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # Filename is latest index filename + 1\n",
    "    if len(os.listdir('../data/hourglass_localization_rectification/yolo_failures/train/images/')) == 0:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = int(max(os.listdir('../data/hourglass_localization_rectification/yolo_failures/train/images/'), key=lambda x: int(x.split('.')[0])).split('.')[0]) + 1\n",
    "\n",
    "    image.save(f'../data/hourglass_localization_rectification/yolo_failures/train/images/{idx}.jpg')\n",
    "\n",
    "    # Get brightest pixels in heatmaps in YOLO train format (class_index x1 y1 x2 y2 x3 y3 x4 y4)\n",
    "    brightest_0_y, brightest_0_x = (heatmap[0]==torch.max(heatmap[0])).nonzero().tolist()[0]\n",
    "    brightest_1_y, brightest_1_x = (heatmap[1]==torch.max(heatmap[1])).nonzero().tolist()[0]\n",
    "    brightest_2_y, brightest_2_x = (heatmap[2]==torch.max(heatmap[2])).nonzero().tolist()[0]\n",
    "    brightest_3_y, brightest_3_x = (heatmap[3]==torch.max(heatmap[3])).nonzero().tolist()[0]\n",
    "\n",
    "    # Convert to proportional coordinates\n",
    "    height, width = image.size\n",
    "    x1 = brightest_0_x / width\n",
    "    y1 = brightest_0_y / height\n",
    "    x2 = brightest_1_x / width\n",
    "    y2 = brightest_1_y / height\n",
    "    x3 = brightest_2_x / width\n",
    "    y3 = brightest_2_y / height\n",
    "    x4 = brightest_3_x / width\n",
    "    y4 = brightest_3_y / height\n",
    "\n",
    "    # Save label in YOLO format\n",
    "    with open(f'../data/hourglass_localization_rectification/yolo_failures/train/labels/{idx}.txt', 'w') as f:\n",
    "        f.write(f'0 {x1} {y1} {x2} {y2} {x3} {y3} {x4} {y4}')\n",
    "    \n",
    "    return\n",
    "\n",
    "def YOLO_crop(model, image, heatmap):\n",
    "    '''Uses YOLO model to detect DMC in image tensor, and crops both image and heatmap tensors accordingly\n",
    "    Returns cropped image and heatmap tensors and error flag for if no DMC was found'''\n",
    "    # Combine image tensors into one RGB tensor\n",
    "    image = torch.stack((image[0], image[1], image[2]), dim=0)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Run YOLO model on image\n",
    "    results = model.predict(image, verbose=False)\n",
    "\n",
    "    # If no detections, raise error and save image and label for further training of YOLO\n",
    "    if results[0].obb is None or len(results[0].obb.xywhr) == 0:\n",
    "        print('No detections')\n",
    "        save_yolo_failure(image, heatmap)\n",
    "        return None, None, True\n",
    "\n",
    "    # Crop image tensor to bounding box\n",
    "    x, y, w, h, r = results[0].obb.xywhr[0]\n",
    "    cos_r, sin_r = torch.cos(r), torch.sin(r)\n",
    "    H, W = image.shape[2], image.shape[3]\n",
    "    pixel_pad = W * 0.1 # 10% of image size\n",
    "    h, w = h + pixel_pad, w + pixel_pad # Add padding to height and width\n",
    "    tx = 2*x / W - 1\n",
    "    ty = 2*y / H - 1\n",
    "    sx = w / W\n",
    "    sy = h / H\n",
    "    theta = torch.tensor([\n",
    "        [cos_r * sx, -sin_r * sy, tx],\n",
    "        [sin_r * sx, cos_r * sy, ty]\n",
    "        ], dtype=torch.float32)\n",
    "    theta = theta.unsqueeze(0)\n",
    "    h, w = int(h.item()), int(w.item())\n",
    "    grid = F.affine_grid(theta, (1, image.shape[1], h, w), align_corners=True)\n",
    "    image_crop = F.grid_sample(image, grid, align_corners=True)\n",
    "    image_crop = image_crop.squeeze(0)\n",
    "\n",
    "    # Crop heatmap tensors to bounding box\n",
    "    # theta = theta.repeat(4, 1, 1) # Repeat theta for each heatmap\n",
    "    grid = F.affine_grid(theta, (1, 4, h, w), align_corners=True)\n",
    "    heatmap_crop = heatmap.unsqueeze(0)\n",
    "    heatmap_crop = F.grid_sample(heatmap_crop, grid, align_corners=True)\n",
    "    heatmap_crop = heatmap_crop.squeeze(0)\n",
    "\n",
    "    # Keep only the brightest pixel in each heatmap\n",
    "    for i in range(4):\n",
    "        brightest = heatmap_crop[i].max()\n",
    "        torch_mask = torch.where(heatmap_crop[i] == brightest, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        heatmap_crop[i] = torch_mask\n",
    "    # for i in range(4):\n",
    "    #     print(f'heatmap {i} nonzero: {torch.count_nonzero(heatmap[i])}')\n",
    "\n",
    "    # # Display image (DEBUG)\n",
    "    # image_crop = image_crop.squeeze(0).permute(1, 2, 0).numpy()\n",
    "    # image_crop = (image_crop * 255).astype(np.uint8)\n",
    "    # image_crop = Image.fromarray(image_crop)\n",
    "    # display(image_crop)\n",
    "\n",
    "    # # Display grayscale heatmaps (DEBUG)\n",
    "    # plt.figure(figsize=(12, 12))\n",
    "    # for i in range(4):\n",
    "    #     plt.subplot(2, 2, i+1)\n",
    "    #     heatmap_i = heatmap_crop[i].squeeze(0).numpy()\n",
    "    #     heatmap_i = (heatmap_i * 255).astype(np.uint8)\n",
    "    #     heatmap_i = Image.fromarray(heatmap_i)\n",
    "    #     plt.imshow(heatmap_i)\n",
    "    #     plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    # Validate that each heatmap still has a single bright pixel\n",
    "    for i in range(4):\n",
    "        brightest = heatmap_crop[i].max()\n",
    "        torch_mask = torch.where(heatmap_crop[i] == brightest, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        if torch_mask.nonzero().shape[0] != 1:\n",
    "            print('generation_error - after YOLO crop')\n",
    "            save_yolo_failure(image, heatmap)\n",
    "            # Display image\n",
    "            image_crop = image_crop.squeeze(0).permute(1, 2, 0).numpy()\n",
    "            image_crop = (image_crop * 255).astype(np.uint8)\n",
    "            image_crop = Image.fromarray(image_crop)\n",
    "            display(image_crop)\n",
    "            return None, None, True\n",
    "\n",
    "    return image_crop, heatmap_crop, False\n",
    "\n",
    "image, heatmaps, yolo_err = YOLO_crop(YOLO_model, test_tensor[:3], test_tensor[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing YOLO failure folders\n",
    "for folder in ['train', 'val']:\n",
    "    for file in os.listdir(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/images/'):\n",
    "        os.remove(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/images/{file}')\n",
    "    for file in os.listdir(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/labels/'):\n",
    "        os.remove(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/labels/{file}')\n",
    "\n",
    "    # delete cache if exists\n",
    "    if os.path.exists(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/labels.cache'):\n",
    "        os.remove(f'../data/hourglass_localization_rectification/yolo_failures/{folder}/labels.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "class DMCDataset(Dataset):\n",
    "    def __init__(self, image_dir, training_mode=False, epoch=None, max_complex_epoch=max_complex_epoch):\n",
    "        self.image_dir = image_dir\n",
    "        self.files = os.listdir(image_dir)\n",
    "        self.training_mode = training_mode\n",
    "        self.epoch = epoch\n",
    "        self.max_complex_epoch = max_complex_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yolo_err = True\n",
    "        yolo_err_count = 0\n",
    "        yolo_max_err = 100\n",
    "\n",
    "        while yolo_err == True:\n",
    "\n",
    "            # Shape transform dmc and heatmaps\n",
    "            synth_err = True\n",
    "            synth_err_count = 0\n",
    "            while synth_err == True:\n",
    "                # Load the tensor containing dmc, texture, and heatmaps\n",
    "                tensor = torch.load(f'{self.image_dir}/{self.files[idx]}')\n",
    "                tensor[:3], tensor[6:] = add_text_n_symbols(tensor) # Add text and symbols\n",
    "                tensor[:3], tensor[6:], synth_err = shape_transform(tensor, training_mode=self.training_mode, epoch=self.epoch, max_complex_epoch=self.max_complex_epoch)\n",
    "                synth_err_count += 1\n",
    "\n",
    "            # Combine dmc and texture pair\n",
    "            dmc = combine_images(tensor)\n",
    "\n",
    "            # Color transform dmc\n",
    "            dmc = color_transform(dmc)\n",
    "\n",
    "            # Crop image and heatmaps using YOLO\n",
    "            dmc, heatmaps, yolo_err = YOLO_crop(YOLO_model, dmc, tensor[6:])\n",
    "\n",
    "            if yolo_err == True:\n",
    "                yolo_err_count += 1\n",
    "            if yolo_err_count >= yolo_max_err:\n",
    "                print('YOLO max error reached')\n",
    "                error\n",
    "\n",
    "        # Resize image to input size\n",
    "        dmc = v2.functional.resize(dmc, (input_size, input_size), interpolation=Image.BILINEAR)\n",
    "\n",
    "        # Resize heatmaps to output size\n",
    "        heatmaps = resize_heatmaps(heatmaps, output_size)\n",
    "\n",
    "        # Create actual heatmaps from binarized heatmaps\n",
    "        heatmap = create_heatmap(heatmaps, output_size, sigma)\n",
    "\n",
    "        return dmc, heatmap, synth_err_count\n",
    "\n",
    "def define_train_loader(epoch):\n",
    "    train_dataset = DMCDataset(\n",
    "        image_dir='../data/hourglass_localization_rectification/train',\n",
    "        training_mode=True,\n",
    "        epoch=epoch,\n",
    "        max_complex_epoch=max_complex_epoch,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "train_loader = define_train_loader(epoch=0)\n",
    "\n",
    "# Testing dataloader\n",
    "for dmc, heatmap, count in train_loader:\n",
    "    print(dmc.shape)\n",
    "    print(heatmap.shape)\n",
    "    print(count)\n",
    "    print(f'{count.sum().item()-batch_size} generation errors')\n",
    "\n",
    "    # Display dmc and heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('Off')\n",
    "    plt.imshow(dmc[0].squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('Off')\n",
    "    plt.imshow(heatmap[0].squeeze(0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DMCDataset(\n",
    "    image_dir='../data/hourglass_localization_rectification/val',\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataset = DMCDataset(\n",
    "    image_dir='../data/hourglass_localization_rectification/test',\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Hourglass Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with dataloader\n",
    "model = StackedHourglassNetwork(\n",
    "    num_stacks=n_stacks,\n",
    "    num_features=input_size,\n",
    "    num_output_points=1,\n",
    ")\n",
    "\n",
    "for images, heatmaps, count in train_loader:\n",
    "    print(images.shape)\n",
    "    print(heatmaps.shape)\n",
    "    print(f'{count.sum().item()-batch_size} generation errors')\n",
    "    outputs = model(images)\n",
    "    for output in outputs:\n",
    "        print(output.shape) # Expected: (8, 1, H, W) representing 1 heatmap per image\n",
    "        break\n",
    "    break\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to numpy for visualization\n",
    "heatmap = output[0, 0].detach().cpu().numpy()  # Shape: (1, H, W)\n",
    "print(heatmap.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(heatmap, cmap='jet')\n",
    "plt.title('Produced Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(heatmap, input_size, output_size):\n",
    "    heatmap = heatmap.copy() # Don't modify original heatmap\n",
    "\n",
    "    scale = input_size / output_size  # Scale factor to match original resolution\n",
    "    keypoints = []\n",
    "\n",
    "    # Find 4 brightest points in heatmap\n",
    "    for _ in range(4):\n",
    "        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "        keypoints.append((int(x * scale), int(y * scale))) # Scale keypoints\n",
    "        heatmap[y, x] = 0 # Remove brightest point to find next brightest point\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "def heatmap_viz(image, keypoints, heatmap, alpha=0.5):\n",
    "    # Combined figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Original image with keypoints\n",
    "    image_kp = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert for OpenCV\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)] # Colors are blue, green, red, yellow (BGR)\n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        cv2.circle(image_kp, (x, y), 5, colors[i], -1)  # Draw colored circle\n",
    "    axes[0].imshow(cv2.cvtColor(image_kp, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    # Heatmaps\n",
    "    heatmap_resized = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize to input image size\n",
    "    heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())  # Normalize\n",
    "    heatmap_colored = cv2.applyColorMap((heatmap_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)  # Apply color map\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, heatmap_colored, alpha, 0)  # Blend images\n",
    "    axes[1].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Heatmap')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vizualization usage with dataloader\n",
    "model = StackedHourglassNetwork(\n",
    "    num_stacks=n_stacks,\n",
    "    num_features=input_size,\n",
    "    num_output_points=1,\n",
    ")\n",
    "\n",
    "for images, labels, count in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "heatmaps = outputs[0][0][0].detach().cpu().numpy() # Convert tensor to numpy\n",
    "print(heatmaps.shape)\n",
    "keypoints = extract_keypoints(heatmaps, input_size=input_size, output_size=output_size)\n",
    "print('Predicted keypoints:', keypoints)\n",
    "\n",
    "img = images[0].permute(1, 2, 0).numpy() * 255\n",
    "img = img.astype(np.uint8) # Convert to ints only\n",
    "heatmap_viz(img, keypoints, heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vizualization of true keypoints and heatmaps\n",
    "for images, labels, count in train_loader:\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i].permute(1, 2, 0).numpy() * 255\n",
    "        image = image.astype(np.uint8) # Convert to ints only\n",
    "\n",
    "        heatmaps = labels[i].detach().cpu().numpy()  # Convert tensor to numpy\n",
    "        print(heatmaps.shape)\n",
    "        keypoints = extract_keypoints(heatmaps, input_size=input_size, output_size=output_size)\n",
    "        print('True keypoints:', keypoints)\n",
    "        heatmap_viz(image, keypoints, heatmaps)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(outputs, heatmaps):\n",
    "    '''Calculates the loss between predicted and target heatmaps'''\n",
    "\n",
    "    # outputs dims: (batch_size, num_stacks, num_keypoints, H, W)\n",
    "    # heatmaps dims: (batch_size, num_keypoints, H, W)\n",
    "\n",
    "    # Remove batch dims\n",
    "    outputs = outputs.squeeze(0) # Now dims: (num_stacks, num_keypoints, H, W)\n",
    "    heatmap = heatmaps.squeeze(0) # Now dims: (num_keypoints, H, W)\n",
    "\n",
    "    # IF I WANT TO TRY DOING BATCHES AGAIN JUST SUM OVER BATCH DIMENSION INSTEAD OF AVEREGING AT THE END\n",
    "\n",
    "    # losses = torch.zeros(outputs.shape[0]).to(device) # Loss for samples in batch\n",
    "\n",
    "    # # Iterate over batch\n",
    "    # for i in range(outputs.shape[0]):\n",
    "    #     heatmap = heatmaps[i]\n",
    "    #     weights = (heatmap > 0).float() * weight + 1 # Weighted loss (1 for pixels not part of gaussian, gaussian pixel value * weight for pixels part of gaussian)\n",
    "\n",
    "    #     # Iterate over stacks\n",
    "    #     for j in range(outputs.shape[1]):\n",
    "    #         output = outputs[i, j]\n",
    "    #         losses[i] += torch.mean((heatmap - output)**2 * weights)\n",
    "    \n",
    "    # loss = torch.mean(losses) # Average loss over batch\n",
    "\n",
    "    # Calculate loss for each stack\n",
    "    weights = (heatmap > 0).float() * weight + 1 # Weighted loss (1 for pixels not part of gaussian, gaussian pixel value * weight for pixels part of gaussian)\n",
    "    loss = 0\n",
    "    for i in range(outputs.shape[0]):\n",
    "        loss += torch.mean((heatmap - outputs[i])**2 * weights) # Accumulate loss over each stack and ground truth (weighted version)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output and Heatmaps with circle in the middle\n",
    "outputs = torch.zeros((batch_size, n_stacks, 1, 64, 64), device=device)\n",
    "outputs[:, :, :, 32, 32] = 0\n",
    "heatmaps = torch.zeros((batch_size, 1, 64, 64), device=device)\n",
    "heatmaps[:, :, 32, 32] = 1\n",
    "\n",
    "# Calculate loss\n",
    "print(f'outputs from model: {outputs.shape}')\n",
    "print(f'heatmaps pre loss: {heatmaps.shape}')\n",
    "loss = calculate_loss(outputs, heatmaps)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, early_stopper, train_loader, val_loader, epochs=10, lr=0.001, synthetic_training=False):\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = {'train': [], 'val': []} # Store losses\n",
    "\n",
    "    lowest_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    generation_errors = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        if synthetic_training:\n",
    "            train_loader = define_train_loader(epoch)\n",
    "\n",
    "        train_loss = 0\n",
    "        for images, heatmaps, count in train_loader:\n",
    "            images, heatmaps = images.to(device), heatmaps.to(device)\n",
    "\n",
    "            # Add generation errros\n",
    "            generation_errors += count.sum().item()-batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Custom loss function\n",
    "            loss = calculate_loss(outputs, heatmaps)\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        losses['train'].append(train_loss/len(train_loader))\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks, _ in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = calculate_loss(outputs, masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        losses['val'].append(val_loss/len(val_loader))\n",
    "\n",
    "        # Keep best model based on validation loss\n",
    "        if val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict() # Save best model state\n",
    "\n",
    "        # Early stopping if most complex examples are in and validation loss increases\n",
    "        if epoch > max_complex_epoch+early_stopper.patience and early_stopper.early_stop(val_loss):\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.5f}, Val Loss: {val_loss/len(val_loader):.5f}')\n",
    "\n",
    "    print('Best epoch:', best_epoch+1)\n",
    "    print(f'Total generation errors: {generation_errors}')\n",
    "\n",
    "    # Reload the best model weights\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, losses, lowest_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_train_model():\n",
    "    model = StackedHourglassNetwork(\n",
    "        num_stacks=n_stacks,\n",
    "        num_features=input_size,\n",
    "        num_output_points=1,\n",
    "        )\n",
    "\n",
    "    print('Initial training...')\n",
    "    early_stopper = EarlyStopper(patience=20, min_delta=0)\n",
    "    train_loader = define_train_loader(epoch=0)\n",
    "    model, losses, lowest_val_loss = train_model(model,\n",
    "                                                 early_stopper,\n",
    "                                                 train_loader,\n",
    "                                                 val_loader,\n",
    "                                                 epochs=1000,\n",
    "                                                 lr=2.5e-4, # As in paper\n",
    "                                                 synthetic_training=True\n",
    "                                                 )\n",
    "\n",
    "    print('\\nReduced LR training...')\n",
    "    early_stopper = EarlyStopper(patience=20, min_delta=0)\n",
    "    train_loader = define_train_loader(epoch=max_complex_epoch+1) # Ensuring most complex examples are being used\n",
    "    model, losses_tmp, lowest_val_loss_tmp = train_model(model,\n",
    "                                                         early_stopper,\n",
    "                                                         train_loader,\n",
    "                                                         val_loader,\n",
    "                                                         epochs=1000,\n",
    "                                                         lr=(2.5e-4) / 5, # Reduce by factor 5 (as in paper)\n",
    "                                                         synthetic_training=True\n",
    "                                                         )\n",
    "    \n",
    "    for loss in losses_tmp['train']:\n",
    "        losses['train'].append(loss)\n",
    "    for loss in losses_tmp['val']:\n",
    "        losses['val'].append(loss)\n",
    "\n",
    "    print('\\nTraining complete!')\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most recent change: random text and symbols added to synthesis\n",
    "model, losses = fancy_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and validation losses\n",
    "plt.plot(losses['train'], label='Train Loss')\n",
    "plt.plot(losses['val'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/hourglass_localization_rectification.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/hourglass_localization_rectification.pth', weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, heatmaps, _ in loader:\n",
    "            images, heatmaps = images.to(device), heatmaps.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = calculate_loss(outputs, heatmaps)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    print(f'Total loss: {total_loss:.4f}')\n",
    "    print(f'Average batch loss: {total_loss/count:.4f}')\n",
    "    print(f'Average single loss: {total_loss/(count*images.shape[0]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val loss\n",
    "evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_keypoints(orig_image, outputs, true_heatmaps):\n",
    "    '''Compares predicted and true keypoints for a single image'''\n",
    "    img = orig_image.permute(1, 2, 0).cpu().numpy() * 255\n",
    "    img = img.astype(np.uint8) # Convert to ints only\n",
    "\n",
    "    outputs_reshape = outputs[-1][0] # Grab last stack output heatmap 0 (64, 64)\n",
    "\n",
    "    pred_keypoints = extract_keypoints(outputs_reshape, input_size=input_size, output_size=output_size)\n",
    "    true_keypoints = extract_keypoints(true_heatmaps, input_size=input_size, output_size=output_size)\n",
    "\n",
    "    print(f'Predicted keypoints: {pred_keypoints}')\n",
    "    heatmap_viz(img, pred_keypoints, outputs_reshape)\n",
    "\n",
    "    print(f'True keypoints: {true_keypoints}')\n",
    "    heatmap_viz(img, true_keypoints, true_heatmaps)\n",
    "\n",
    "    return\n",
    "\n",
    "def compare_hourglass_outputs(orig_image, outputs, true_heatmaps):\n",
    "    '''Compares heatmaps across all hourglass outputs for a single image'''\n",
    "    # outputs is a tensor of heatmaps for each hourglass output, with dims (num_stacks, 1, 64, 64)\n",
    "    # true_heatmaps is the true heatmaps for the image (64, 64)\n",
    "    \n",
    "    # Display combined heatmaps\n",
    "    fig, axes = plt.subplots(1, len(outputs)+2, figsize=(20, 5))\n",
    "\n",
    "    # Display original image\n",
    "    dmc_image = orig_image.permute(1, 2, 0).cpu().numpy() * 255\n",
    "    dmc_image = dmc_image.astype(np.uint8) # Convert to ints only\n",
    "    axes[0].imshow(dmc_image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Display each hourglass output\n",
    "    for i in range(len(outputs)):\n",
    "        axes[i+1].imshow(outputs[i][0], cmap='jet')\n",
    "        axes[i+1].set_title(f'Hourglass {i+1}')\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "    # Display true heatmap\n",
    "    axes[-1].imshow(true_heatmaps, cmap='jet')\n",
    "    axes[-1].set_title('True Heatmap')\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "def print_single_result(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, true_heatmaps, _ in loader:\n",
    "            image, true_heatmaps = images.to(device)[0], true_heatmaps.numpy()[0]\n",
    "            # image shape: (3, 256, 256)\n",
    "            # true_heatmaps shape: (64, 64)\n",
    "            break\n",
    "\n",
    "        # Convert image\n",
    "        dmc_image = image.permute(1, 2, 0).cpu().numpy() * 255\n",
    "        dmc_image = dmc_image.astype(np.uint8) # Convert to ints only\n",
    "\n",
    "        # Single image forward pass\n",
    "        images = image.unsqueeze(0)\n",
    "        outputs = model(images)\n",
    "        outputs = outputs[0].detach().cpu().numpy() # Grab first batch (8, 1, 64, 64)\n",
    "\n",
    "        compare_keypoints(image, outputs, true_heatmaps)\n",
    "        compare_hourglass_outputs(image, outputs, true_heatmaps)\n",
    "\n",
    "def print_multiple_results(model, loader, n_print):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        printed = 0\n",
    "        for images, true_heatmaps, _ in loader:  \n",
    "            images, true_heatmaps = images.to(device), true_heatmaps.numpy()\n",
    "\n",
    "            for i in range(images.shape[0]):\n",
    "                if printed >= n_print:\n",
    "                    break\n",
    "                image, true_heatmap = images[i], true_heatmaps[i]\n",
    "\n",
    "                # Convert image\n",
    "                dmc_image = image.permute(1, 2, 0).cpu().numpy() * 255\n",
    "                dmc_image = dmc_image.astype(np.uint8)\n",
    "\n",
    "                # Single image forward pass\n",
    "                images = image.unsqueeze(0)\n",
    "                outputs = model(images)\n",
    "                outputs = outputs[0].detach().cpu().numpy() # Grab first batch (8, 1, 64, 64)\n",
    "\n",
    "                compare_keypoints(image, outputs, true_heatmap)\n",
    "                compare_hourglass_outputs(image, outputs, true_heatmap)\n",
    "\n",
    "                printed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_single_result(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_single_result(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_single_result(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning to MAN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (train, with augmentations)\n",
    "class MANDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, augment=False, retinex=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.label_dir = label_dir\n",
    "        self.label_files = sorted(os.listdir(label_dir))\n",
    "        self.augment = augment\n",
    "        self.retinex = retinex\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yolo_err = True\n",
    "        yolo_err_count = 0\n",
    "        yolo_max_err = 10\n",
    "\n",
    "        while yolo_err == True:\n",
    "            # Load the png image\n",
    "            image = Image.open(f'{self.image_dir}/{self.image_files[idx]}')\n",
    "\n",
    "            # Convert to tensor\n",
    "            transform = v2.Compose([\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True)\n",
    "            ])\n",
    "            dmc = transform(image)\n",
    "\n",
    "            # Resize to input size for model\n",
    "            dmc = v2.functional.resize(dmc, (yolo_size, yolo_size))\n",
    "\n",
    "            # Load the txt label\n",
    "            label = np.loadtxt(f'{self.label_dir}/{self.label_files[idx]}')\n",
    "\n",
    "            # Extract relative coords from label and convert to raw coords\n",
    "            relative_coords = [(label[1], label[2]), (label[3], label[4]), (label[5], label[6]), (label[7], label[8])]\n",
    "            raw_coords = []\n",
    "            for coord in relative_coords:\n",
    "                raw_coords.append((int(coord[0] * image.width), int(coord[1] * image.height)))\n",
    "\n",
    "            # Create basic heatmap\n",
    "            heatmap_basic = get_heatmaps_basic(image, raw_coords, yolo_size, debug=False)\n",
    "            heatmap_basic = torch.tensor(heatmap_basic).float()\n",
    "\n",
    "            if self.augment:\n",
    "                # Combine dmc and heatmap pair into single tensor\n",
    "                combined = torch.cat((dmc, heatmap_basic), dim=0)\n",
    "\n",
    "                # Apply augmentation transforms\n",
    "                transforms = v2.Compose([\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.RandomVerticalFlip(p=0.5),\n",
    "                ])\n",
    "                combined = transforms(combined)\n",
    "\n",
    "                # Split back into DMC and heatmaps\n",
    "                dmc = combined[:3]\n",
    "                heatmap_basic = combined[3:]\n",
    "\n",
    "            # Avoid crop if not augmenting and already failed once\n",
    "            if self.augment == False and yolo_err_count == 1:\n",
    "                print('YOLO max error reached (with no augmentation), not cropping with YOLO')\n",
    "                break\n",
    "            # Avoid crop if augmenting and error count too high\n",
    "            if yolo_err_count >= yolo_max_err:\n",
    "                print('YOLO max error reached, not cropping with YOLO')\n",
    "                break\n",
    "\n",
    "            # Crop image and heatmaps using YOLO\n",
    "            dmc, heatmap_basic, yolo_err = YOLO_crop(YOLO_model, dmc, heatmap_basic)\n",
    "\n",
    "            if yolo_err == True:\n",
    "                yolo_err_count += 1\n",
    "        \n",
    "        # Resize image to input size\n",
    "        dmc = v2.functional.resize(dmc, (input_size, input_size), interpolation=Image.BILINEAR)\n",
    "\n",
    "        # Resize heatmap to output size\n",
    "        heatmap_basic = v2.functional.resize(heatmap_basic, (output_size, output_size))\n",
    "        \n",
    "        if self.retinex:\n",
    "            img = dmc.permute(1, 2, 0).numpy()\n",
    "            img = skimage.img_as_ubyte(img)\n",
    "            img = msrcr(img, sigmas=(25., 50., 100.))\n",
    "            img = img.astype(np.float32) / 255\n",
    "            dmc = torch.tensor(img).permute(2, 0, 1)\n",
    "\n",
    "        # Create gaussian heatmap from basic heatmap\n",
    "        heatmap_gaussian = create_heatmap(heatmap_basic, output_size, sigma)\n",
    "\n",
    "        return dmc, heatmap_gaussian, 1\n",
    "\n",
    "MAN_train_dataset = MANDataset(\n",
    "    image_dir='../data/MAN/roboflow_oriented_boxes/train/images',\n",
    "    label_dir='../data/MAN/roboflow_oriented_boxes/train/labels',\n",
    "    augment=True,\n",
    "    retinex=False,\n",
    ")\n",
    "\n",
    "MAN_train_loader = torch.utils.data.DataLoader(\n",
    "    MAN_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Testing dataloader\n",
    "for dmc, heatmap, _ in MAN_train_loader:\n",
    "    print(dmc.shape)\n",
    "    print(heatmap.shape)\n",
    "\n",
    "    # Display dmc and heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('Off')\n",
    "    plt.imshow(dmc.squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('Off')\n",
    "    plt.imshow(heatmap.squeeze(0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAN_val_dataset = MANDataset(\n",
    "    image_dir='../data/MAN/roboflow_oriented_boxes/valid/images',\n",
    "    label_dir='../data/MAN/roboflow_oriented_boxes/valid/labels',\n",
    "    retinex=False\n",
    ")\n",
    "\n",
    "MAN_val_loader = torch.utils.data.DataLoader(\n",
    "    MAN_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "MAN_test_dataset = MANDataset(\n",
    "    image_dir='../data/MAN/roboflow_oriented_boxes/test/images',\n",
    "    label_dir='../data/MAN/roboflow_oriented_boxes/test/labels',\n",
    "    retinex=False\n",
    ")\n",
    "\n",
    "MAN_test_loader = torch.utils.data.DataLoader(\n",
    "    MAN_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune training\n",
    "def finetune_train_model(model):\n",
    "    '''Finetuning the model to MAN dataset. Similar training process to original training'''\n",
    "\n",
    "    # print('Initial training...')\n",
    "    # early_stopper = EarlyStopper(patience=20, min_delta=0)\n",
    "    # model, losses, lowest_val_loss = train_model(model,\n",
    "    #                                              early_stopper,\n",
    "    #                                              MAN_train_loader,\n",
    "    #                                              MAN_val_loader,\n",
    "    #                                              epochs=1000,\n",
    "    #                                              lr=2.5e-4 # As in paper\n",
    "    #                                              )\n",
    "\n",
    "    print('\\nReduced LR training...')\n",
    "    early_stopper = EarlyStopper(patience=20, min_delta=0)\n",
    "    model, losses, lowest_val_loss_tmp = train_model(model,\n",
    "                                                     early_stopper,\n",
    "                                                     MAN_train_loader,\n",
    "                                                     MAN_val_loader,\n",
    "                                                     epochs=1000,\n",
    "                                                     lr=(2.5e-4) / 5 # Reduce by factor 5 (as in paper)\n",
    "                                                     )\n",
    "    \n",
    "    # for loss in losses_tmp['train']:\n",
    "    #     losses['train'].append(loss)\n",
    "    # for loss in losses_tmp['val']:\n",
    "    #     losses['val'].append(loss)\n",
    "\n",
    "    print('\\nTraining complete!')\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic trained model\n",
    "model = torch.load('../models/hourglass_localization_rectification.pth', weights_only=False, map_location=device)\n",
    "\n",
    "# Make max_complex_epoch 0 since we are finetuning (no synthesis)\n",
    "max_complex_epoch = 0\n",
    "\n",
    "# Finetune model\n",
    "model, losses = finetune_train_model(model)\n",
    "\n",
    "# Save it\n",
    "torch.save(model, '../models/hourglass_localization_rectification_finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/hourglass_localization_rectification_finetuned.pth', weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss\n",
    "evaluate_model(model, MAN_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val loss\n",
    "evaluate_model(model, MAN_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss\n",
    "evaluate_model(model, MAN_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_single_result(model, MAN_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_single_result(model, MAN_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_multiple_results(model, MAN_test_loader, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
