{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e876f5cd",
   "metadata": {},
   "source": [
    "# Notes for Possible Improvements\n",
    "- Grid estimation could be improved by linear regression after assigning each point to appropriate gridline.\n",
    "    - Estimation using x,y coords of point centers for each horizontal and vertical grid. This would improve the gridline intersection accuracy on deformations and orientation errors.\n",
    "    - Would increase complexity with e.g. 256 regressions with 16x16 DMC\n",
    "    - Possible efficient solution could be to only have 2 or 4 regressions on DMC alignment pattern, and copying their params accross other gridlines.\n",
    "- Simple guaranteed error correction before final decoding process\n",
    "    - Force L and timing to be black as they are consistent\n",
    "- Implementation of decoding instead of using pylibdmtx\n",
    "    - Have not done yet due to complexity of it (even though it is deterministic algorithms)\n",
    "    - Could possibly \"steal\" relevant parts from pylibdmtx or libdmtx and rewrite for this purpose\n",
    "- Rewrites for squeezing out performance is possible\n",
    "    - Avoiding later matrix inversion by assigning 1s and 0s in reverse\n",
    "    - Avoiding multiple sorting by ensuring methods do not reorder points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a45fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from pylibdmtx.pylibdmtx import decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef5fd4",
   "metadata": {},
   "source": [
    "## Yucheng Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d6f2a",
   "metadata": {},
   "source": [
    "## Yucheng Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_scale_retinex(image, sigma=30):\n",
    "    \"\"\"\n",
    "    Does single scale retinex on the input image.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        sigma: Gaussian kernel size (default is 30)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of reflectance and illumination images\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32) + 1.0\n",
    "    illumination = cv2.GaussianBlur(image, (0, 0), sigma)\n",
    "    illumination += 1.0\n",
    "    reflectance = np.log(image) - np.log(illumination)\n",
    "    reflectance_display = cv2.normalize(reflectance, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    reflectance_display = reflectance_display.astype(np.uint8)\n",
    "    illumination_display = cv2.normalize(illumination, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return reflectance_display, illumination_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, scores, overlap_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression on the bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "        scores: List of scores for each bounding box\n",
    "        overlap_thresh: Overlap threshold for suppression (default is 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after non-maximum suppression\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 0] + boxes[:, 2]\n",
    "    y2 = boxes[:, 1] + boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while len(idxs) > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        overlap = inter / (areas[i] + areas[idxs[1:]] - inter)\n",
    "        idxs = idxs[1:][overlap < overlap_thresh]\n",
    "    return boxes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef549aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_dot_template(image, min_area=20, max_area=300, patch_size=(24, 24), offset=5, size_tol=0.5):\n",
    "    \"\"\"\n",
    "    Extracts the dominant dot template from the image.\n",
    "    The function applies a series of image processing techniques to identify and extract the dot template.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        min_area: Minimum area of the dot to be considered (default is 20)\n",
    "        max_area: Maximum area of the dot to be considered (default is 300)\n",
    "        patch_size: Size of the patch to be extracted (default is (24, 24))\n",
    "        offset: Offset for bounding box around the detected dot (default is 5)\n",
    "        size_tol: Tolerance for size consistency (default is 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the extracted patch and contours of the detected dots.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no valid dot candidates are found or if no size-consistent patches are found.\n",
    "    \"\"\"\n",
    "    image_clean = cv2.bilateralFilter(image, d=15, sigmaColor=50, sigmaSpace=5)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image_clean = clahe.apply(image_clean)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "    tophat = cv2.morphologyEx(image_clean, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    _, binary_top = cv2.threshold(tophat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_top, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    candidates = []\n",
    "    sizes = []\n",
    "    img_w, img_h = image.shape\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            crop_x_start = x - offset\n",
    "            crop_x_end = x + w + offset\n",
    "            crop_y_start = y - offset\n",
    "            crop_y_end = y + h + offset\n",
    "\n",
    "            if crop_x_start < 0 or crop_x_end >= img_w or crop_y_start < 0 or crop_y_end >= img_h:\n",
    "                continue\n",
    "\n",
    "            patch = image[crop_y_start:crop_y_end, crop_x_start:crop_x_end]\n",
    "            candidates.append((patch, h, w))\n",
    "            sizes.append((h, w))\n",
    "\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No valid dot candidates found.\")\n",
    "\n",
    "    # Compute median size\n",
    "    heights = [s[0] for s in sizes]\n",
    "    widths = [s[1] for s in sizes]\n",
    "    median_area = np.median(heights) * np.median(widths)\n",
    "\n",
    "    # Keep only patches with similar size\n",
    "    patches_filtered = []\n",
    "    resized_for_matching = []\n",
    "    for (patch, h, w) in candidates:\n",
    "        # print(abs(h * w - median_area))\n",
    "        if abs(h * w - median_area) / median_area < size_tol:\n",
    "            patches_filtered.append(patch)\n",
    "            resized_for_matching.append(cv2.resize(patch, patch_size))\n",
    "\n",
    "    if not patches_filtered:\n",
    "        raise ValueError(\"No size-consistent patches found.\")\n",
    "\n",
    "    # Find patch closest to the median template\n",
    "    stack = np.stack(resized_for_matching, axis=0).astype(np.float32)\n",
    "    median_template = np.median(stack, axis=0)\n",
    "    diffs = [np.linalg.norm(p.astype(np.float32) - median_template) for p in resized_for_matching]\n",
    "    best_idx = np.argmin(diffs)\n",
    "\n",
    "    return patches_filtered[best_idx], contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours_from_patch(patch):\n",
    "    \"\"\"\n",
    "    Extracts contours from supplied patch image.\n",
    "    \"\"\"\n",
    "    _, binary_patch = cv2.threshold(patch, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_patch, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f710876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Displays the numpy image using PIL and notebook display functionality.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        size: Size to which the image should be resized (default is (300, 300))\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, size)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a90c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_yucheng_methods(nms_boxes, reflectance, dot_contours, img, illumination, dot_template):\n",
    "    \"\"\"\n",
    "    Displays the results of Yuchengs methods for dot detection and template matching.\n",
    "\n",
    "    Args:\n",
    "        nms_boxes: List of bounding boxes after non-maximum suppression\n",
    "        reflectance: Reflectance map (numpy array)\n",
    "        dot_contours: Contours of the detected dots\n",
    "        img: Original image (numpy array)\n",
    "        illumination: Estimated illumination (numpy array)\n",
    "        dot_template: Dot template (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # === Draw matching result ===\n",
    "    output = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h) in nms_boxes:\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # === Draw contours over reflectance ===\n",
    "    contour_vis = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.drawContours(contour_vis, dot_contours, -1, (0, 0, 255), 1)\n",
    "\n",
    "    # === Show results ===\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "    axs[0, 0].imshow(img, cmap='gray')\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(illumination, cmap='gray')\n",
    "    axs[0, 1].set_title(\"Estimated Illumination\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    axs[0, 2].imshow(reflectance, cmap='gray')\n",
    "    axs[0, 2].set_title(\"Reflectance Map (SSR)\")\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    axs[1, 0].imshow(cv2.cvtColor(contour_vis, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Dot Contours\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(dot_template, cmap='gray')\n",
    "    axs[1, 1].set_title(\"Dot template (median of patches)\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    axs[1, 2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 2].set_title(\"Template matching\")\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb86f39",
   "metadata": {},
   "source": [
    "## Yucheng Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load image (grayscale) ===\n",
    "img_to_test = \"../data/delete.jpg\"\n",
    "template_to_test = \"../data/delete_template.jpg\"\n",
    "img = cv2.imread(img_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (320, 320))\n",
    "dot_template = cv2.imread(template_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "dot_template = cv2.resize(dot_template, (19, 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply Retinex ===\n",
    "reflectance, illumination = single_scale_retinex(img, sigma=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec23de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_contours = contours_from_patch(dot_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Extract dominant template and contours ===\n",
    "# # REPLACE WITH YOUR ACTUAL TEMPLATE\n",
    "# dot_template, dot_contours = extract_dominant_dot_template(reflectance,\n",
    "#                                                            min_area=20,\n",
    "#                                                            max_area=300,\n",
    "#                                                            patch_size=(24, 24),\n",
    "#                                                            offset=5,\n",
    "#                                                            size_tol=0.5)\n",
    "\n",
    "# display_image(dot_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2930ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Template matching ===\n",
    "result = cv2.matchTemplate(reflectance, dot_template, cv2.TM_CCOEFF_NORMED)\n",
    "threshold = 0.7\n",
    "locations = zip(*np.where(result >= threshold)[::-1])\n",
    "scores = result[result >= threshold].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5835ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bounding boxes (x, y, w, h) for each match ===\n",
    "h, w = dot_template.shape\n",
    "boxes = [(int(x), int(y), w, h) for (x, y) in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply NMS ===\n",
    "nms_boxes = non_max_suppression_fast(boxes, scores, overlap_thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_yucheng_methods(nms_boxes, reflectance, dot_contours, img, illumination, dot_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2725a5",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcffc71",
   "metadata": {},
   "source": [
    "## Decoding Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e39847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_affine(x0, y0, a, b, c, d, grid_size=16):\n",
    "    coords = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])\n",
    "    A = np.array([[a, b], [c, d]])\n",
    "    transformed = (A @ coords.T).T\n",
    "    return transformed + np.array([x0, y0]), coords  # return both transformed grid and (i,j) indices\n",
    "\n",
    "def invert_affine(p, x0, y0, a, b, c, d):\n",
    "    A = np.array([[a, b], [c, d]])\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    return (A_inv @ (p - np.array([x0, y0]))).T  # returns (i, j)\n",
    "\n",
    "def cost(params, observed_points):\n",
    "    x0, y0, a, b, c, d = params\n",
    "    grid_points, _ = generate_grid_affine(x0, y0, a, b, c, d)\n",
    "    dists = np.linalg.norm(observed_points[:, None, :] - grid_points[None, :, :], axis=2)\n",
    "    min_dists = np.min(dists, axis=1)\n",
    "    return np.mean(np.minimum(min_dists**2, 10.0))  # robust loss\n",
    "\n",
    "def show_grid(img, grid_pts):\n",
    "    for p in grid_pts:\n",
    "        cv2.circle(img, (int(p[0]), int(p[1])), 3, (0, 255, 0), -1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(\"Grid Points\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def estimate_grid(nms_boxes):\n",
    "    observed_pts = np.array([[x + w / 2, y + h / 2] for (x, y, w, h) in nms_boxes])\n",
    "    # x, y based on most left top corner (coordinate with smallest x y)\n",
    "    smallest_xy = float('inf')\n",
    "    x, y = 0, 0\n",
    "    for (x0, y0, w, h) in nms_boxes:\n",
    "        avg = ((x0 + w / 2) + (y0 + h / 2)) / 2\n",
    "        if avg < smallest_xy:\n",
    "            smallest_xy = avg\n",
    "            x, y = x0 + w / 2, y0 + h / 2\n",
    "\n",
    "    print(f\"Estimated top left corner: ({x}, {y})\")\n",
    "    init_params = [x, y, 18, 0, 0, 18]  # reasonable guess\n",
    "    result = minimize(cost, init_params, args=(observed_pts,), method='Powell')\n",
    "    x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt = result.x\n",
    "\n",
    "    return x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt, observed_pts\n",
    "\n",
    "def plot_grid_and_observed(grid_pts, observed_pts, ij_valid, observed_valid, x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Plot grid lines\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            p = invert_affine(np.array([i, j]), 0, 0, a_opt, b_opt, c_opt, d_opt)\n",
    "    grid_lines, _ = generate_grid_affine(x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt)\n",
    "\n",
    "    # Plot full grid\n",
    "    plt.scatter(grid_pts[:,0], grid_pts[:,1], color='lightgray', label='Grid Points', s=10)\n",
    "\n",
    "    # Plot observed dots\n",
    "    plt.scatter(observed_pts[:,0], observed_pts[:,1], color='blue', label='Observed Dots')\n",
    "\n",
    "    # Connect observed to nearest estimated grid point\n",
    "    for pt, (i,j) in zip(observed_valid, ij_valid):\n",
    "        grid_xy = generate_grid_affine(x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt, grid_size=16)[0][i*16 + j]\n",
    "        plt.plot([pt[0], grid_xy[0]], [pt[1], grid_xy[1]], 'r-', alpha=0.3)\n",
    "\n",
    "    # Optionally, annotate with grid indices\n",
    "    for pt, (i, j) in zip(observed_valid, ij_valid):\n",
    "        plt.text(pt[0]+0.5, pt[1]+0.5, f'({i},{j})', fontsize=8, color='green')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.title('Observed Points Mapped to Grid Indices')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_DMC(matrix):\n",
    "    \"\"\"\n",
    "    Display the DMC matrix like normal DMC.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "    \"\"\"\n",
    "    # Invert the matrix for display\n",
    "    matrix = np.invert(matrix)\n",
    "    plt.imshow(matrix, cmap='gray', interpolation='nearest')\n",
    "    plt.title(\"DMC Matrix\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea99af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_DMC(matrix):\n",
    "    \"\"\"\n",
    "    Decodes the DMC matrix using pylibdmtx.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # Converting binary matrix to uint8 image\n",
    "    image = np.zeros((matrix.shape[0], matrix.shape[1]), dtype=np.uint8)\n",
    "    image[matrix == 1] = 255\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Inverting the image for decoding\n",
    "    image = Image.eval(image, lambda x: 255 - x)\n",
    "\n",
    "    # Padding the image by 2 pixels to add margin larger than a DMC module (https://www.keyence.eu/ss/products/auto_id/codereader/basic_2d/datamatrix.jsp)\n",
    "    image = np.pad(np.array(image), ((2, 2), (2, 2)), mode='constant', constant_values=255)\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Resizing to larger image for better decoding\n",
    "    image = image.resize((image.size[0] * 10, image.size[1] * 10), Image.NEAREST)\n",
    "\n",
    "    # Decode using pylibdmtx\n",
    "    decoded = decode(image)\n",
    "    if decoded:\n",
    "        return decoded[0].data.decode('utf-8')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467774b",
   "metadata": {},
   "source": [
    "## Decoding Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Estimate grid and plot ===\n",
    "x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt, observed_pts = estimate_grid(nms_boxes)\n",
    "\n",
    "grid_pts, grid_indices = generate_grid_affine(x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6af117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert each observed point to get (i, j)\n",
    "ij_estimates = np.array([invert_affine(p, x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt) for p in observed_pts])\n",
    "\n",
    "# Round to nearest integer to get index estimate\n",
    "ij_rounded = np.round(ij_estimates).astype(int)\n",
    "\n",
    "# Keep only valid indices (within grid bounds)\n",
    "valid_mask = np.all((ij_rounded >= 0) & (ij_rounded < 16), axis=1)\n",
    "ij_valid = ij_rounded[valid_mask] # valid indices (what we use for decoding!!!)\n",
    "observed_valid = observed_pts[valid_mask]\n",
    "\n",
    "# Will look flipped and rotated\n",
    "plot_grid_and_observed(grid_pts, observed_pts, ij_valid, observed_valid, x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b28986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decode the Data Matrix ===\n",
    "# Get filled grid points\n",
    "filled_grid = np.zeros((16, 16), dtype=bool)\n",
    "for i, j in ij_valid:\n",
    "    filled_grid[i, j] = True\n",
    "\n",
    "# flip grid vertically\n",
    "filled_grid_flipped = np.flipud(filled_grid)\n",
    "# rotate grid 90 deg clockwise\n",
    "filled_grid_rotated = np.rot90(filled_grid_flipped, k=-1)\n",
    "display_DMC(filled_grid_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decoding DMC ===\n",
    "decoded_data = decode_DMC(filled_grid_rotated)\n",
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff2487",
   "metadata": {},
   "source": [
    "# Full Decoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pipeline(image_path, template_path, debug=False, rotation=None):\n",
    "    \"\"\"\n",
    "    Performs the entire decoding pipeline on the input image and template.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        template_path: Path to the template image\n",
    "    \n",
    "    Returns:\n",
    "        Decoded data from the DMC matrix or None if decoding fails.\n",
    "    \"\"\"\n",
    "    # === Load image (grayscale) ===\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (320, 320))\n",
    "    dot_template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "    dot_template = cv2.resize(dot_template, (19, 19))\n",
    "\n",
    "    # === Rotation test ===\n",
    "    if rotation is not None:\n",
    "        M = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), rotation, 1)\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # === Apply Retinex ===\n",
    "    reflectance, illumination = single_scale_retinex(img, sigma=64)\n",
    "\n",
    "    # === Extract dot contours ===\n",
    "    dot_contours = contours_from_patch(dot_template)\n",
    "\n",
    "    # === Template matching ===\n",
    "    result = cv2.matchTemplate(reflectance, dot_template, cv2.TM_CCOEFF_NORMED)\n",
    "    threshold = 0.7\n",
    "    locations = zip(*np.where(result >= threshold)[::-1])\n",
    "    scores = result[result >= threshold].flatten()\n",
    "\n",
    "    # === Bounding boxes (x, y, w, h) for each match ===\n",
    "    h, w = dot_template.shape\n",
    "    boxes = [(int(x), int(y), w, h) for (x, y) in locations]\n",
    "\n",
    "    # === Apply NMS ===\n",
    "    nms_boxes = non_max_suppression_fast(boxes, scores, overlap_thresh=0.3)\n",
    "\n",
    "    if debug:\n",
    "        display_yucheng_methods(nms_boxes, reflectance, dot_contours, img, illumination, dot_template)\n",
    "\n",
    "    # === Estimating Grid ===\n",
    "    x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt, observed_pts = estimate_grid(nms_boxes)\n",
    "\n",
    "    # === Show the grid points ===\n",
    "    grid_pts, grid_indices = generate_grid_affine(x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt)\n",
    "    if debug:\n",
    "        show_grid(img, grid_pts)\n",
    "    \n",
    "    # Invert each observed point to get (i, j)\n",
    "    ij_estimates = np.array([invert_affine(p, x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt) for p in observed_pts])\n",
    "\n",
    "    # Round to nearest integer to get index estimate\n",
    "    ij_rounded = np.round(ij_estimates).astype(int)\n",
    "\n",
    "    # Keep only valid indices (within grid bounds)\n",
    "    valid_mask = np.all((ij_rounded >= 0) & (ij_rounded < 16), axis=1)\n",
    "    ij_valid = ij_rounded[valid_mask] # valid indices (what we use for decoding!!!)\n",
    "    observed_valid = observed_pts[valid_mask]\n",
    "\n",
    "    if debug:\n",
    "        plot_grid_and_observed(grid_pts, observed_pts, ij_valid, observed_valid, x0_opt, y0_opt, a_opt, b_opt, c_opt, d_opt)\n",
    "    \n",
    "    # === Decode the Data Matrix ===\n",
    "    # Get filled grid points\n",
    "    matrix = np.zeros((16, 16), dtype=bool)\n",
    "    for i, j in ij_valid:\n",
    "        matrix[i, j] = True\n",
    "\n",
    "    # flip grid vertically\n",
    "    matrix = np.flipud(matrix)\n",
    "    # rotate grid 90 deg clockwise\n",
    "    matrix = np.rot90(matrix, k=-1)\n",
    "    display_DMC(matrix)\n",
    "\n",
    "    # === Extra fix for finder patter ===\n",
    "    matrix[:, 0] = 1  # left finder pattern\n",
    "    matrix[-1, :] = 1 # bottom finder pattern\n",
    "    # top finder pattern (top even indices)\n",
    "    for i in range(0, 16, 2):\n",
    "        matrix[0, i] = 1\n",
    "    # right finder pattern (right odd indices)\n",
    "    for i in range(1, 16, 2):\n",
    "        matrix[i, -1] = 1\n",
    "\n",
    "    # === Decoding DMC ===\n",
    "    decoded_data = decode_DMC(matrix)\n",
    "\n",
    "    return decoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba11c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_test = \"../data/delete.jpg\"\n",
    "template_to_test = \"../data/delete_template.jpg\"\n",
    "decode_pipeline(img_to_test, template_to_test, debug=True, rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
