{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e876f5cd",
   "metadata": {},
   "source": [
    "# Notes for Possible Improvements\n",
    "- Implementation of decoding instead of using pylibdmtx\n",
    "    - Have not done yet due to complexity of it (even though it is deterministic algorithms)\n",
    "    - Could possibly \"steal\" relevant parts from pylibdmtx or libdmtx and rewrite for this purpose\n",
    "- Rewrites for squeezing out performance is possible\n",
    "    - Avoiding later matrix inversion by assigning 1s and 0s in reverse\n",
    "    - Avoiding multiple sorting by ensuring methods do not reorder points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a45fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import math\n",
    "\n",
    "from pylibdmtx.pylibdmtx import decode\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dd122",
   "metadata": {},
   "source": [
    "## Simple Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple helper functions\n",
    "def get_mode(lst):\n",
    "    \"\"\"Get the mode of a list.\"\"\"\n",
    "    return Counter(lst).most_common(1)[0][0]\n",
    "\n",
    "def load_image_to_device(image_path):\n",
    "    \"\"\"Load an image to the device.\"\"\"\n",
    "    image = read_image(image_path).to(device) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # if image has 4 channels, convert to 3 channels\n",
    "    if image.shape[0] == 4:\n",
    "        image = image[:3, :, :]\n",
    "\n",
    "    return image.unsqueeze(0) # Add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fbfee",
   "metadata": {},
   "source": [
    "# YOLO Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo(model_path):\n",
    "    \"\"\"\n",
    "    Load the YOLOv11 model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the YOLOv11 model file.\n",
    "\n",
    "    Returns:\n",
    "        YOLO: The loaded YOLOv11 model.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    model.fuse()  # fuse model for faster inference\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def yolo_detect(image, model, debug=False):\n",
    "    \"\"\"\n",
    "    Detect object with highest confidence in an image using oriented YOLO.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        model (YOLO): The YOLOv11 model.\n",
    "        debug (bool): If True, display the image with bounding box.\n",
    "\n",
    "    Returns:\n",
    "        list: xywhr bounding box for object detected in the image.\n",
    "    \"\"\"\n",
    "    results = model.predict(image, verbose=False)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"YOLO did not detect any objects.\")\n",
    "        return None\n",
    "\n",
    "    result = results[0]\n",
    "    if len(result.obb.xywhr) == 0:\n",
    "        print(\"YOLO did not detect any objects.\")\n",
    "        return None\n",
    "    \n",
    "    if debug:\n",
    "        result.save(filename='deleteme.jpg')\n",
    "        debug_image = Image.open('deleteme.jpg')\n",
    "        plt.imshow(debug_image)\n",
    "        plt.axis('off')\n",
    "        plt.title('YOLO detection')\n",
    "        plt.show()\n",
    "        os.remove('deleteme.jpg')\n",
    "\n",
    "    return result.obb.xywhr[0]\n",
    "\n",
    "def yolo_crop(image, xywhr, pad):\n",
    "    \"\"\"\n",
    "    Crop the image using YOLOv11 oriented bounding box.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        xywhr (torch.Tensor): xywhr bounding box for object detected in the image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The cropped image.\n",
    "        float: The padding applied to the image.\n",
    "    \"\"\"\n",
    "    x, y, w, h, r = xywhr\n",
    "    x, y, w, h, r = float(x), float(y), float(w), float(h), float(r)\n",
    "\n",
    "    # padding by pad% of longest side\n",
    "    pad = max(image.shape[2:]) * pad\n",
    "    w_crop = int(w + 2 * pad)\n",
    "    h_crop = int(h + 2 * pad)\n",
    "\n",
    "    # image dimensions\n",
    "    _, _, H, W = image.shape\n",
    "\n",
    "    # normalize center to [-1, 1] (as required by affine_grid)\n",
    "    cx = (2 * x / W) - 1\n",
    "    cy = (2 * y / H) - 1\n",
    "\n",
    "    # normalize width/height to [0, 2] (scale relative to image)\n",
    "    sx = w_crop / W\n",
    "    sy = h_crop / H\n",
    "\n",
    "    # convert deg to rad and compute rotation matrix\n",
    "    theta = -math.radians(r)\n",
    "    cos_t = math.cos(theta)\n",
    "    sin_t = math.sin(theta)\n",
    "\n",
    "    # build affine matrix (batch of 1)\n",
    "    affine_matrix = torch.tensor([[\n",
    "        [cos_t * sx, -sin_t * sy, cx],\n",
    "        [sin_t * sx,  cos_t * sy, cy],\n",
    "    ]], dtype=torch.float32, device=image.device)\n",
    "\n",
    "    # creat grid and sample the image\n",
    "    grid = F.affine_grid(affine_matrix, size=(1, image.shape[1], h_crop, w_crop), align_corners=False)\n",
    "    cropped_image = F.grid_sample(image, grid, align_corners=False, padding_mode='zeros')\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def yolo_detect_and_crop(image, model, pad, debug=False):\n",
    "    \"\"\"\n",
    "    Detects an crops down to an object in the image using oriented YOLOv11.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        model (YOLO): The YOLOv11 model.\n",
    "        debug (bool): If True, display the cropped image.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The cropped image.\n",
    "        float: The height of the bounding box.\n",
    "        float: The width of the bounding box.\n",
    "    \"\"\"\n",
    "    image_yolo = transforms.Resize((640, 640))(image.clone()) # resize image to 640x640\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xywhr = yolo_detect(image_yolo, model, debug)\n",
    "\n",
    "    if xywhr is None:\n",
    "        print('WARNING, no object detected, returning original image')\n",
    "        return image, 0, 0 # return original image if no object detected\n",
    "\n",
    "    # scale detection back to original image size\n",
    "    _, _, H_orig, W_orig = image.shape\n",
    "\n",
    "    scale_x = W_orig / 640\n",
    "    scale_y = H_orig / 640\n",
    "\n",
    "    x, y, w, h, r = [float(v) for v in xywhr]\n",
    "    x *= scale_x\n",
    "    y *= scale_y\n",
    "    w *= scale_x\n",
    "    h *= scale_y\n",
    "    xywhr_scaled = torch.tensor([x, y, w, h, r], dtype=torch.float32, device=image.device)\n",
    "\n",
    "    image = yolo_crop(image, xywhr_scaled, pad)\n",
    "\n",
    "    if debug:\n",
    "        image_tmp = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        plt.imshow(image_tmp)\n",
    "        plt.axis('off')\n",
    "        plt.title('YOLO crop post-padding')\n",
    "        plt.show()\n",
    "\n",
    "    return image\n",
    "\n",
    "# === load image ===\n",
    "# train4 is original oriented YOLO no rotation lock\n",
    "yolo_path = '../yolo/runs/obb/train4/weights/best.pt'\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === load & run YOLOv11 ===\n",
    "yolo_path = '../yolo/runs/obb/train4/weights/best.pt'\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48a00f",
   "metadata": {},
   "source": [
    "# Template Acquiring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b986",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf394224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv → BN → ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling → concat → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # pad if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_c=64):\n",
    "        super().__init__()\n",
    "        self.in_conv = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c, base_c * 2)\n",
    "        self.down2 = Down(base_c * 2, base_c * 4)\n",
    "        self.down3 = Down(base_c * 4, base_c * 8)\n",
    "        self.down4 = Down(base_c * 8, base_c * 8)\n",
    "\n",
    "        self.up1 = Up(base_c * 16, base_c * 4)\n",
    "        self.up2 = Up(base_c * 8, base_c * 2)\n",
    "        self.up3 = Up(base_c * 4, base_c)\n",
    "        self.up4 = Up(base_c * 2, base_c)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_c, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return torch.sigmoid(self.out_conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e1928",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_retinex_reflectance_torch(img_tensor, sigma=30):\n",
    "    \"\"\"Reflectance extraction using Retinex algorithm.\"\"\"\n",
    "    eps = 1e-6\n",
    "    img = img_tensor.clamp(min=eps) # avoids log(0) without shifting scale\n",
    "    log_img = torch.log(img)\n",
    "\n",
    "    def get_gaussian_kernel2d(kernel_size, sigma):\n",
    "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.).to(img_tensor.device)\n",
    "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "        return kernel\n",
    "    \n",
    "    # approximate kernel size from sigma\n",
    "    kernel_size = int(2 * math.ceil(3 * sigma) + 1)\n",
    "    kernel = get_gaussian_kernel2d(kernel_size, sigma)\n",
    "    kernel = kernel.to(img_tensor.device).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "\n",
    "    # apply gaussian blur to each channel\n",
    "    channels = img_tensor.shape[1]\n",
    "    kernel = kernel.expand(channels, 1, -1, -1) # [C, 1, kH, kW]\n",
    "    blurred = F.conv2d(img_tensor, kernel, padding=kernel_size//2, groups=channels)\n",
    "    blurred = blurred.clamp(min=eps) # avoids log(0) without shifting scale\n",
    "    log_blur = torch.log(blurred)\n",
    "\n",
    "    reflectance = log_img - log_blur\n",
    "\n",
    "    def normalize(tensor):\n",
    "        return (tensor - tensor.amin(dim=(1,2,3), keepdim=True)) / (tensor.amax(dim=(1,2,3), keepdim=True) + eps)\n",
    "\n",
    "    # normalize to [0, 1]\n",
    "    reflectance = normalize(reflectance)\n",
    "    illumination = normalize(log_blur)\n",
    "\n",
    "    return reflectance, illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unet(model_path):\n",
    "    \"\"\"\n",
    "    Load the UNet model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the UNet model file.\n",
    "\n",
    "    Returns:\n",
    "        UNet: The loaded UNet model.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def unet_get_template(image_yolo, model, k_templates, debug=False):\n",
    "    \"\"\"\n",
    "    Get the template from the image using UNet.\n",
    "\n",
    "    Args:\n",
    "        image_yolo (torch.Tensor): The input image as a tensor.\n",
    "        h (float): Height of the YOLO bounding box.\n",
    "        w (float): Width of the YOLO bounding box.\n",
    "        model (UNet): The UNet model.\n",
    "        debug (bool): Whether to show debug information.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The template extracted from the image.\n",
    "    \"\"\"\n",
    "    # === prepare image for UNet ===\n",
    "    image = transforms.Resize((384, 384), Image.BILINEAR)(image_yolo)\n",
    "    image = transforms.Grayscale(num_output_channels=1)(image) # convert to grayscale\n",
    "    reflectance, illumination = compute_retinex_reflectance_torch(image, sigma=50)\n",
    "\n",
    "    if debug:\n",
    "        reflectance_np = reflectance.squeeze().cpu().numpy()\n",
    "        plt.imshow(reflectance_np, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Reflectance Image')\n",
    "        plt.show()\n",
    "\n",
    "        illumination_np = illumination.squeeze().cpu().numpy()\n",
    "        plt.imshow(illumination_np, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Illumination Image')\n",
    "        plt.show()\n",
    "\n",
    "    # === get UNet heatmap ===\n",
    "    with torch.no_grad():\n",
    "        heatmap = model(reflectance)\n",
    "    heatmap_np = heatmap.squeeze().cpu().numpy()\n",
    "    if debug:\n",
    "        plt.imshow(heatmap_np, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('UNet Heatmap')\n",
    "        plt.show()\n",
    "\n",
    "    # === get the templates from the heatmap ===\n",
    "    templates = []\n",
    "    for i in range(k_templates):\n",
    "        # selected from brightest pixel\n",
    "        y, x = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "\n",
    "        if debug:\n",
    "            plt.imshow(heatmap_np, cmap='gray')\n",
    "            plt.scatter(x, y, color='red', s=50)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'UNet Heatmap with Brightest Pixel ({x}, {y})')\n",
    "            plt.show()\n",
    "\n",
    "        mask = np.zeros_like(heatmap_np, dtype=np.uint8)\n",
    "        cv2.circle(mask, (x, y), radius=12, color=1, thickness=-1) # keep small region around the pixel\n",
    "        local_heatmap = heatmap_np * mask\n",
    "        threshold = np.max(local_heatmap) * 0.05 # can adjust (higher = smaller area)\n",
    "        binary_mask = (local_heatmap >= threshold).astype(np.uint8)\n",
    "\n",
    "        # optional: clean with morphological opening to remove noise\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # get contour of bounding box\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not contours:\n",
    "            print(\"No contours found in the binary mask.\")\n",
    "            return None\n",
    "        \n",
    "        # choose largest contour\n",
    "        largest_contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "        x, y, w_box, h_box = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        # clamp box size to expected dot size range (e.g. max 5% of image size)\n",
    "        max_size = min(heatmap_np.shape) * 0.05\n",
    "        side = int(min(max(w_box, h_box), max_size))\n",
    "\n",
    "        # recenter bounding box around centroid (not top-left)\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M[\"m00\"] == 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        else:\n",
    "            cx, cy = x + w_box // 2, y + h_box // 2\n",
    "\n",
    "        # compute square bounds\n",
    "        x1 = max(cx - side // 2, 0)\n",
    "        y1 = max(cy - side // 2, 0)\n",
    "        x2 = min(x1 + side, heatmap_np.shape[1])\n",
    "        y2 = min(y1 + side, heatmap_np.shape[0])\n",
    "\n",
    "        if debug:\n",
    "            plt.imshow(heatmap_np, cmap='gray')\n",
    "            plt.gca().add_patch(plt.Rectangle((x1, y1), side, side, edgecolor='red', facecolor='none', lw=2))\n",
    "            plt.title(\"Template Area on Heatmap\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            # extract the template from the heatmap\n",
    "            template = heatmap_np[y1:y2, x1:x2]\n",
    "            plt.imshow(template, cmap='gray')\n",
    "            plt.title(\"Extracted Template from Heatmap\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            # getting template from reflectance image\n",
    "            template = reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            template = template[y1:y2, x1:x2]\n",
    "            template = cv2.resize(template, (side, side)) # Resize to square\n",
    "\n",
    "            if debug:\n",
    "                plt.imshow(reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "                plt.gca().add_patch(plt.Rectangle((x1, y1), side, side, edgecolor='red', facecolor='none', lw=2))\n",
    "                plt.title(\"Template Area on Reflectance Image\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                plt.imshow(template, cmap='gray')\n",
    "                plt.title(\"Extracted Template from reflectance\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "        # extract the template from the reflectance image\n",
    "        template = reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        template = template[y1:y2, x1:x2]\n",
    "        template = template[:, :, 0] # (H, W, 1) -> (H, W)\n",
    "\n",
    "        # if not square, resize to square\n",
    "        if template.shape[0] != template.shape[1]:\n",
    "            side = max(template.shape)\n",
    "            template = cv2.resize(template, (side, side))\n",
    "\n",
    "        # normalize template to [0, 255]\n",
    "        template = ((template - np.min(template)) / (np.max(template) - np.min(template)) * 255).astype(np.uint8)\n",
    "        \n",
    "        # add template to list and remove from heatmap\n",
    "        templates.append(template)\n",
    "        heatmap_np[y1:y2, x1:x2] = 0\n",
    "\n",
    "    # === post-process for use in template matching ===\n",
    "    reflectance = reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 1)\n",
    "    reflectance = reflectance[:, :, 0] # remove extra channel\n",
    "    illumination = illumination.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 1)\n",
    "    illumination = illumination[:, :, 0] # remove extra channel\n",
    "\n",
    "    # normalize both to [0, 255]\n",
    "    reflectance = ((reflectance - np.min(reflectance)) / (np.max(reflectance) - np.min(reflectance)) * 255).astype(np.uint8)\n",
    "\n",
    "    return reflectance, illumination, templates\n",
    "\n",
    "# === load image ===\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === get yolo crop ===\n",
    "yolo_path = '../yolo/runs/obb/train4/weights/best.pt'\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad, debug=True)\n",
    "\n",
    "# === get unet template ===\n",
    "unet_path = '../models/dot_detection/checkpoints/unet_best.pth'\n",
    "unet_model = load_unet(unet_path)\n",
    "reflectance, illumination, templates = unet_get_template(image_yolo, unet_model, k_templates=3, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d176e",
   "metadata": {},
   "source": [
    "# Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_scale_retinex(image, sigma=30):\n",
    "    \"\"\"\n",
    "    Does single scale retinex on the input image.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        sigma: Gaussian kernel size (default is 30)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of reflectance and illumination images\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32) + 1.0\n",
    "    illumination = cv2.GaussianBlur(image, (0, 0), sigma)\n",
    "    illumination += 1.0\n",
    "    reflectance = np.log(image) - np.log(illumination)\n",
    "    reflectance_display = cv2.normalize(reflectance, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    reflectance_display = reflectance_display.astype(np.uint8)\n",
    "    illumination_display = cv2.normalize(illumination, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return reflectance_display, illumination_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, scores, overlap_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression on the bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "        scores: List of scores for each bounding box\n",
    "        overlap_thresh: Overlap threshold for suppression (default is 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after non-maximum suppression\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 0] + boxes[:, 2]\n",
    "    y2 = boxes[:, 1] + boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while len(idxs) > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        overlap = inter / (areas[i] + areas[idxs[1:]] - inter)\n",
    "        idxs = idxs[1:][overlap < overlap_thresh]\n",
    "    return boxes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef549aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_dot_template(image, min_area=20, max_area=300, patch_size=(24, 24), offset=5, size_tol=0.5):\n",
    "    \"\"\"\n",
    "    Extracts the dominant dot template from the image.\n",
    "    The function applies a series of image processing techniques to identify and extract the dot template.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        min_area: Minimum area of the dot to be considered (default is 20)\n",
    "        max_area: Maximum area of the dot to be considered (default is 300)\n",
    "        patch_size: Size of the patch to be extracted (default is (24, 24))\n",
    "        offset: Offset for bounding box around the detected dot (default is 5)\n",
    "        size_tol: Tolerance for size consistency (default is 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the extracted patch and contours of the detected dots.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no valid dot candidates are found or if no size-consistent patches are found.\n",
    "    \"\"\"\n",
    "    image_clean = cv2.bilateralFilter(image, d=15, sigmaColor=50, sigmaSpace=5)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image_clean = clahe.apply(image_clean)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "    tophat = cv2.morphologyEx(image_clean, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    _, binary_top = cv2.threshold(tophat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_top, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    candidates = []\n",
    "    sizes = []\n",
    "    img_w, img_h = image.shape\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            crop_x_start = x - offset\n",
    "            crop_x_end = x + w + offset\n",
    "            crop_y_start = y - offset\n",
    "            crop_y_end = y + h + offset\n",
    "\n",
    "            if crop_x_start < 0 or crop_x_end >= img_w or crop_y_start < 0 or crop_y_end >= img_h:\n",
    "                continue\n",
    "\n",
    "            patch = image[crop_y_start:crop_y_end, crop_x_start:crop_x_end]\n",
    "            candidates.append((patch, h, w))\n",
    "            sizes.append((h, w))\n",
    "\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No valid dot candidates found.\")\n",
    "\n",
    "    # Compute median size\n",
    "    heights = [s[0] for s in sizes]\n",
    "    widths = [s[1] for s in sizes]\n",
    "    median_area = np.median(heights) * np.median(widths)\n",
    "\n",
    "    # Keep only patches with similar size\n",
    "    patches_filtered = []\n",
    "    resized_for_matching = []\n",
    "    for (patch, h, w) in candidates:\n",
    "        # print(abs(h * w - median_area))\n",
    "        if abs(h * w - median_area) / median_area < size_tol:\n",
    "            patches_filtered.append(patch)\n",
    "            resized_for_matching.append(cv2.resize(patch, patch_size))\n",
    "\n",
    "    if not patches_filtered:\n",
    "        raise ValueError(\"No size-consistent patches found.\")\n",
    "\n",
    "    # Find patch closest to the median template\n",
    "    stack = np.stack(resized_for_matching, axis=0).astype(np.float32)\n",
    "    median_template = np.median(stack, axis=0)\n",
    "    diffs = [np.linalg.norm(p.astype(np.float32) - median_template) for p in resized_for_matching]\n",
    "    best_idx = np.argmin(diffs)\n",
    "\n",
    "    return patches_filtered[best_idx], contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b33432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_matching(reflectance, template, method, match_thresh=0.7, nms_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform template matching to find the best match for the template in the reflectance image.\n",
    "\n",
    "    Args:\n",
    "        reflectance: Input reflectance image (numpy array)\n",
    "        templates: Template image (numpy array)\n",
    "        method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        match_thresh: Threshold for template matching (default is 0.7)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "\n",
    "    Returns:\n",
    "        List of bounding boxes for the detected matches.\n",
    "    \"\"\"\n",
    "    # === Template matching ===\n",
    "    result = cv2.matchTemplate(reflectance, template, method)\n",
    "    locations = zip(*np.where(result >= match_thresh)[::-1])\n",
    "    scores = result[result >= match_thresh].flatten()\n",
    "\n",
    "    # === Bounding boxes (x, y, w, h) for each match ===\n",
    "    h, w = template.shape\n",
    "    boxes = [(int(x), int(y), w, h) for (x, y) in locations]\n",
    "\n",
    "    # === Apply NMS ===\n",
    "    nms_boxes = non_max_suppression_fast(boxes, scores, overlap_thresh=nms_thresh)\n",
    "\n",
    "    return nms_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours_from_patch(patch):\n",
    "    \"\"\"\n",
    "    Extracts contours from supplied patch image.\n",
    "    \"\"\"\n",
    "    _, binary_patch = cv2.threshold(patch, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_patch, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hu_descriptor(patch):\n",
    "    \"\"\"\n",
    "    Computes Hu moments for a given patch.\n",
    "\n",
    "    Args:\n",
    "        patch: Input patch (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Hu moments of the patch (numpy array)\n",
    "    \"\"\"\n",
    "    contours = contours_from_patch(patch)\n",
    "    if not contours:\n",
    "        return np.zeros(7)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    moments = cv2.moments(largest_contour)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    return -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)\n",
    "\n",
    "def select_diverse_templates(candidates, k):\n",
    "    \"\"\"\n",
    "    Selects k diverse templates from the candidates using greedy farthest-point sampling.\n",
    "\n",
    "    Args:\n",
    "        candidates: List of candidate patches (numpy arrays)\n",
    "        k: Number of templates to select\n",
    "    \n",
    "    Returns:\n",
    "        List of selected templates (numpy arrays)\n",
    "    \"\"\"\n",
    "    selected = [candidates[0]]\n",
    "    selected_ids = {id(candidates[0])}\n",
    "\n",
    "    while len(selected) < k and len(selected) < len(candidates):\n",
    "        remaining = [c for c in candidates if id(c) not in selected_ids]\n",
    "        if not remaining:\n",
    "            break\n",
    "        best = max(\n",
    "            remaining,\n",
    "            key=lambda c: min(np.linalg.norm(c[0] - s[0]) for s in selected)\n",
    "        )\n",
    "        selected.append(best)\n",
    "        selected_ids.add(id(best))\n",
    "\n",
    "    return [s[1] for s in selected] # return patches only\n",
    "\n",
    "def remove_outlier_boxes(boxes, eps=50, min_samples=3):\n",
    "    \"\"\"\n",
    "    Removes outlier boxes if they are too far from the main cluster of boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after removing outliers\n",
    "    \"\"\"\n",
    "    centers = np.column_stack((boxes[:, 0] + boxes[:, 2] / 2,\n",
    "                               boxes[:, 1] + boxes[:, 3] / 2))\n",
    "    \n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    # keep only boxes from the largest cluster (label >= 0)\n",
    "    valid_labels = labels[labels >= 0]\n",
    "    if len(valid_labels) == 0:\n",
    "        return boxes.tolist() # fallback: keep all\n",
    "    main_cluster = np.argmax(np.bincount(valid_labels))\n",
    "    keep_indices = np.where(labels == main_cluster)[0]\n",
    "\n",
    "    return boxes[keep_indices].tolist()\n",
    "\n",
    "def cascade_template_matching(reflectance, templates, method, match_thresh=0.7, nms_thresh=0.3, k_templates=4, debug=False):\n",
    "    \"\"\"\n",
    "    Performs template matching repeatedly using new matches as templates until enough matches found.\n",
    "\n",
    "    Args:\n",
    "        reflectance: Input reflectance image (numpy array)\n",
    "        templates: List of template images (numpy arrays)\n",
    "        method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        match_thresh: Threshold for template matching (default is 0.7)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "        debug: If True, display debug information (default is False)\n",
    "\n",
    "    Returns:\n",
    "        List of bounding boxes for the detected matches.\n",
    "    \"\"\"\n",
    "    # Initial template matchings\n",
    "    initial_boxes = []\n",
    "    for template in templates:\n",
    "        matches = template_matching(reflectance, template, method, match_thresh, nms_thresh)\n",
    "        initial_boxes.extend(matches)\n",
    "    if len(initial_boxes) == 0:\n",
    "        raise ValueError(\"No matches found in initial template matching.\")\n",
    "    \n",
    "    # === Apply NMS to the initial boxes ===\n",
    "    all_scores = [1] * len(initial_boxes) # optionally use actual scores\n",
    "    initial_boxes = np.array(initial_boxes)\n",
    "    initial_boxes = non_max_suppression_fast(initial_boxes, all_scores, overlap_thresh=nms_thresh)\n",
    "\n",
    "    h, w = template.shape\n",
    "    candidates = []\n",
    "    for box in initial_boxes:\n",
    "        x, y, bw, bh = box\n",
    "        patch = reflectance[y:y + bh, x:x + bw]\n",
    "        # resize patch to original template size\n",
    "        if patch.shape != template.shape:\n",
    "            patch = cv2.resize(patch, (w, h), interpolation=cv2.INTER_AREA)\n",
    "        descriptor = hu_descriptor(patch)\n",
    "        candidates.append((descriptor, patch, box))\n",
    "\n",
    "    if len(candidates) < k_templates:\n",
    "        k_templates = len(candidates)\n",
    "    \n",
    "    # select k diverse templates from the candidates\n",
    "    diverse_templates = select_diverse_templates(candidates, k_templates)\n",
    "\n",
    "    # accumulate boxes from new templates\n",
    "    all_boxes = list(initial_boxes)\n",
    "    for new_template in diverse_templates:\n",
    "        matches = template_matching(reflectance, new_template, method, match_thresh, nms_thresh)\n",
    "        all_boxes.extend(matches)\n",
    "    all_boxes = np.array(all_boxes)\n",
    "\n",
    "    # Apply NMS to the new boxes\n",
    "    all_scores = [1] * len(all_boxes) # optionally use actual scores\n",
    "    final_boxes = non_max_suppression_fast(all_boxes, all_scores, overlap_thresh=nms_thresh)\n",
    "\n",
    "    # Remove outlier boxes\n",
    "    final_boxes = remove_outlier_boxes(final_boxes)\n",
    "\n",
    "    return final_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f710876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Displays the numpy image using PIL and notebook display functionality.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        size: Size to which the image should be resized (default is (300, 300))\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, size)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a90c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_yucheng_methods(nms_boxes, reflectance, img, illumination, template_A, template_B):\n",
    "    \"\"\"\n",
    "    Displays the results of Yuchengs methods for dot detection and template matching.\n",
    "\n",
    "    Args:\n",
    "        nms_boxes: List of bounding boxes after non-maximum suppression\n",
    "        reflectance: Reflectance map (numpy array)\n",
    "        dot_contours: Contours of the detected dots\n",
    "        img: Original image (numpy array)\n",
    "        illumination: Estimated illumination (numpy array)\n",
    "        dot_template: Dot template (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # === Draw matching result ===\n",
    "    output = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h) in nms_boxes:\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # === Show results ===\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "    axs[0, 0].imshow(img, cmap='gray')\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(illumination, cmap='gray')\n",
    "    axs[0, 1].set_title(\"Estimated Illumination\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    axs[0, 2].imshow(reflectance, cmap='gray')\n",
    "    axs[0, 2].set_title(\"Reflectance Map (SSR)\")\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    axs[1, 0].imshow(cv2.cvtColor(template_A, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Template A\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(template_B, cmap='gray')\n",
    "    axs[1, 1].set_title(\"Template B\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    axs[1, 2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 2].set_title(\"Template matching\")\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb86f39",
   "metadata": {},
   "source": [
    "## Yucheng Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === load image ===\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === get yolo crop ===\n",
    "yolo_path = '../yolo/runs/obb/train4/weights/best.pt'\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad)\n",
    "\n",
    "# === get image and unet template ===\n",
    "unet_path = '../models/dot_detection/checkpoints/unet_best.pth'\n",
    "unet_model = load_unet(unet_path)\n",
    "reflectance, illumination, templates = unet_get_template(image_yolo, unet_model, k_templates=3)\n",
    "\n",
    "# === cascade template matching ===\n",
    "nms_boxes = cascade_template_matching(reflectance, templates, cv2.TM_CCOEFF_NORMED, match_thresh=0.7, nms_thresh=0.3, k_templates=3, debug=True)\n",
    "\n",
    "# === visualize results ===\n",
    "display_yucheng_methods(nms_boxes, reflectance, image_yolo.squeeze(0).permute(1, 2, 0).cpu().numpy(), illumination, templates[0], templates[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2725a5",
   "metadata": {},
   "source": [
    "# Grid Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc81c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(params, grid_size=16):\n",
    "    \"\"\"\n",
    "    Generates a grid of DMC points based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x0: X coordinate of the center of the grid\n",
    "        y0: Y coordinate of the center of the grid\n",
    "        sx: Scale factor in the X direction\n",
    "        sy: Scale factor in the Y direction\n",
    "        theta: Rotation angle in radians\n",
    "        grid_size: Size of the grid (default is 16)\n",
    "    \n",
    "    Returns:\n",
    "        array of grid points (x, y) in the original coordinate system\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = params\n",
    "\n",
    "    # force sx and sy to be minimum of 1.0 (to avoid zero size)\n",
    "    sx = max(sx, 1.0)\n",
    "    sy = max(sy, 1.0)\n",
    "\n",
    "    # building standard grid of DMC points (finder pattern + all inner points)\n",
    "    coords = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            # finder pattern specific points\n",
    "            if 0 in [i, j] or grid_size - 1 in [i, j]: # points on the finder pattern\n",
    "                if j == 0 and i in range(1, grid_size - 1, 2): # top timing pattern\n",
    "                    continue\n",
    "                elif i == grid_size - 1 and j in range(0, grid_size - 1, 2): # right timing pattern\n",
    "                    continue\n",
    "                else: # left and bottom finder pattern\n",
    "                    coords.append([i, j])\n",
    "            else: # inner points\n",
    "                coords.append([i, j])\n",
    "    coords = np.array(coords).astype(float)\n",
    "\n",
    "    # center the grid around (0, 0)\n",
    "    coords -= (grid_size - 1) / 2\n",
    "\n",
    "    # Convert to original coordinate system\n",
    "    coords = np.dot(coords, np.array([[sx, 0], [0, sy]])) # scale\n",
    "    coords = np.dot(coords, np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])) # rotate\n",
    "    coords += np.array([[x0, y0]]) # translate\n",
    "\n",
    "    return coords\n",
    "\n",
    "def inverse_grid_transform(grid_pts, params, grid_size=16):\n",
    "    \"\"\"\n",
    "    Inverse transformation of the grid points to the original coordinate system.\n",
    "\n",
    "    Args:\n",
    "        grid_pts: Grid points to be transformed (numpy array)\n",
    "        params: Parameters used for the transformation (x0, y0, sx, sy, theta)\n",
    "        grid_size: Size of the grid (default is 16)\n",
    "\n",
    "    Returns:\n",
    "        array of grid points (x, y) in the dmc coordinate system\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = params\n",
    "\n",
    "    # force sx and sy to be minimum of 1.0 (same as in generate_grid)\n",
    "    sx = max(sx, 1.0)\n",
    "    sy = max(sy, 1.0)\n",
    "\n",
    "    # undo translation\n",
    "    grid_pts = grid_pts.astype(float)\n",
    "    grid_pts -= np.array([[x0, y0]])\n",
    "\n",
    "    # undo rotation\n",
    "    rot_mat_inv = np.array([[np.cos(theta), np.sin(theta)],\n",
    "                            [-np.sin(theta), np.cos(theta)]])\n",
    "    grid_pts = np.dot(grid_pts, rot_mat_inv)\n",
    "\n",
    "    # undo scaling\n",
    "    grid_pts = np.dot(grid_pts, np.linalg.inv(np.diag([sx, sy])))\n",
    "\n",
    "    # convert to standard grid coordinates\n",
    "    grid_pts += (grid_size - 1) / 2\n",
    "\n",
    "    # round to nearest integer\n",
    "    grid_pts = np.rint(grid_pts).astype(int)\n",
    "\n",
    "    # clip to valid range (to avoid errors)\n",
    "    if np.any(grid_pts < 0) or np.any(grid_pts >= grid_size):\n",
    "        print(\"Warning: one or more grid points are out of bounds!\")\n",
    "        grid_pts = np.clip(grid_pts, 0, grid_size - 1)\n",
    "\n",
    "    return grid_pts\n",
    "\n",
    "\n",
    "def show_grid(img, grid_pts, title=\"Grid Points\"):\n",
    "    \"\"\"\n",
    "    Shows the grid points on the image.\n",
    "\n",
    "    Args:\n",
    "        img: Input image (numpy array)\n",
    "        grid_pts: Grid points to be displayed (numpy array)\n",
    "        title: Title of the plot (default is \"Grid Points\")\n",
    "    \"\"\"\n",
    "    img = img.copy() # to avoid modifying the original image\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # Convert grayscale to BGR\n",
    "    for p in grid_pts:\n",
    "        # cv2.circle(img, (int(p[0]), int(p[1])), 3, (0, 255, 0), -1)\n",
    "        cv2.circle(img, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1) # red color for grid points\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# === Example usage of grid generation ===\n",
    "img = reflectance\n",
    "\n",
    "# params to cover entire image\n",
    "img_size = img.shape[0]\n",
    "params = [\n",
    "    img_size / 2, # x0\n",
    "    img_size / 2, # y0\n",
    "    img_size / 15, # sx\n",
    "    img_size / 15, # sy\n",
    "    0.0 # theta\n",
    "]\n",
    "grid_pts = generate_grid(params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_grid_params(nms_boxes, debug=False):\n",
    "    \"\"\"\n",
    "    Estimates grid starting parameters using the observed points from the detected boxes.\n",
    "    The function uses the observed points to compute the initial parameters for the affine transformation.\n",
    "    \"\"\"\n",
    "    observed_pts = np.array([[x + w / 2, y + h / 2] for (x, y, w, h) in nms_boxes])\n",
    "\n",
    "    # estimating reasonable x, y based on the average of all points\n",
    "    x = np.mean(observed_pts[:, 0])\n",
    "    y = np.mean(observed_pts[:, 1])\n",
    "    if debug:\n",
    "        print(f\"Estimated x, y: ({x}, {y})\")\n",
    "\n",
    "    # getting mode distance between closest points for later usage\n",
    "    distances = []\n",
    "    for i in range(len(observed_pts)):\n",
    "        closest_pt = float('inf')\n",
    "        for j in range(i + 1, len(observed_pts)):\n",
    "            if i != j:\n",
    "                # find the closest point to i\n",
    "                dist = np.linalg.norm(observed_pts[i] - observed_pts[j])\n",
    "                if dist < closest_pt:\n",
    "                    closest_pt = dist\n",
    "        if closest_pt != float('inf'):\n",
    "            distances.append(np.round(closest_pt, 2))\n",
    "    # mode distance is used because we want the most common distance, not the average\n",
    "    mod_dist = get_mode(distances)\n",
    "    if debug:\n",
    "        print(f\"Modal distance between (close) points: {mod_dist}\")\n",
    "\n",
    "    # estimating sx & sy based on vectors of points and their closest \"L\" neighbors (points at distances of mod_dist or less)\n",
    "    # simultaneously estimating theta based on the angle between the two vectors\n",
    "    sxs = []\n",
    "    sys = []\n",
    "    thetas = []\n",
    "    for i in range(len(observed_pts)):\n",
    "        # find closest point to i\n",
    "        dist_a = float('inf')\n",
    "        a_idx = -1\n",
    "        for j in range(i + 1, len(observed_pts)):\n",
    "            dist = np.linalg.norm(observed_pts[i] - observed_pts[j])\n",
    "            if dist < dist_a:\n",
    "                dist_a = dist\n",
    "                a_idx = j\n",
    "                point_a = observed_pts[j]\n",
    "                vec_a = observed_pts[j] - observed_pts[i]\n",
    "        \n",
    "        # find next closest point to i that forms a tight enough \"L\" shape with regards to first point\n",
    "        # (i.e. the two points are orthogonal to each other from i)\n",
    "        point_b = None\n",
    "        for j in range(i + 1, len(observed_pts)):\n",
    "            if j == a_idx:\n",
    "                continue\n",
    "            \n",
    "            # check if vectors from point i to point_a and from i to point_b are orthogonal\n",
    "            vec_b_tmp = observed_pts[j] - observed_pts[i]\n",
    "            cos_angle = np.dot(vec_a, vec_b_tmp) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b_tmp) + 1e-8)\n",
    "            angle_deg = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "            if 80 <= angle_deg <= 100:\n",
    "                # points i, a_idx, and j form an \"L\" shape!\n",
    "                # now to make sure both points are close enough to i\n",
    "                dist_b = np.linalg.norm(vec_b_tmp)\n",
    "                if dist_a <= mod_dist*2 and dist_b <= mod_dist*2:\n",
    "                    # point_b is close enough to i!\n",
    "                    point_b = observed_pts[j]\n",
    "                    vec_b = vec_b_tmp\n",
    "                    # if debug:\n",
    "                        # print(f\"Found L shape: {observed_pts[i]} -> {point_a} -> {point_b}\")\n",
    "                        # print(f\"Vectors: {vec_a}, {vec_b}\")\n",
    "                        # print(f\"Distances: {np.linalg.norm(vec_a)}, {np.linalg.norm(vec_b)}\")\n",
    "        \n",
    "        # if we found a point that is orthogonal to point_a, we can use it to estimate sx and sy\n",
    "        if point_a is not None and point_b is not None:\n",
    "            # stricter dist check for sx and sy than theta\n",
    "            # we can use the distance between the points to estimate sx and sy\n",
    "            if dist_a <= mod_dist and dist_b <= mod_dist:\n",
    "                sx_a = np.linalg.norm(vec_a)\n",
    "                sx_b = np.linalg.norm(vec_b)\n",
    "                \n",
    "                # round to 2 decimal places for consistent mode calculation\n",
    "                sx_a = np.round(sx_a, 2)\n",
    "                sx_b = np.round(sx_b, 2)\n",
    "\n",
    "                # weird case where numpy differentiates between -0.0 and 0.0\n",
    "                if sx_a == 0:\n",
    "                    sx_a = 0\n",
    "                if sx_b == 0:\n",
    "                    sx_b = 0\n",
    "\n",
    "                sxs.append(sx_a)\n",
    "                sys.append(sx_b)\n",
    "                \n",
    "                # sxs.append(np.linalg.norm(vec_a))\n",
    "                # sys.append(np.linalg.norm(vec_b))\n",
    "            # we can also compare angle between the two vectors and default vector (0, 1)\n",
    "            theta_a = np.arctan2(vec_a[1], vec_a[0]) - np.arctan2(0, 1)\n",
    "            theta_b = np.arctan2(vec_b[1], vec_b[0]) - np.arctan2(0, 1)\n",
    "            # for consistency, we want to use the angle of the vector that is closest to the default vector (0, 1)\n",
    "            if abs(theta_a) < abs(theta_b):\n",
    "                theta = theta_a\n",
    "            else:\n",
    "                theta = theta_b\n",
    "            theta = np.round(theta, 2) # round to 2 decimal places for consistent mode calculation\n",
    "            if theta == 0: # weird case where numpy differentiates between -0.0 and 0.0\n",
    "                theta = 0\n",
    "            thetas.append(-theta) # flipped angle\n",
    "            # if debug:\n",
    "            #     print(f\"Angle: {theta}\")\n",
    "\n",
    "                # plot found L shapes\n",
    "                # plt.plot([observed_pts[i][0], point_a[0]], [observed_pts[i][1], point_a[1]], 'r-')\n",
    "                # plt.plot([observed_pts[i][0], point_b[0]], [observed_pts[i][1], point_b[1]], 'r-')\n",
    "                # plt.plot([point_a[0], point_b[0]], [point_a[1], point_b[1]], 'r-')\n",
    "                # plt.scatter(observed_pts[i][0], observed_pts[i][1], color='blue')\n",
    "                # plt.scatter(point_a[0], point_a[1], color='green')\n",
    "                # plt.scatter(point_b[0], point_b[1], color='green')\n",
    "                # plt.xlim(0, img.shape[1])\n",
    "                # plt.ylim(img.shape[0], 0)\n",
    "                # plt.title(\"L shape\")\n",
    "                # plt.show()\n",
    "    # take the mode of the distances to estimate sx and sy. if no L shapes are found, use the overall modal distance\n",
    "    if len(sxs) > 0:\n",
    "        sx = get_mode(sxs)\n",
    "        sy = get_mode(sys)\n",
    "    else:\n",
    "        sx = mod_dist\n",
    "        sy = mod_dist\n",
    "        theta = 0\n",
    "    if debug:\n",
    "        print(f\"Estimated sx & sy: {sx}, {sy}\")\n",
    "\n",
    "    theta = get_mode(thetas) # use mode for theta to avoid outliers\n",
    "    if debug:\n",
    "        print(f\"Estimated theta: {theta}\")\n",
    "\n",
    "    init_params = [x, y, sx, sy, theta]\n",
    "    if debug:\n",
    "        print(f\"Initial parameters: {init_params}\")\n",
    "\n",
    "    return init_params, observed_pts\n",
    "\n",
    "# === Example usage of grid parameter estimation ===\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, debug=True)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d78d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convex_hull(observed_points, grid_points=None):\n",
    "    hull = ConvexHull(observed_points)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(observed_points[:, 0], observed_points[:, 1], 'o', label='Observed Points')\n",
    "\n",
    "    # Draw convex hull edges\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(observed_points[simplex, 0], observed_points[simplex, 1], 'k-')\n",
    "\n",
    "    # Optionally plot grid points\n",
    "    if grid_points is not None:\n",
    "        plt.plot(grid_points[:, 0], grid_points[:, 1], 'rx', label='Grid Points')\n",
    "\n",
    "    plt.title(\"Observed Points and Convex Hull\")\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ed101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, observed_points, N, alpha):\n",
    "    \"\"\"\n",
    "    Cost function for optimization. It computes the mean squared distance between observed points and grid points.\n",
    "    The loss function is robust to outliers by focusing on the N closest points.\n",
    "\n",
    "    Args:\n",
    "        params: Parameters for grid (x0, y0, sx, sy, theta)\n",
    "        observed_points: Observed points (numpy array)\n",
    "        N: Number of closest points to consider for cost function\n",
    "        alpha: Weighting factor for the cost function\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    grid_points = generate_grid(params)\n",
    "\n",
    "    center = np.mean(observed_points, axis=0)\n",
    "    distances_from_center = np.linalg.norm(observed_points - center, axis=1)\n",
    "    \n",
    "    # compute weights based on distance from center\n",
    "    mean_distance = np.mean(distances_from_center)\n",
    "    normalized_distances = distances_from_center / mean_distance\n",
    "    weights = normalized_distances ** alpha\n",
    "\n",
    "    # normalize weights to have mean 1\n",
    "    weights = weights / np.mean(weights)\n",
    "\n",
    "    all_N_closest_dists = []\n",
    "    for i, obs_point in enumerate(observed_points):\n",
    "        # finding N closest dists to point\n",
    "        dists = np.linalg.norm(obs_point - grid_points, axis=1)\n",
    "        sorted_dists = np.sort(dists)\n",
    "        closest_dists = sorted_dists[:N]\n",
    "\n",
    "        # \n",
    "        weighted_squared_dist = weights[i] * (closest_dists ** 2)\n",
    "        all_N_closest_dists.extend(weighted_squared_dist)\n",
    "\n",
    "    cost_ = np.mean(all_N_closest_dists)\n",
    "\n",
    "    return cost_\n",
    "\n",
    "# === Example usage of cost function ===\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "N = 1000 # number of closest points to consider for cost function (higher = better, but slower)\n",
    "alpha = 1.0\n",
    "\n",
    "# cost of grid covering entire image\n",
    "params = [\n",
    "    img.shape[0] / 2, # x0\n",
    "    img.shape[0] / 2, # y0\n",
    "    img.shape[0] / 15, # sx\n",
    "    img.shape[0] / 15, # sy\n",
    "    0.0 # theta\n",
    "]\n",
    "grid_pts = generate_grid(params)\n",
    "show_grid(img, grid_pts)\n",
    "observed_pts = np.array([[x + w / 2, y + h / 2] for (x, y, w, h) in nms_boxes])\n",
    "cost_value = cost(params, observed_pts, N, alpha)\n",
    "print(f\"Cost value (naive start): {cost_value}\")\n",
    "\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, debug=False)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)\n",
    "cost_value = cost(init_params, observed_pts, N, alpha)\n",
    "print(f\"Cost value (estimated start): {cost_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33131f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_xy(xy, params, observed_pts, N, alpha):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the xy coordinates and the rest of the parameters separately.\n",
    "    Used for optimizing x0 and y0.\n",
    "\n",
    "    Args:\n",
    "        xy: X and Y coordinates of the center of the grid\n",
    "        params: Parameters for grid (sx, sy, theta)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    x0, y0 = xy\n",
    "    sx, sy, theta = params\n",
    "    return cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)\n",
    "\n",
    "def cost_sx_sy(sx_sy, params, observed_pts, N, alpha):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the sx and sy coordinates and the rest of the parameters separately.\n",
    "    Used for optimizing sx and sy.\n",
    "\n",
    "    Args:\n",
    "        sx_sy: Scale factors in the X and Y directions\n",
    "        params: Parameters for grid (x0, y0, theta)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    sx, sy = sx_sy\n",
    "    x0, y0, theta = params\n",
    "    return cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)\n",
    "\n",
    "def cost_theta(theta, params, observed_pts, N, alpha):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the theta coordinate and the rest of the parameters separately.\n",
    "    Used for optimizing theta.\n",
    "\n",
    "    Args:\n",
    "        theta: Rotation angle in radians\n",
    "        params: Parameters for grid (x0, y0, sx, sy)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    theta = theta[0]  # Extract the single value from the array\n",
    "    x0, y0, sx, sy = params\n",
    "    return cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_grid(init_params, observed_pts, img, N, alpha, debug=False):\n",
    "    \"\"\"\n",
    "    Estimates the grid parameters using optimization. The function optimizes the parameters\n",
    "    (x0, y0, sx, sy, theta) to minimize the cost function based on the observed points.\n",
    "\n",
    "    The optimization is done in the following order:\n",
    "    1. Optimize x0 and y0\n",
    "    2. Optimize sx and sy\n",
    "    3. Optimize theta\n",
    "    4. Optimize all parameters together\n",
    "\n",
    "    Args:\n",
    "        init_params: Initial parameters for the grid (x0, y0, sx, sy, theta)\n",
    "        observed_pts: Observed points (numpy array)\n",
    "        img: Input image (numpy array)\n",
    "        N: Number of closest points to consider for cost function\n",
    "        alpha: Weighting factor for the cost function\n",
    "        debug: If True, display debug information (default is False)\n",
    "    \n",
    "    Returns:\n",
    "        Optimized parameters (x0, y0, sx, sy, theta) and the observed points.\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = init_params\n",
    "\n",
    "    # 0. check for better initial theta\n",
    "    # trying other 3 90 degree angles to see if they are better\n",
    "    # lowest_cost = cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)\n",
    "    # for i in range(1, 4):\n",
    "    #     theta_tmp = (theta + i * np.pi / 2) % (2 * np.pi)\n",
    "    #     theta_tmp_cost = cost([x0, y0, sx, sy, theta_tmp], observed_pts, N, alpha)\n",
    "    #     if theta_tmp_cost < lowest_cost:\n",
    "    #         lowest_cost = theta_tmp_cost\n",
    "    #         theta = theta_tmp\n",
    "    # if debug:\n",
    "    #     print(f\"Better initial theta found: {theta}\")\n",
    "    #     print(f\"Cost value: {cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)}\")\n",
    "    #     show_grid(img, generate_grid([x0, y0, sx, sy, theta]), title=\"Initial Estimate\")\n",
    "\n",
    "    # 1. optimize for x0, y0 only\n",
    "    result = minimize(cost_xy, [x0, y0], args=([sx, sy, theta], observed_pts, N, alpha), method='Powell')\n",
    "    x0, y0 = result.x\n",
    "    if debug:\n",
    "        print(f\"Optimized x0, y0: {x0}, {y0}\")\n",
    "        print(f\"Cost value: {cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)}\")\n",
    "        show_grid(img, generate_grid([x0, y0, sx, sy, theta]), title=\"Optimized x0, y0\")\n",
    "    \n",
    "    # 2. optimize for sx, sy only\n",
    "    result = minimize(cost_sx_sy, [sx, sy], args=([x0, y0, theta], observed_pts, N, alpha), method='Powell')\n",
    "    sx, sy = result.x\n",
    "    if debug:\n",
    "        print(f\"Optimized sx, sy: {sx}, {sy}\")\n",
    "        print(f\"Cost value: {cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)}\")\n",
    "        show_grid(img, generate_grid([x0, y0, sx, sy, theta]), title=\"Optimized sx, sy\")\n",
    "\n",
    "    # 3. optimize for theta only\n",
    "    result = minimize(cost_theta, [theta], args=([x0, y0, sx, sy], observed_pts, N, alpha), method='Powell')\n",
    "    theta = result.x[0]\n",
    "    # trying other 3 90 degree angles to see if they are better\n",
    "    lowest_cost = cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)\n",
    "    for i in range(1, 4):\n",
    "        theta_tmp = (theta + i * np.pi / 2) % (2 * np.pi)\n",
    "        theta_tmp_cost = cost([x0, y0, sx, sy, theta_tmp], observed_pts, N, alpha)\n",
    "        if theta_tmp_cost < lowest_cost:\n",
    "            if debug:\n",
    "                print(f\"Found better theta: {theta_tmp} with cost: {theta_tmp_cost}. (previous: {theta} with cost: {lowest_cost})\")\n",
    "            lowest_cost = theta_tmp_cost\n",
    "            theta = theta_tmp\n",
    "    if debug:\n",
    "        print(f\"Optimized theta: {theta}\")\n",
    "        print(f\"Cost value: {cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)}\")\n",
    "        show_grid(img, generate_grid([x0, y0, sx, sy, theta]), title=\"Optimized theta\")\n",
    "\n",
    "    # 4. optimize for all parameters together\n",
    "    result = minimize(cost, [x0, y0, sx, sy, theta], args=(observed_pts, N, alpha), method='Powell')\n",
    "    x0, y0, sx, sy, theta = result.x\n",
    "    if debug:\n",
    "        print(f\"Optimized all: {x0}, {y0}, {sx}, {sy}, {theta}\")\n",
    "        print(f\"Cost value: {cost([x0, y0, sx, sy, theta], observed_pts, N, alpha)}\")\n",
    "        show_grid(img, generate_grid([x0, y0, sx, sy, theta]), title=\"Optimized all\")\n",
    "\n",
    "    return [x0, y0, sx, sy, theta], observed_pts\n",
    "\n",
    "# === Example usage of grid estimation ===\n",
    "N = 1\n",
    "alpha = 5.0 # weight for cost function\n",
    "\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, debug=False)\n",
    "print(f\"Initial parameters: {init_params}\")\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)\n",
    "opt_params, observed_pts = estimate_grid(init_params, observed_pts, img, N, alpha, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449422b",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_observed_to_grid(observed_pts, grid_pts):\n",
    "    \"\"\"\n",
    "    Maps observed points to grid points using the Hungarian algorithm to ensure 1-1 mapping.\n",
    "\n",
    "    Args:\n",
    "        observed_pts: Observed points (numpy array)\n",
    "        grid_pts: Grid points (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Mapped grid points (numpy array)\n",
    "    \"\"\"\n",
    "    cost_matrix = cdist(observed_pts, grid_pts, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    mapped_grid_pts = grid_pts[col_ind]\n",
    "    return mapped_grid_pts\n",
    "\n",
    "# === Example usage of mapping observed points to grid points ===\n",
    "# img = cv2.imread(img_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "# img = cv2.resize(img, (320, 320))\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, debug=False)\n",
    "grid_pts = generate_grid(init_params)\n",
    "mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "show_grid(img, mapped_grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92542d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dmc_matrix(dmc_pts, grid_size=16):\n",
    "    \"\"\"\n",
    "    Converts DMC points to a matrix representation.\n",
    "\n",
    "    Args:\n",
    "        dmc_pts: DMC points (numpy array)\n",
    "        grid_size: Size of the grid (default is 16)\n",
    "    \n",
    "    Returns:\n",
    "        DMC matrix (numpy array)\n",
    "    \"\"\"\n",
    "    dmc_matrix = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    for pt in dmc_pts:\n",
    "        x, y = pt\n",
    "        dmc_matrix[y, x] = 1\n",
    "    return dmc_matrix\n",
    "\n",
    "# === Example usage of mapping to DMC points ===\n",
    "# img = cv2.imread(img_to_test, cv2.IMREAD_GRAYSCALE)\n",
    "# img = cv2.resize(img, (320, 320))\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, debug=False)\n",
    "grid_pts = generate_grid(init_params)\n",
    "mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "dmc_pts = inverse_grid_transform(mapped_grid_pts, init_params)\n",
    "dmc_matrix = to_dmc_matrix(dmc_pts)\n",
    "print(\"DMC matrix:\")\n",
    "print(dmc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_DMC(matrix, title=\"DMC Matrix\"):\n",
    "    \"\"\"\n",
    "    Display the DMC matrix like normal DMC.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "        title: Title of the plot (default is \"DMC Matrix\")\n",
    "    \"\"\"\n",
    "    # Invert the matrix for display\n",
    "    matrix = np.invert(matrix)\n",
    "    plt.imshow(matrix, cmap='gray', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea99af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_DMC(matrix):\n",
    "    \"\"\"\n",
    "    Decodes the DMC matrix using pylibdmtx.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "    \n",
    "    Returns:\n",
    "        Decoded string if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    # Converting binary matrix to uint8 image\n",
    "    image = np.zeros((matrix.shape[0], matrix.shape[1]), dtype=np.uint8)\n",
    "    image[matrix == 1] = 255\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Inverting the image for decoding\n",
    "    image = Image.eval(image, lambda x: 255 - x)\n",
    "\n",
    "    # Padding the image by 2 pixels to add margin larger than a DMC module (https://www.keyence.eu/ss/products/auto_id/codereader/basic_2d/datamatrix.jsp)\n",
    "    image = np.pad(np.array(image), ((2, 2), (2, 2)), mode='constant', constant_values=255)\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Resizing to larger image for better decoding\n",
    "    image = image.resize((image.size[0] * 10, image.size[1] * 10), Image.NEAREST)\n",
    "\n",
    "    # Decode using pylibdmtx\n",
    "    decoded = decode(image)\n",
    "    if decoded:\n",
    "        return decoded[0].data.decode('utf-8')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d695c8c",
   "metadata": {},
   "source": [
    "# Example of Grid Fitting Followed by Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b12085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Estimate initial grid parameters ===\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optimize grid parameters ===\n",
    "opt_params, observed_pts = estimate_grid(init_params, observed_pts, img, N, alpha)\n",
    "grid_pts = generate_grid(opt_params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mapping observed points to grid points ===\n",
    "mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "show_grid(img, mapped_grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Convert to DMC matrix ===\n",
    "dmc_pts = inverse_grid_transform(mapped_grid_pts, opt_params)\n",
    "dmc_matrix = to_dmc_matrix(dmc_pts)\n",
    "display_DMC(dmc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decoding DMC ===\n",
    "decoded_data = decode_DMC(dmc_matrix)\n",
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff2487",
   "metadata": {},
   "source": [
    "# Full Decoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7262d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pipeline(image_path, yolo_path, UNet_path, yolo_pad, tm_method=cv2.TM_CCOEFF_NORMED, tmm_thresh=0.7, nms_thresh=0.3, k_templates=3, N=1, alpha=4.0, debug=False, rotation=None):\n",
    "    \"\"\"\n",
    "    Performs the entire decoding pipeline on the input image and template.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        template_path: Path to the template image\n",
    "        tm_method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        tmm_thresh: Threshold for template matching (default is 0.7)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "        N: Number of closest points to consider for cost function (default is 1)\n",
    "        alpha: Weighting factor for the cost function (default is 4.0)\n",
    "        debug: Flag for debugging (default is False)\n",
    "        rotation: Rotation angle in degrees (default is None, no rotation)\n",
    "    \n",
    "    Returns:\n",
    "        Decoded data from the DMC matrix or None if decoding fails.\n",
    "    \"\"\"\n",
    "    # === Load image ===\n",
    "    img = load_image_to_device(image_path)\n",
    "\n",
    "    # === Rotation ===\n",
    "    if rotation is not None:\n",
    "        _, _, h, w = img.shape\n",
    "        center = (w / 2, h / 2)\n",
    "\n",
    "        # rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(center, rotation, 1)\n",
    "\n",
    "        # getting size of new box to avoid cutting off corners\n",
    "        cos = np.abs(M[0, 0])\n",
    "        sin = np.abs(M[0, 1])\n",
    "        new_w = int(h * sin + w * cos)\n",
    "        new_h = int(h * cos + w * sin)\n",
    "\n",
    "        # adjusting the rotation matrix to take into account translation\n",
    "        M[0, 2] += new_w / 2 - center[0]\n",
    "        M[1, 2] += new_h / 2 - center[1]\n",
    "\n",
    "        # temporarily convert tensor to cv2 image\n",
    "        img = img.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 3)\n",
    "\n",
    "        # rotating with new bounds\n",
    "        img = cv2.warpAffine(img, M, (new_w, new_h))\n",
    "\n",
    "        if debug:\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Rotated Image\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "        # converting back to tensor\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # === YOLO crop ===\n",
    "    yolo_model = load_yolo(yolo_path)\n",
    "    img_yolo = yolo_detect_and_crop(img, yolo_model, yolo_pad, debug=True)\n",
    "\n",
    "    # === UNet template ===\n",
    "    unet_model = load_unet(UNet_path)\n",
    "    reflectance, illumination, templates = unet_get_template(img_yolo, unet_model, k_templates, debug=True)\n",
    "\n",
    "    # === Template matching ===\n",
    "    nms_boxes = cascade_template_matching(reflectance, templates, method=tm_method, match_thresh=tmm_thresh, nms_thresh=nms_thresh, k_templates=k_templates, debug=debug)\n",
    "\n",
    "    if debug:\n",
    "        display_yucheng_methods(nms_boxes, reflectance, image_yolo.squeeze(0).permute(1, 2, 0).cpu().numpy(), illumination, templates[0], templates[1])\n",
    "\n",
    "    # === Estimate initial grid parameters ===\n",
    "    init_params, observed_pts = estimate_grid_params(nms_boxes, debug)\n",
    "    if debug:\n",
    "        grid_pts = generate_grid(init_params)\n",
    "        show_grid(reflectance, grid_pts, title=\"Initial Estimate\")\n",
    "\n",
    "    # === Optimizing Grid Parameters ===\n",
    "    opt_params, observed_pts = estimate_grid(init_params, observed_pts, reflectance, N, alpha, debug)\n",
    "    grid_pts = generate_grid(opt_params)\n",
    "\n",
    "    # === Mapping observed points to grid points ===\n",
    "    mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "    if debug:\n",
    "        print(f\"Mapped grid points\")\n",
    "        show_grid(reflectance, mapped_grid_pts, title=\"Mapped\")\n",
    "\n",
    "    # === Convert to DMC matrix ===\n",
    "    dmc_pts = inverse_grid_transform(mapped_grid_pts, opt_params)\n",
    "    dmc_matrix = to_dmc_matrix(dmc_pts)\n",
    "    if debug:\n",
    "        display_DMC(dmc_matrix, title=\"DMC Matrix pre fix\")\n",
    "\n",
    "    # === Extra fix for finder patter ===\n",
    "    dmc_matrix[:, 0] = 1  # left finder pattern\n",
    "    dmc_matrix[-1, :] = 1 # bottom finder pattern\n",
    "    for i in range(0, 16, 2):\n",
    "        dmc_matrix[0, i] = 1  # top finder pattern 1s\n",
    "        dmc_matrix[i, -1] = 0 # right finder pattern 0s\n",
    "    for i in range(1, 16, 2):\n",
    "        dmc_matrix[0, i] = 0  # top finder pattern 0s\n",
    "        dmc_matrix[i, -1] = 1 # right finder pattern 1s\n",
    "    if debug:\n",
    "        display_DMC(dmc_matrix, title=\"DMC Matrix post fix\")\n",
    "\n",
    "    # === Decoding DMC ===\n",
    "    decoded_data = decode_DMC(dmc_matrix)\n",
    "    if debug:\n",
    "        print(f\"Decoded data: {decoded_data}\")\n",
    "\n",
    "    return decoded_data\n",
    "\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "yolo_path = '../yolo/runs/obb/train4/weights/best.pt'\n",
    "unet_path = '../models/dot_detection/checkpoints/unet_best.pth'\n",
    "\n",
    "yolo_pad = 0.025 # % to pad the yolo crop by\n",
    "\n",
    "tm_method = cv2.TM_CCOEFF_NORMED # method for template matching\n",
    "tmm_thresh = 0.7 # threshold for template matching (higher = more strict matching)\n",
    "nms_thresh = 0.3 # threshold for non-maximum suppression (higher = more overlap)\n",
    "k_templates = 3  # number of templates to use for cascade template matching\n",
    "\n",
    "N = 1            # number of closest points to consider for cost function\n",
    "alpha = 4        # weight for cost function (higher = more weight on points away from center)\n",
    "\n",
    "decoded_data = decode_pipeline(img_to_test, yolo_path, unet_path, yolo_pad, tm_method, tmm_thresh, nms_thresh, k_templates, N, alpha, debug=True, rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
