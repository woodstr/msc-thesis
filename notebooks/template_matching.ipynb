{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159a45fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# third-party imports\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize, linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import ConvexHull\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "from pylibdmtx.pylibdmtx import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dd122",
   "metadata": {},
   "source": [
    "## Simple Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple helper functions\n",
    "def get_mode(lst):\n",
    "    \"\"\"Get the mode of a list.\"\"\"\n",
    "    return Counter(lst).most_common(1)[0][0]\n",
    "\n",
    "def load_image_to_device(image_path):\n",
    "    \"\"\"Load an image to the device.\"\"\"\n",
    "    image = read_image(image_path).to(device) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # if image has 4 channels, convert to 3 channels\n",
    "    if image.shape[0] == 4:\n",
    "        image = image[:3, :, :]\n",
    "\n",
    "    return image.unsqueeze(0) # Add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fbfee",
   "metadata": {},
   "source": [
    "# YOLO Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo(model_path):\n",
    "    \"\"\"\n",
    "    Load the YOLOv11 model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the YOLOv11 model file.\n",
    "\n",
    "    Returns:\n",
    "        YOLO: The loaded YOLOv11 model.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    model.fuse()  # fuse model for faster inference\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def yolo_detect(image, model, debug=False):\n",
    "    \"\"\"\n",
    "    Detect object with highest confidence in an image using YOLOv11.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        model (YOLO): The YOLOv11 model.\n",
    "        debug (bool): If True, display the image with bounding box.\n",
    "\n",
    "    Returns:\n",
    "        list: xywh bounding box for object detected in the image.\n",
    "    \"\"\"\n",
    "    results = model.predict(image, verbose=False)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"YOLO did not detect any objects.\")\n",
    "        return None\n",
    "\n",
    "    result = results[0]\n",
    "    if len(result.boxes.xywh) == 0:\n",
    "        print(\"YOLO did not detect any objects.\")\n",
    "        return None\n",
    "\n",
    "    if debug:\n",
    "        result.save(filename='deleteme.jpg')\n",
    "\n",
    "    return result.boxes.xywh[0]\n",
    "\n",
    "def yolo_crop(image, xywh, pad):\n",
    "    \"\"\"\n",
    "    Crop the image using YOLOv11 bounding box.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        xywh (torch.Tensor): xywh bounding box for object detected in the image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The cropped image.\n",
    "    \"\"\"\n",
    "    x, y, w, h = xywh\n",
    "    x, y, w, h = float(x), float(y), float(w), float(h)\n",
    "\n",
    "    # padding by pad% of longest side\n",
    "    pad = max(image.shape[2:]) * pad\n",
    "    w_crop = int(w + 2 * pad)\n",
    "    h_crop = int(h + 2 * pad)\n",
    "\n",
    "    # image dimensions\n",
    "    _, _, H, W = image.shape\n",
    "\n",
    "    # crop to the bounding box\n",
    "    x1 = int(max(0, x - w_crop / 2))\n",
    "    y1 = int(max(0, y - h_crop / 2))\n",
    "    x2 = int(min(W, x + w_crop / 2))\n",
    "    y2 = int(min(H, y + h_crop / 2))\n",
    "    cropped_image = image[:, :, y1:y2, x1:x2]\n",
    "    cropped_image = F.interpolate(cropped_image, size=(h_crop, w_crop), mode='bilinear', align_corners=False)\n",
    "    cropped_image = torch.clamp(cropped_image, 0, 1)  # Ensure values are in [0, 1]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def yolo_detect_and_crop(image, model, pad, debug=False):\n",
    "    \"\"\"\n",
    "    Detects an crops down to an object in the image using YOLOv11.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image as a tensor.\n",
    "        model (YOLO): The YOLOv11 model.\n",
    "        debug (bool): If True, display the cropped image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The cropped image.\n",
    "    \"\"\"\n",
    "    image_yolo = transforms.Resize((640, 640))(image.clone()) # resize image to 640x640\n",
    "\n",
    "    # sometimes resize creates values above 1.0, so clamp to [0, 1]\n",
    "    image_yolo = torch.clamp(image_yolo, 0, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xywh = yolo_detect(image_yolo, model, debug)\n",
    "\n",
    "    if xywh is None:\n",
    "        print('WARNING, no object detected, returning original image')\n",
    "        return image # return original image if no object detected\n",
    "\n",
    "    # scale detection back to original image size\n",
    "    _, _, H_orig, W_orig = image.shape\n",
    "\n",
    "    scale_x = W_orig / 640\n",
    "    scale_y = H_orig / 640\n",
    "\n",
    "    x, y, w, h = [float(v) for v in xywh]\n",
    "    x *= scale_x\n",
    "    y *= scale_y\n",
    "    w *= scale_x\n",
    "    h *= scale_y\n",
    "    xywh_scaled = torch.tensor([x, y, w, h], dtype=torch.float32, device=image.device)\n",
    "\n",
    "    image_cropped = yolo_crop(image, xywh_scaled, pad)\n",
    "\n",
    "    if debug:\n",
    "        figure, axis = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axis[0].imshow(image.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "        axis[0].axis('off')\n",
    "        axis[0].set_title('Original Image')\n",
    "\n",
    "        yolo_detection = Image.open('deleteme.jpg')\n",
    "        axis[1].imshow(yolo_detection)\n",
    "        axis[1].axis('off')\n",
    "        axis[1].set_title('YOLO Detection')\n",
    "        os.remove('deleteme.jpg')\n",
    "\n",
    "        axis[2].imshow(image_cropped.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "        axis[2].axis('off')\n",
    "        axis[2].set_title('YOLO Crop (Post Padding)')\n",
    "        plt.show()\n",
    "\n",
    "    return image_cropped\n",
    "\n",
    "# === load image ===\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === load & run YOLOv11 ===\n",
    "yolo_path = '../yolo/runs/obb/train7/weights/best.pt' # train4 is original oriented YOLO no rotation lock\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48a00f",
   "metadata": {},
   "source": [
    "# Template Acquiring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b986",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf394224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv → BN → ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling → concat → double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # pad if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_c=64):\n",
    "        super().__init__()\n",
    "        self.in_conv = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c, base_c * 2)\n",
    "        self.down2 = Down(base_c * 2, base_c * 4)\n",
    "        self.down3 = Down(base_c * 4, base_c * 8)\n",
    "        self.down4 = Down(base_c * 8, base_c * 8)\n",
    "\n",
    "        self.up1 = Up(base_c * 16, base_c * 4)\n",
    "        self.up2 = Up(base_c * 8, base_c * 2)\n",
    "        self.up3 = Up(base_c * 4, base_c)\n",
    "        self.up4 = Up(base_c * 2, base_c)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_c, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return torch.sigmoid(self.out_conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e1928",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_retinex_reflectance_torch(img_tensor, sigma=30):\n",
    "    \"\"\"Reflectance extraction using Retinex algorithm.\"\"\"\n",
    "    eps = 1e-6\n",
    "    img = img_tensor.clamp(min=eps) # avoids log(0) without shifting scale\n",
    "    log_img = torch.log(img)\n",
    "\n",
    "    def get_gaussian_kernel2d(kernel_size, sigma):\n",
    "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.).to(img_tensor.device)\n",
    "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "        return kernel\n",
    "    \n",
    "    # approximate kernel size from sigma\n",
    "    kernel_size = int(2 * math.ceil(3 * sigma) + 1)\n",
    "    kernel = get_gaussian_kernel2d(kernel_size, sigma)\n",
    "    kernel = kernel.to(img_tensor.device).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "\n",
    "    # apply gaussian blur to each channel\n",
    "    channels = img_tensor.shape[1]\n",
    "    kernel = kernel.expand(channels, 1, -1, -1) # [C, 1, kH, kW]\n",
    "    blurred = F.conv2d(img_tensor, kernel, padding=kernel_size//2, groups=channels)\n",
    "    blurred = blurred.clamp(min=eps) # avoids log(0) without shifting scale\n",
    "    log_blur = torch.log(blurred)\n",
    "\n",
    "    reflectance = log_img - log_blur\n",
    "\n",
    "    def normalize(tensor):\n",
    "        return (tensor - tensor.amin(dim=(1,2,3), keepdim=True)) / (tensor.amax(dim=(1,2,3), keepdim=True) + eps)\n",
    "\n",
    "    # normalize to [0, 1]\n",
    "    reflectance = normalize(reflectance)\n",
    "    illumination = normalize(log_blur)\n",
    "\n",
    "    return reflectance, illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unet(model_path):\n",
    "    \"\"\"\n",
    "    Load the UNet model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the UNet model file.\n",
    "\n",
    "    Returns:\n",
    "        UNet: The loaded UNet model.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# === Watershed-based template extraction ===\n",
    "def extract_templates_watershed(heatmap_np, reflectance_np, k_templates, debug=False):\n",
    "    templates = []\n",
    "    template_bounds = []\n",
    "\n",
    "    # 1. Threshold to create binary mask\n",
    "    thresh_val = np.max(heatmap_np) * 0.1\n",
    "    binary = (heatmap_np >= thresh_val).astype(np.uint8)\n",
    "\n",
    "    # 2. Distance transform\n",
    "    distance = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n",
    "\n",
    "    # 3. Local maxima as markers\n",
    "    coords = peak_local_max(distance, min_distance=10, labels=binary) # can add num_peaks limit\n",
    "    mask = np.zeros(distance.shape, dtype=bool)\n",
    "    mask[tuple(coords.T)] = True\n",
    "    markers, _ = ndi.label(mask)\n",
    "\n",
    "    # 4. Watershed segmentation\n",
    "    labels = watershed(-distance, markers, mask=binary)\n",
    "\n",
    "    # 5. Extract contours and compute blob areas\n",
    "    blob_areas = []\n",
    "    blob_data = []\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        if label == 0:\n",
    "            continue # ignore background\n",
    "        region_mask = np.zeros_like(heatmap_np, dtype=np.uint8)\n",
    "        region_mask[labels == label] = 255\n",
    "\n",
    "        contours, _ = cv2.findContours(region_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not contours:\n",
    "            print(f\"No contours found for label {label}.\")\n",
    "            continue\n",
    "\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if perimeter == 0:\n",
    "            continue\n",
    "\n",
    "        circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "        blob_areas.append(area)\n",
    "        blob_data.append((cnt, area, circularity))\n",
    "\n",
    "    if not blob_areas:\n",
    "        print(\"No blobs found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 6. Compute average area, set thresholds\n",
    "    avg_area = np.median(blob_areas)\n",
    "    max_area = 1.5 * avg_area     # Reject blobs much larger than average\n",
    "    min_circularity = 0.5         # Reject elongated blobs\n",
    "\n",
    "    for cnt, area, circularity in sorted(blob_data, key=lambda x: x[1], reverse=True): # sorted by area (largest first)\n",
    "        if area > max_area:\n",
    "            # print(f\"Skipping large blob with area {area:.2f} > {max_area:.2f}\")\n",
    "            continue\n",
    "        if circularity < min_circularity:\n",
    "            # print(f\"Skipping elongated blob with circularity {circularity:.2f} < {min_circularity:.2f}\")\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "        pad = int(0.10 * max(w, h)) # 10% padding\n",
    "        side = max(w, h) + 2 * pad  # padded square\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        x1 = max(cx - side // 2, 0)\n",
    "        y1 = max(cy - side // 2, 0)\n",
    "        x2 = min(x1 + side, heatmap_np.shape[1])\n",
    "        y2 = min(y1 + side, heatmap_np.shape[0])\n",
    "\n",
    "        template = reflectance_np[y1:y2, x1:x2]\n",
    "        template = template[:, :, 0]  # (H, W, 1) -> (H, W)\n",
    "\n",
    "        if template.shape[0] != template.shape[1]:\n",
    "            template = cv2.resize(template, (side, side))\n",
    "\n",
    "        template = ((template - np.min(template)) / (np.max(template) - np.min(template)) * 255).astype(np.uint8)\n",
    "        templates.append(template)\n",
    "        template_bounds.append([x1, y1, side, side])\n",
    "\n",
    "        if len(templates) >= k_templates:\n",
    "            break\n",
    "\n",
    "    if debug and templates:\n",
    "        if len(templates) > 1:\n",
    "            figure, axis = plt.subplots(3, len(templates), figsize=(15, 5))\n",
    "            for i, template in enumerate(templates):\n",
    "                x1, y1, side1, side2 = template_bounds[i]\n",
    "\n",
    "                axis[0, i].imshow(heatmap_np, cmap='gray')\n",
    "                axis[0, i].add_patch(plt.Rectangle((x1, y1), side1, side2, edgecolor='red', facecolor='none', lw=2))\n",
    "                axis[0, i].set_title(f\"T {i+1}\")\n",
    "                axis[0, i].axis('off')\n",
    "\n",
    "                axis[1, i].imshow(reflectance_np, cmap='gray')\n",
    "                axis[1, i].add_patch(plt.Rectangle((x1, y1), side1, side2, edgecolor='red', facecolor='none', lw=2))\n",
    "                axis[1, i].axis('off')\n",
    "\n",
    "                axis[2, i].imshow(template, cmap='gray')\n",
    "                axis[2, i].axis('off')\n",
    "        else:\n",
    "            figure, axis = plt.subplots(3, 1, figsize=(10, 5))\n",
    "            x1, y1, side1, side2 = template_bounds[0]\n",
    "            axis[0].imshow(heatmap_np, cmap='gray')\n",
    "            axis[0].add_patch(plt.Rectangle((x1, y1), side1, side2, edgecolor='red', facecolor='none', lw=2))\n",
    "            axis[0].set_title(\"T 1\")\n",
    "            axis[0].axis('off')\n",
    "\n",
    "            axis[1].imshow(reflectance_np, cmap='gray')\n",
    "            axis[1].add_patch(plt.Rectangle((x1, y1), side1, side2, edgecolor='red', facecolor='none', lw=2))\n",
    "            axis[1].axis('off')\n",
    "\n",
    "            axis[2].imshow(templates[0], cmap='gray')\n",
    "            axis[2].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return templates, template_bounds\n",
    "\n",
    "def unet_get_template(image_yolo, model, k_templates, debug=False):\n",
    "    \"\"\"\n",
    "    Get the template from the image using UNet.\n",
    "\n",
    "    Args:\n",
    "        image_yolo (torch.Tensor): The input image as a tensor.\n",
    "        h (float): Height of the YOLO bounding box.\n",
    "        w (float): Width of the YOLO bounding box.\n",
    "        model (UNet): The UNet model.\n",
    "        debug (bool): Whether to show debug information.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The template extracted from the image.\n",
    "    \"\"\"\n",
    "    # === prepare image for UNet ===\n",
    "    image = transforms.Resize((384, 384), Image.BILINEAR)(image_yolo)\n",
    "    image = transforms.Grayscale(num_output_channels=1)(image) # convert to grayscale\n",
    "    reflectance, illumination = compute_retinex_reflectance_torch(image, sigma=50)\n",
    "\n",
    "    # === get UNet heatmap ===\n",
    "    with torch.no_grad():\n",
    "        heatmap = model(reflectance)\n",
    "    heatmap_np = heatmap.squeeze().cpu().numpy()\n",
    "    templates, template_bounds = extract_templates_watershed(heatmap_np, reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy(), k_templates, debug)\n",
    "\n",
    "    # === post-process for use in template matching ===\n",
    "    reflectance = reflectance.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 1)\n",
    "    reflectance = reflectance[:, :, 0] # remove extra channel\n",
    "    illumination = illumination.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 1)\n",
    "    illumination = illumination[:, :, 0] # remove extra channel\n",
    "\n",
    "    # normalize both to [0, 255]\n",
    "    reflectance = ((reflectance - np.min(reflectance)) / (np.max(reflectance) - np.min(reflectance)) * 255).astype(np.uint8)\n",
    "\n",
    "    return reflectance, illumination, templates, template_bounds, heatmap_np\n",
    "\n",
    "# === load image ===\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === get yolo crop ===\n",
    "yolo_path = '../yolo/runs/obb/train7/weights/best.pt'\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad, debug=True)\n",
    "\n",
    "# === get unet template ===\n",
    "unet_path = '../models/dot_detection/checkpoints/unet_best.pth'\n",
    "unet_model = load_unet(unet_path)\n",
    "reflectance, illumination, templates, template_bounds, heatmap_np = unet_get_template(image_yolo, unet_model, k_templates=3, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d176e",
   "metadata": {},
   "source": [
    "# Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, scores, overlap_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression on the bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "        scores: List of scores for each bounding box\n",
    "        overlap_thresh: Overlap threshold for suppression (default is 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after non-maximum suppression\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 0] + boxes[:, 2]\n",
    "    y2 = boxes[:, 1] + boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while len(idxs) > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        overlap = inter / (areas[i] + areas[idxs[1:]] - inter)\n",
    "        idxs = idxs[1:][overlap < overlap_thresh]\n",
    "    return boxes[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b33432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_matching(reflectance, template, method, match_thresh=0.7, nms_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Perform template matching to find the best match for the template in the reflectance image.\n",
    "\n",
    "    Args:\n",
    "        reflectance: Input reflectance image (numpy array)\n",
    "        templates: Template image (numpy array)\n",
    "        method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        match_thresh: Threshold for template matching (default is 0.7)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "\n",
    "    Returns:\n",
    "        List of bounding boxes for the detected matches.\n",
    "    \"\"\"\n",
    "    # === Template matching ===\n",
    "    result = cv2.matchTemplate(reflectance, template, method)\n",
    "    locations = zip(*np.where(result >= match_thresh)[::-1])\n",
    "    scores = result[result >= match_thresh].flatten()\n",
    "\n",
    "    # === Bounding boxes (x, y, w, h) for each match ===\n",
    "    h, w = template.shape\n",
    "    boxes = [(int(x), int(y), w, h) for (x, y) in locations]\n",
    "\n",
    "    # === Apply NMS ===\n",
    "    nms_boxes = non_max_suppression_fast(boxes, scores, overlap_thresh=nms_thresh)\n",
    "\n",
    "    return nms_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours_from_patch(patch):\n",
    "    \"\"\"\n",
    "    Extracts contours from supplied patch image.\n",
    "    \"\"\"\n",
    "    _, binary_patch = cv2.threshold(patch, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary_patch, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hu_descriptor(patch):\n",
    "    \"\"\"\n",
    "    Computes Hu moments for a given patch.\n",
    "\n",
    "    Args:\n",
    "        patch: Input patch (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Hu moments of the patch (numpy array)\n",
    "    \"\"\"\n",
    "    contours = contours_from_patch(patch)\n",
    "    if not contours:\n",
    "        return np.zeros(7)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    moments = cv2.moments(largest_contour)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    return -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)\n",
    "\n",
    "def select_diverse_templates(candidates, k):\n",
    "    \"\"\"\n",
    "    Selects k diverse templates from the candidates using greedy farthest-point sampling.\n",
    "\n",
    "    Args:\n",
    "        candidates: List of candidate patches (numpy arrays)\n",
    "        k: Number of templates to select\n",
    "\n",
    "    Returns:\n",
    "        List of selected templates (numpy arrays)\n",
    "    \"\"\"\n",
    "    selected = [candidates[0]]\n",
    "    selected_ids = {id(candidates[0])}\n",
    "\n",
    "    while len(selected) < k and len(selected) < len(candidates):\n",
    "        remaining = [c for c in candidates if id(c) not in selected_ids]\n",
    "        if not remaining:\n",
    "            break\n",
    "        best = max(\n",
    "            remaining,\n",
    "            key=lambda c: min(np.linalg.norm(c[0] - s[0]) for s in selected)\n",
    "        )\n",
    "        selected.append(best)\n",
    "        selected_ids.add(id(best))\n",
    "\n",
    "    # return patches and their bounding boxes separately\n",
    "    return [s[1] for s in selected], [s[2] for s in selected]\n",
    "\n",
    "def remove_outlier_boxes(boxes, eps=50, min_samples=3):\n",
    "    \"\"\"\n",
    "    Removes outlier boxes if they are too far from the main cluster of boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: List of bounding boxes (x, y, width, height)\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes after removing outliers\n",
    "    \"\"\"\n",
    "    centers = np.column_stack((boxes[:, 0] + boxes[:, 2] / 2,\n",
    "                               boxes[:, 1] + boxes[:, 3] / 2))\n",
    "    \n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    # keep only boxes from the largest cluster (label >= 0)\n",
    "    valid_labels = labels[labels >= 0]\n",
    "    if len(valid_labels) == 0:\n",
    "        return boxes.tolist() # fallback: keep all\n",
    "    main_cluster = np.argmax(np.bincount(valid_labels))\n",
    "    keep_indices = np.where(labels == main_cluster)[0]\n",
    "\n",
    "    return boxes[keep_indices].tolist()\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Computes the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        box1: First bounding box (x, y, width, height)\n",
    "        box2: Second bounding box (x, y, width, height)\n",
    "\n",
    "    Returns:\n",
    "        IoU value\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    xa = max(x1, x2)\n",
    "    ya = max(y1, y2)\n",
    "    xb = min(x1 + w1, x2 + w2)\n",
    "    yb = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def cascade_template_matching(reflectance, template_bounds, method, match_thresh=0.9, nms_thresh=0.3, debug=False):\n",
    "    \"\"\"\n",
    "    Performs template matching repeatedly using new matches as templates until enough matches found.\n",
    "\n",
    "    Args:\n",
    "        reflectance: Input reflectance image (numpy array)\n",
    "        heatmap_np: UNet heatmap (numpy array)\n",
    "        templates: List of template images (numpy arrays)\n",
    "        template_bounds: List of bounding boxes for the templates (x, y, width, height)\n",
    "        method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        match_thresh_t: Threshold for initial template matching (default is 0.7)\n",
    "        match_thresh_ct: Threshold for cascade template matching (default is 0.8)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "        k_templates: Number of templates to select (default is 4)\n",
    "        N_cascades: Number of cascades to perform (default is 1)\n",
    "        debug: If True, display debug information (default is False)\n",
    "\n",
    "    Returns:\n",
    "        List of bounding boxes for the detected matches.\n",
    "    \"\"\"\n",
    "    templates = template_bounds # tracking box coords of matches found in template matching\n",
    "    unused_templates = templates.copy() # track unused templates\n",
    "    used_templates = [] # track used templates\n",
    "\n",
    "    debug_snapshots = [] # for debugging, store snapshots of the cascade process\n",
    "\n",
    "    # perform template matching until all matches have been used as templates or number of matches above max possible\n",
    "    count = 0\n",
    "    while unused_templates:\n",
    "        if len(templates) >= 256:  # max number of templates to avoid excessive computation\n",
    "            print(\"Maximum number of templates reached, stopping cascade.\")\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Cascade {count}, templates found: {len(templates)}, unused: {len(unused_templates)}, used: {len(used_templates)}\")\n",
    "\n",
    "        for template in unused_templates:\n",
    "            if type(template) is not list:\n",
    "                template = template.tolist()\n",
    "            \n",
    "            used_templates.append(template) # add to used templates\n",
    "\n",
    "            # extract template from reflectance image\n",
    "            x, y, side1, side2 = template\n",
    "            template = reflectance[y:y + side2, x:x + side1]\n",
    "            \n",
    "            # do template matching\n",
    "            matches = template_matching(reflectance, template, method, match_thresh, nms_thresh).tolist()\n",
    "\n",
    "            # add new matches to template list if not overlapping with already existing templates\n",
    "            matches = [m for m in matches if not any(iou(m, template) > nms_thresh for template in templates)]\n",
    "            templates.extend(matches)\n",
    "\n",
    "        # remove overlapping templates\n",
    "        reduced = list(non_max_suppression_fast(np.array(templates), [1]*len(templates), overlap_thresh=nms_thresh))\n",
    "        templates = [t.tolist() for t in reduced] # convert list of numpy arrays to list of lists\n",
    "\n",
    "        # create new unused_templates list\n",
    "        unused_templates = []\n",
    "        for template in templates:\n",
    "            # only add templates that do not overlap with used templates\n",
    "            if any(iou(template, used_template) > nms_thresh for used_template in used_templates):\n",
    "                continue\n",
    "            unused_templates.append(template)\n",
    "        \n",
    "        if debug:\n",
    "            debug_snapshots.append({\n",
    "                'templates': templates.copy(),\n",
    "                'used_templates': used_templates.copy(),\n",
    "                'count': count\n",
    "            })\n",
    "\n",
    "    if debug and debug_snapshots:\n",
    "        n = len(debug_snapshots)\n",
    "        fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, snapshot in zip(axes, debug_snapshots):\n",
    "            ax.imshow(reflectance, cmap='gray')\n",
    "            for i, template in enumerate(snapshot['templates']):\n",
    "                x, y, side1, side2 = template\n",
    "                color = 'green' if template in snapshot['used_templates'] else 'red'\n",
    "                ax.add_patch(plt.Rectangle((x, y), side1, side2, edgecolor=color, facecolor='none', lw=2))\n",
    "            ax.set_title(f\"Cascade {snapshot['count']}\")\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    final_templates = np.array(templates)\n",
    "    final_templates = remove_outlier_boxes(final_templates)\n",
    "\n",
    "    return final_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f710876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Displays the numpy image using PIL and notebook display functionality.\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        size: Size to which the image should be resized (default is (300, 300))\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, size)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a90c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_yucheng_methods(nms_boxes, reflectance, img, illumination, template_A, template_B):\n",
    "    \"\"\"\n",
    "    Displays the results of Yuchengs methods for dot detection and template matching.\n",
    "\n",
    "    Args:\n",
    "        nms_boxes: List of bounding boxes after non-maximum suppression\n",
    "        reflectance: Reflectance map (numpy array)\n",
    "        dot_contours: Contours of the detected dots\n",
    "        img: Original image (numpy array)\n",
    "        illumination: Estimated illumination (numpy array)\n",
    "        dot_template: Dot template (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # === Draw matching result ===\n",
    "    output = cv2.cvtColor(reflectance, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h) in nms_boxes:\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # === Show results ===\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "    axs[0, 0].imshow(img, cmap='gray')\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(illumination, cmap='gray')\n",
    "    axs[0, 1].set_title(\"Estimated Illumination\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    axs[0, 2].imshow(reflectance, cmap='gray')\n",
    "    axs[0, 2].set_title(\"Reflectance Map (SSR)\")\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    axs[1, 0].imshow(cv2.cvtColor(template_A, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Template A\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(template_B, cmap='gray')\n",
    "    axs[1, 1].set_title(\"Template B\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    axs[1, 2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 2].set_title(\"Template matching\")\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb86f39",
   "metadata": {},
   "source": [
    "## Yucheng Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === load image ===\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "image = load_image_to_device(img_to_test)\n",
    "\n",
    "# === get yolo crop ===\n",
    "yolo_path = '../yolo/runs/obb/train7/weights/best.pt'\n",
    "pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo(yolo_path)\n",
    "image_yolo = yolo_detect_and_crop(image, yolo_model, pad)\n",
    "\n",
    "# === get image and unet template ===\n",
    "unet_path = '../models/dot_detection/checkpoints/unet_best.pth'\n",
    "unet_model = load_unet(unet_path)\n",
    "reflectance, illumination, templates, template_bounds, heatmap_np = unet_get_template(image_yolo, unet_model, k_templates=3, debug=True)\n",
    "\n",
    "# === cascade template matching ===\n",
    "nms_boxes = cascade_template_matching(reflectance, template_bounds, cv2.TM_CCOEFF_NORMED, match_thresh=0.925, nms_thresh=0.2, debug=True)\n",
    "\n",
    "# === visualize results ===\n",
    "display_yucheng_methods(nms_boxes, reflectance, image_yolo.squeeze(0).permute(1, 2, 0).cpu().numpy(), illumination, templates[0], templates[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2725a5",
   "metadata": {},
   "source": [
    "# Grid Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc81c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(params, grid_size=20):\n",
    "    \"\"\"\n",
    "    Generates a grid of DMC points based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x0: X coordinate of the center of the grid\n",
    "        y0: Y coordinate of the center of the grid\n",
    "        sx: Scale factor in the X direction\n",
    "        sy: Scale factor in the Y direction\n",
    "        theta: Rotation angle in radians\n",
    "        grid_size: Size of the grid (default is 16)\n",
    "\n",
    "    Returns:\n",
    "        array of grid points (x, y) in the original coordinate system\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = params\n",
    "\n",
    "    # force sx and sy to be minimum of 1.0 (to avoid zero size)\n",
    "    sx = max(sx, 1.0)\n",
    "    sy = max(sy, 1.0)\n",
    "\n",
    "    # building full grid of DMC points\n",
    "    coords = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            coords.append([i, j])\n",
    "    coords = np.array(coords).astype(float)\n",
    "\n",
    "    # center the grid around (0, 0)\n",
    "    coords -= (grid_size - 1) / 2\n",
    "\n",
    "    # Convert to original coordinate system\n",
    "    coords = np.dot(coords, np.array([[sx, 0], [0, sy]])) # scale\n",
    "    coords = np.dot(coords, np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])) # rotate\n",
    "    coords += np.array([[x0, y0]]) # translate\n",
    "\n",
    "    return coords\n",
    "\n",
    "def inverse_grid_transform(grid_pts, params, grid_size=20):\n",
    "    \"\"\"\n",
    "    Inverse transformation of the grid points to the original coordinate system.\n",
    "\n",
    "    Args:\n",
    "        grid_pts: Grid points to be transformed (numpy array)\n",
    "        params: Parameters used for the transformation (x0, y0, sx, sy, theta)\n",
    "        grid_size: Size of the grid (default is 16)\n",
    "\n",
    "    Returns:\n",
    "        array of grid points (x, y) in the dmc coordinate system\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = params\n",
    "\n",
    "    # force sx and sy to be minimum of 1.0 (same as in generate_grid)\n",
    "    sx = max(sx, 1.0)\n",
    "    sy = max(sy, 1.0)\n",
    "\n",
    "    # undo translation\n",
    "    grid_pts = grid_pts.astype(float)\n",
    "    grid_pts -= np.array([[x0, y0]])\n",
    "\n",
    "    # undo rotation\n",
    "    rot_mat_inv = np.array([[np.cos(theta), np.sin(theta)],\n",
    "                            [-np.sin(theta), np.cos(theta)]])\n",
    "    grid_pts = np.dot(grid_pts, rot_mat_inv)\n",
    "\n",
    "    # undo scaling\n",
    "    grid_pts = np.dot(grid_pts, np.linalg.inv(np.diag([sx, sy])))\n",
    "\n",
    "    # convert to standard grid coordinates\n",
    "    grid_pts += (grid_size - 1) / 2\n",
    "\n",
    "    # round to nearest integer\n",
    "    grid_pts = np.rint(grid_pts).astype(int)\n",
    "\n",
    "    # clip to valid range (to avoid errors)\n",
    "    if np.any(grid_pts < 0) or np.any(grid_pts >= grid_size):\n",
    "        print(\"Warning: one or more grid points are out of bounds!\")\n",
    "        grid_pts = np.clip(grid_pts, 0, grid_size - 1)\n",
    "\n",
    "    return grid_pts\n",
    "\n",
    "\n",
    "def show_grid(img, grid_pts, title=\"Grid Points\"):\n",
    "    \"\"\"\n",
    "    Shows the grid points on the image.\n",
    "\n",
    "    Args:\n",
    "        img: Input image (numpy array)\n",
    "        grid_pts: Grid points to be displayed (numpy array)\n",
    "        title: Title of the plot (default is \"Grid Points\")\n",
    "    \"\"\"\n",
    "    img = img.copy() # to avoid modifying the original image\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # Convert grayscale to BGR\n",
    "    for p in grid_pts:\n",
    "        # cv2.circle(img, (int(p[0]), int(p[1])), 3, (0, 255, 0), -1)\n",
    "        cv2.circle(img, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1) # red color for grid points\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def show_grids(img, init_estimate, opt_center, opt_spacing, opt_theta, opt_all, mapped):\n",
    "    \"\"\"\n",
    "    Shows multiple grid debug images on the same plot.\n",
    "\n",
    "    Args:\n",
    "        img: Input image (numpy array)\n",
    "        init_estimate: Initial estimate of the grid points (numpy array)\n",
    "        opt_center: Optimized center of the grid (numpy array)\n",
    "        opt_spacing: Optimized spacing of the grid (numpy array)\n",
    "        opt_theta: Optimized rotation angle of the grid (float)\n",
    "        opt_all: Optimized grid points (numpy array)\n",
    "        mapped: Mapped grid points (numpy array)\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # Convert grayscale to BGR\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in init_estimate:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1) # red color for grid points\n",
    "    axs[0, 0].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 0].set_title(\"Initial Estimate\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in opt_center:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1)\n",
    "    axs[0, 1].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 1].set_title(\"Optimized Center\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in opt_spacing:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1)\n",
    "    axs[0, 2].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 2].set_title(\"Optimized Spacing\")\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in opt_theta:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1)\n",
    "    axs[1, 0].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Optimized Theta\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in opt_all:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1)\n",
    "    axs[1, 1].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 1].set_title(\"Optimized All\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    for p in mapped:\n",
    "        cv2.circle(img_copy, (int(p[0]), int(p[1])), 3, (0, 0, 255), -1)\n",
    "    axs[1, 2].imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 2].set_title(\"Mapped Points\")\n",
    "    axs[1, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Example usage of grid generation ===\n",
    "img = reflectance\n",
    "grid_size = 20\n",
    "\n",
    "# params to cover entire image\n",
    "img_size = img.shape[0]\n",
    "params = [\n",
    "    img_size / 2, # x0\n",
    "    img_size / 2, # y0\n",
    "    img_size / (grid_size-1), # sx\n",
    "    img_size / (grid_size-1), # sy\n",
    "    0.0 # theta\n",
    "]\n",
    "grid_pts = generate_grid(params, grid_size)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_grid_params(nms_boxes, img, debug=False):\n",
    "    \"\"\"\n",
    "    Estimates grid starting parameters using the observed points from the detected boxes.\n",
    "    The function uses the observed points to compute the initial parameters for the affine transformation.\n",
    "    \"\"\"\n",
    "    observed_pts = np.array([[x + w / 2, y + h / 2] for (x, y, w, h) in nms_boxes])\n",
    "\n",
    "    # estimating reasonable x, y based on the average of all points\n",
    "    x = np.mean(observed_pts[:, 0])\n",
    "    y = np.mean(observed_pts[:, 1])\n",
    "\n",
    "    # getting mode distance between closest points for later usage\n",
    "    distances = []\n",
    "    for i in range(len(observed_pts)):\n",
    "        closest_pt = float('inf')\n",
    "        for j in range(i + 1, len(observed_pts)):\n",
    "            if i != j:\n",
    "                # find the closest point to i\n",
    "                dist = np.linalg.norm(observed_pts[i] - observed_pts[j])\n",
    "                if dist < closest_pt:\n",
    "                    closest_pt = dist\n",
    "        if closest_pt != float('inf'):\n",
    "            distances.append(np.round(closest_pt, 2))\n",
    "    # mode distance is used because we want the most common distance, not the average\n",
    "    mod_dist = get_mode(distances)\n",
    "    if debug:\n",
    "        print(f\"Modal distance between (close) points: {mod_dist}\")\n",
    "\n",
    "    # estimating sx & sy & theta based on \"L shapes\" formed by the points\n",
    "    L_shapes = set()\n",
    "    all_L_shapes = set() # to keep track of all L shapes found\n",
    "    for i in range(len(observed_pts)):\n",
    "        # find closest 2 points to i\n",
    "        dist_a = float('inf')\n",
    "        a_idx = -1\n",
    "        dist_b = float('inf')\n",
    "        b_idx = -1\n",
    "        for j in range(len(observed_pts)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dist = np.linalg.norm(observed_pts[i] - observed_pts[j])\n",
    "\n",
    "            # if point closer than a, update a and b accordingly\n",
    "            if dist < dist_a:\n",
    "                # b takes the place of a\n",
    "                dist_b = dist_a\n",
    "                b_idx = a_idx\n",
    "                \n",
    "                # a takes the place of j\n",
    "                dist_a = dist\n",
    "                a_idx = j\n",
    "            # if point is only closer than b, update b\n",
    "            elif dist < dist_b:\n",
    "                dist_b = dist\n",
    "                b_idx = j\n",
    "        \n",
    "        # closest points found, now calculate all 3 \"L\" shapes (each point can be the corner of the \"L\")\n",
    "        L1 = (observed_pts[a_idx], observed_pts[i], observed_pts[b_idx]) # i in the corner\n",
    "        L2 = (observed_pts[i], observed_pts[a_idx], observed_pts[b_idx]) # a in the corner\n",
    "        L3 = (observed_pts[i], observed_pts[b_idx], observed_pts[a_idx]) # b in the corner\n",
    "\n",
    "        for L in [L1, L2, L3]:\n",
    "            # check if L forms a 90 degree angle\n",
    "\n",
    "            vec_a = L[0] - L[1] # vector from corner (L[1]) to first edge point (L[0])\n",
    "            vec_b = L[2] - L[1] # vector from corner (L[1]) to second edge point (L[2])\n",
    "            cos_angle = np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b) + 1e-8)\n",
    "            angle_deg = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "            if 80 <= angle_deg <= 100:\n",
    "                # L shape has valid angle\n",
    "                # check if distances from edge points to corner point are relatively equal\n",
    "                dist_a = np.linalg.norm(vec_a)\n",
    "                dist_b = np.linalg.norm(vec_b)\n",
    "                abs_diff = abs(dist_a - dist_b)\n",
    "                if abs_diff <= img.shape[0] * 0.01:  # allow distances to differ by 1% of the image size\n",
    "\n",
    "                    # vertical distance is the distance of the most vertical vector\n",
    "                    if abs(vec_a[0]) < abs(vec_b[0]): # vec_a is more vertical than vec_b\n",
    "                        vertical_dist = dist_a\n",
    "                        horizontal_dist = dist_b\n",
    "                    else: # vec_b is more vertical than vec_a\n",
    "                        vertical_dist = dist_b\n",
    "                        horizontal_dist = dist_a\n",
    "\n",
    "                    # store the orientation of the L shape (smallest angle away from vertical)\n",
    "                    L_orientation = np.arctan2(vec_a[1], vec_a[0]) - np.arctan2(0, 1) # angle between vector and vertical\n",
    "                    L_orientation = np.round(L_orientation, 2) # round to 2 decimal places for consistent mode calculation\n",
    "                    if L_orientation == 0: # weird case where numpy differentiates between -0.0 and 0.0\n",
    "                        L_orientation = 0\n",
    "                    L_orientation = -L_orientation # flipped angle to match the default vector (0, 1)\n",
    "\n",
    "                    # L shape is temporarily valid, add it to the set with dist and orientation values at end\n",
    "                    L_shapes.add((tuple(L[0].tolist()), tuple(L[1].tolist()), tuple(L[2].tolist()), horizontal_dist, vertical_dist, L_orientation))\n",
    "                    all_L_shapes.add((tuple(L[0].tolist()), tuple(L[1].tolist()), tuple(L[2].tolist()), horizontal_dist, vertical_dist, L_orientation))\n",
    "\n",
    "    # return default parameters if no L shapes found\n",
    "    if not L_shapes:\n",
    "        print(\"No valid L shapes found, using default parameters.\")\n",
    "        return [x, y, mod_dist, mod_dist, 0.0], observed_pts\n",
    "\n",
    "    # round distances to ints and keep only L shapes with distances close to the mode\n",
    "    modal_x = get_mode([int(L[3]) for L in L_shapes])  # horizontal distance\n",
    "    modal_y = get_mode([int(L[4]) for L in L_shapes])  # vertical distance\n",
    "    # keep only L shapes with distances equal to modal_x and modal_y\n",
    "    L_shapes = [L for L in L_shapes if int(L[3]) <= modal_x or int(L[4]) <= modal_y]\n",
    "\n",
    "    # round theta to nearest degree and keep only L shapes with orientation close to the mode\n",
    "    modal_theta = get_mode([int(np.round(L[5] * 180 / np.pi)) for L in L_shapes])  # convert radians to degrees\n",
    "    L_shapes = [L for L in L_shapes if int(np.round(L[5] * 180 / np.pi)) == modal_theta]\n",
    "    \n",
    "    # estimate sx, sy, theta based on the average of the L shapes\n",
    "    sx = np.mean([L[3] for L in L_shapes])  # horizontal distance\n",
    "    sy = np.mean([L[4] for L in L_shapes])  # vertical distance\n",
    "    theta = np.mean([L[5] for L in L_shapes])  # orientation in radians\n",
    "\n",
    "    if debug:\n",
    "        # visualize the L shapes found\n",
    "        if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "            img_copy = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            img_copy = img.copy()\n",
    "        for L in all_L_shapes:\n",
    "            point_a, corner, point_b = L[:3]\n",
    "            point_a = (int(point_a[0]), int(point_a[1]))\n",
    "            corner = (int(corner[0]), int(corner[1]))\n",
    "            point_b = (int(point_b[0]), int(point_b[1]))\n",
    "\n",
    "            # draw color based on if L shape dists and orientation are equal to the estimated sx, sy, theta\n",
    "            if L in L_shapes:\n",
    "                color = (0, 255, 0)  # green for valid L shapes\n",
    "            else:\n",
    "                color = (0, 0, 255) # red for invalid L shapes\n",
    "\n",
    "            cv2.line(img_copy, point_a, corner, color, 2)\n",
    "            cv2.line(img_copy, point_b, corner, color, 2)\n",
    "            cv2.line(img_copy, point_a, point_b, color, 2)\n",
    "        plt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n",
    "        # plt.title(\"Estimated L Shapes\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Estimated parameters: x={x}, y={y}, sx={sx}, sy={sy}, theta={theta}\")\n",
    "\n",
    "    return [x, y, sx, sy, theta], observed_pts\n",
    "\n",
    "# === Example usage of grid parameter estimation ===\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, reflectance, debug=True)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d78d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convex_hull(observed_points, grid_points=None):\n",
    "    hull = ConvexHull(observed_points)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(observed_points[:, 0], observed_points[:, 1], 'o', label='Observed Points')\n",
    "\n",
    "    # Draw convex hull edges\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(observed_points[simplex, 0], observed_points[simplex, 1], 'k-')\n",
    "\n",
    "    # Optionally plot grid points\n",
    "    if grid_points is not None:\n",
    "        plt.plot(grid_points[:, 0], grid_points[:, 1], 'rx', label='Grid Points')\n",
    "\n",
    "    plt.title(\"Observed Points and Convex Hull\")\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hungarian_cost(params, observed_pts):\n",
    "    \"\"\"\n",
    "    Cost function for optimization using the Hungarian algorithm.\n",
    "    Args:\n",
    "        params: Parameters for the grid (x0, y0, sx, sy, theta)\n",
    "        observed_pts: Observed points (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        float: Total cost of the assignment\n",
    "    \"\"\"\n",
    "    grid_pts = generate_grid(params)\n",
    "    cost_matrix = cdist(observed_pts, grid_pts, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    total_cost = cost_matrix[row_ind, col_ind].sum()\n",
    "    if np.isnan(total_cost) or np.isinf(total_cost):\n",
    "        return float('inf')  # Return a large cost if the total cost is invalid\n",
    "    return total_cost\n",
    "\n",
    "# === Example usage of cost function ===\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "\n",
    "# cost of grid covering entire image\n",
    "grid_size = 20\n",
    "params = [\n",
    "    img.shape[0] / 2, # x0\n",
    "    img.shape[0] / 2, # y0\n",
    "    img.shape[0] / (grid_size-1), # sx\n",
    "    img.shape[0] / (grid_size-1), # sy\n",
    "    0.0 # theta\n",
    "]\n",
    "grid_pts = generate_grid(params)\n",
    "show_grid(img, grid_pts)\n",
    "observed_pts = np.array([[x + w / 2, y + h / 2] for (x, y, w, h) in nms_boxes])\n",
    "cost_value = hungarian_cost(params, observed_pts)\n",
    "print(f\"Cost value (naive start): {cost_value}\")\n",
    "\n",
    "init_params, observed_pts = estimate_grid_params(nms_boxes, reflectance, debug=False)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)\n",
    "cost_value = hungarian_cost(init_params, observed_pts)\n",
    "print(f\"Cost value (estimated start): {cost_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33131f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_xy(xy, params, observed_pts):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the xy coordinates and the rest of the parameters separately.\n",
    "    Used for optimizing x0 and y0.\n",
    "\n",
    "    Args:\n",
    "        xy: X and Y coordinates of the center of the grid\n",
    "        params: Parameters for grid (sx, sy, theta)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    x0, y0 = xy\n",
    "    sx, sy, theta = params\n",
    "    return hungarian_cost([x0, y0, sx, sy, theta], observed_pts)\n",
    "\n",
    "def cost_sx_sy(sx_sy, params, observed_pts):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the sx and sy coordinates and the rest of the parameters separately.\n",
    "    Used for optimizing sx and sy.\n",
    "\n",
    "    Args:\n",
    "        sx_sy: Scale factors in the X and Y directions\n",
    "        params: Parameters for grid (x0, y0, theta)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    sx, sy = sx_sy\n",
    "    x0, y0, theta = params\n",
    "    return hungarian_cost([x0, y0, sx, sy, theta], observed_pts)\n",
    "\n",
    "def cost_theta(theta, params, observed_pts):\n",
    "    \"\"\"\n",
    "    Wrapper function for cost function. It takes the theta coordinate and the rest of the parameters separately.\n",
    "    Used for optimizing theta.\n",
    "\n",
    "    Args:\n",
    "        theta: Rotation angle in radians\n",
    "        params: Parameters for grid (x0, y0, sx, sy)\n",
    "    \n",
    "    Returns:\n",
    "        Mean squared distance between observed points and grid points.\n",
    "    \"\"\"\n",
    "    theta = theta[0]  # Extract the single value from the array\n",
    "    x0, y0, sx, sy = params\n",
    "    return hungarian_cost([x0, y0, sx, sy, theta], observed_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_grid(init_params, observed_pts, debug=False):\n",
    "    \"\"\"\n",
    "    Estimates the grid parameters using optimization. The function optimizes the parameters\n",
    "    (x0, y0, sx, sy, theta) to minimize the cost function based on the observed points.\n",
    "\n",
    "    The optimization is done in the following order:\n",
    "    1. Optimize x0 and y0\n",
    "    2. Optimize sx and sy\n",
    "    3. Optimize theta\n",
    "    4. Optimize all parameters together\n",
    "\n",
    "    Args:\n",
    "        init_params: Initial parameters for the grid (x0, y0, sx, sy, theta)\n",
    "        init_pts: Initial estimate for grid points (numpy array)\n",
    "        img: Input image (numpy array)\n",
    "        N: Number of closest points to consider for cost function\n",
    "        alpha: Weighting factor for the cost function\n",
    "        debug: If True, display debug information (default is False)\n",
    "    \n",
    "    Returns:\n",
    "        Optimized parameters (x0, y0, sx, sy, theta) and the observed points.\n",
    "    \"\"\"\n",
    "    x0, y0, sx, sy, theta = init_params\n",
    "    if debug:\n",
    "        init_pts = generate_grid([x0, y0, sx, sy, theta])\n",
    "\n",
    "    # 1. optimize for x0, y0 only\n",
    "    result = minimize(cost_xy, [x0, y0], args=([sx, sy, theta], observed_pts), method='Powell')\n",
    "    x0, y0 = result.x\n",
    "    if debug:\n",
    "        print([x0, y0, sx, sy, theta])\n",
    "        opt_center = generate_grid([x0, y0, sx, sy, theta])\n",
    "    \n",
    "    # 2. optimize for sx, sy only\n",
    "    result = minimize(cost_sx_sy, [sx, sy], args=([x0, y0, theta], observed_pts), method='Powell')\n",
    "    sx, sy = result.x\n",
    "    if debug:\n",
    "        print([x0, y0, sx, sy, theta])\n",
    "        opt_spacing = generate_grid([x0, y0, sx, sy, theta])\n",
    "\n",
    "    # 3. optimize for theta only\n",
    "    result = minimize(cost_theta, [theta], args=([x0, y0, sx, sy], observed_pts), method='Powell')\n",
    "    theta = result.x[0]\n",
    "    # trying other 3 90 degree angles to see if they are better\n",
    "    lowest_cost = hungarian_cost([x0, y0, sx, sy, theta], observed_pts)\n",
    "    for i in range(1, 4):\n",
    "        theta_tmp = (theta + i * np.pi / 2) % (2 * np.pi)\n",
    "        # theta_tmp_cost = cost([x0, y0, sx, sy, theta_tmp], observed_pts, N, alpha)\n",
    "        theta_tmp_cost = hungarian_cost([x0, y0, sx, sy, theta_tmp], observed_pts)\n",
    "        if theta_tmp_cost < lowest_cost:\n",
    "            if debug:\n",
    "                print(f\"Found better theta: {theta_tmp} with cost: {theta_tmp_cost}. (previous: {theta} with cost: {lowest_cost})\")\n",
    "            lowest_cost = theta_tmp_cost\n",
    "            theta = theta_tmp\n",
    "    if debug:\n",
    "        print([x0, y0, sx, sy, theta])\n",
    "        opt_theta = generate_grid([x0, y0, sx, sy, theta])\n",
    "\n",
    "    # 4. optimize for all parameters together\n",
    "    result = minimize(hungarian_cost, [x0, y0, sx, sy, theta], args=(observed_pts,), method='Powell')\n",
    "    x0, y0, sx, sy, theta = result.x\n",
    "    if debug:\n",
    "        print([x0, y0, sx, sy, theta])\n",
    "        opt_all = generate_grid([x0, y0, sx, sy, theta])\n",
    "\n",
    "    if debug:\n",
    "        return [x0, y0, sx, sy, theta], observed_pts, init_pts, opt_center, opt_spacing, opt_theta, opt_all\n",
    "    else:\n",
    "        return [x0, y0, sx, sy, theta], observed_pts\n",
    "\n",
    "# === Example usage of grid estimation ===\n",
    "init_params, init_pts = estimate_grid_params(nms_boxes, reflectance, debug=False)\n",
    "print(f\"Initial parameters: {init_params}\")\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)\n",
    "opt_params, observed_pts, init_pts, opt_center, opt_spacing, opt_theta, opt_all = estimate_grid(init_params, init_pts, debug=True)\n",
    "grid_pts = generate_grid(opt_params)\n",
    "show_grid(img, grid_pts, title=\"Optimized Grid Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69019f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing different grid points with different parameters\n",
    "empty_img = np.zeros_like(img)\n",
    "\n",
    "# alter values to be gray\n",
    "empty_img.fill(200)  # Fill with gray color\n",
    "\n",
    "print(empty_img.shape)\n",
    "\n",
    "example_params = [\n",
    "    img.shape[0] / 2,  # x0\n",
    "    img.shape[0] / 2,  # y0\n",
    "    (img.shape[0] / (grid_size - 1)) / 1.5,  # sx\n",
    "    (img.shape[0] / (grid_size - 1)) / 1.5,  # sy\n",
    "    0.0  # theta\n",
    "]\n",
    "example_grid_pts = generate_grid(example_params, grid_size)\n",
    "print(example_params)\n",
    "show_grid(empty_img, example_grid_pts, title=\"\")\n",
    "\n",
    "example_params = [\n",
    "    img.shape[0] / 1.5,  # x0\n",
    "    img.shape[0] / 2,  # y0\n",
    "    (img.shape[0] / (grid_size - 1)) / 2.5,  # sx\n",
    "    (img.shape[0] / (grid_size - 1)) / 2.5,  # sy\n",
    "    1.0  # theta\n",
    "]\n",
    "example_grid_pts = generate_grid(example_params, grid_size)\n",
    "print(example_params)\n",
    "show_grid(empty_img, example_grid_pts, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_observed_to_grid(observed_pts, grid_pts):\n",
    "    \"\"\"\n",
    "    Maps observed points to grid points using the Hungarian algorithm to ensure 1-1 mapping.\n",
    "\n",
    "    Args:\n",
    "        observed_pts: Observed points (numpy array)\n",
    "        grid_pts: Grid points (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Mapped grid points (numpy array)\n",
    "    \"\"\"\n",
    "    cost_matrix = cdist(observed_pts, grid_pts, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    mapped_grid_pts = grid_pts[col_ind]\n",
    "    return mapped_grid_pts\n",
    "\n",
    "mapped_pts = map_observed_to_grid(observed_pts, opt_all)\n",
    "show_grids(img, init_pts, opt_center, opt_spacing, opt_theta, opt_all, mapped_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449422b",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_DMC(matrix, title=\"DMC Matrix\"):\n",
    "    \"\"\"\n",
    "    Display the DMC matrix like normal DMC.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "        title: Title of the plot (default is \"DMC Matrix\")\n",
    "    \"\"\"\n",
    "    # Invert the matrix for display\n",
    "    matrix = np.invert(matrix)\n",
    "    plt.imshow(matrix, cmap='gray', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92542d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dmc_matrix(dmc_pts, grid_size=20, debug=False):\n",
    "    \"\"\"\n",
    "    Converts DMC points to a matrix representation.\n",
    "\n",
    "    Args:\n",
    "        dmc_pts: DMC points (numpy array)\n",
    "        grid_size: Size of the grid (default is 20)\n",
    "\n",
    "    Returns:\n",
    "        DMC matrix (numpy array)\n",
    "    \"\"\"\n",
    "    if len(dmc_pts) < 50:\n",
    "        print(\"Not enough points to create DMC matrix.\")\n",
    "        return None\n",
    "\n",
    "    dmc_matrix = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    for pt in dmc_pts:\n",
    "        x, y = pt\n",
    "        dmc_matrix[y, x] = 1\n",
    "\n",
    "    if debug:\n",
    "        dmc_straight = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    # delete cols and rows that are all zeros\n",
    "    dmc_matrix = dmc_matrix[np.any(dmc_matrix, axis=1)]\n",
    "    dmc_matrix = dmc_matrix[:, np.any(dmc_matrix, axis=0)]\n",
    "\n",
    "    if debug:\n",
    "        dmc_zero_delete = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    # if edge col or row has a single 1, delete it\n",
    "    if dmc_matrix[0, :].sum() == 1:\n",
    "        dmc_matrix = dmc_matrix[1:, :]\n",
    "    if dmc_matrix[:, 0].sum() == 1:\n",
    "        dmc_matrix = dmc_matrix[:, 1:]\n",
    "    if dmc_matrix[-1, :].sum() == 1:\n",
    "        dmc_matrix = dmc_matrix[:-1, :]\n",
    "    if dmc_matrix[:, -1].sum() == 1:\n",
    "        dmc_matrix = dmc_matrix[:, :-1]\n",
    "\n",
    "    if debug:\n",
    "        dmc_one_delete = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    # two edges with most 1s are filled with 1s\n",
    "    top_count = dmc_matrix[0, :].sum()\n",
    "    left_count = dmc_matrix[:, 0].sum()\n",
    "    bottom_count = dmc_matrix[-1, :].sum()\n",
    "    right_count = dmc_matrix[:, -1].sum()\n",
    "    bl = bottom_count + left_count\n",
    "    br = bottom_count + right_count\n",
    "    tl = top_count + left_count\n",
    "    tr = top_count + right_count\n",
    "    if br > bl and br > tl and br > tr:\n",
    "        dmc_matrix[-1, :] = 1\n",
    "        dmc_matrix[:, -1] = 1\n",
    "        orientation = \"bottom right\"\n",
    "    elif tl > bl and tl > br and tl > tr:\n",
    "        dmc_matrix[0, :] = 1\n",
    "        dmc_matrix[:, 0] = 1\n",
    "        orientation = \"top left\"\n",
    "    elif tr > bl and tr > br and tr > tl:\n",
    "        dmc_matrix[0, :] = 1\n",
    "        dmc_matrix[:, -1] = 1\n",
    "        orientation = \"top right\"\n",
    "    else: # either bottom left is best or all are equal (in which case we default to bottom left)\n",
    "        dmc_matrix[-1, :] = 1\n",
    "        dmc_matrix[:, 0] = 1\n",
    "        orientation = \"bottom left\"\n",
    "\n",
    "    if debug:\n",
    "        dmc_finder_fill = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    # rotate orientation of the grid according to found orientation\n",
    "    if orientation == \"bottom right\": # rotate 90 deg clockwise\n",
    "        dmc_matrix = np.rot90(dmc_matrix, k=3)\n",
    "    elif orientation == \"top left\": # rotate 90 deg anticlockwise\n",
    "        dmc_matrix = np.rot90(dmc_matrix, k=1)\n",
    "    elif orientation == \"top right\": # rotate 180 deg\n",
    "        dmc_matrix = np.rot90(dmc_matrix, k=2)\n",
    "\n",
    "    if debug:\n",
    "        dmc_rotated = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    # iterate over timing pattern points and set accordingly\n",
    "    w_size = dmc_matrix.shape[1]\n",
    "    h_size = dmc_matrix.shape[0]\n",
    "    print(f\"Grid size: {w_size}x{h_size}\")\n",
    "    # unexpected 1s on the top are shifted down a pixel\n",
    "    for i in range(1, w_size-1):\n",
    "        if i % 2 == 0:\n",
    "            dmc_matrix[0, i] = 1\n",
    "        else:\n",
    "            if dmc_matrix[0, i] == 1: # unexpected 1\n",
    "                dmc_matrix[1, i] = 1\n",
    "            dmc_matrix[0, i] = 0\n",
    "    # unexpected 1s on the right are shifted left a pixel\n",
    "    for i in range(1, h_size-1):\n",
    "        if i % 2 == 1:\n",
    "            dmc_matrix[i, -1] = 1\n",
    "        else:\n",
    "            if dmc_matrix[i, -1] == 1: # unexpected 1\n",
    "                dmc_matrix[i, -2] = 1\n",
    "            dmc_matrix[i, -1] = 0\n",
    "\n",
    "    if debug:\n",
    "        dmc_timing_fill = np.invert(dmc_matrix.copy())\n",
    "\n",
    "    if debug:\n",
    "        # display all debug images\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(dmc_straight, cmap='gray')\n",
    "        plt.title(\"Direct Mapping\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(dmc_zero_delete, cmap='gray')\n",
    "        plt.title(\"Zero Delete\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(dmc_one_delete, cmap='gray')\n",
    "        plt.title(\"One Delete\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(dmc_finder_fill, cmap='gray')\n",
    "        plt.title(\"Finder Fill\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(dmc_rotated, cmap='gray')\n",
    "        plt.title(\"Rotated\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.imshow(dmc_timing_fill, cmap='gray')\n",
    "        plt.title(\"Timing Fill\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    return dmc_matrix\n",
    "\n",
    "# === Example usage of mapping to DMC points ===\n",
    "img = reflectance\n",
    "nms_boxes = [(x, y, w, h) for (x, y, w, h) in nms_boxes]\n",
    "init_params, init_pts = estimate_grid_params(nms_boxes, reflectance)\n",
    "\n",
    "opt_params, observed_pts = estimate_grid(init_params, init_pts, debug=False)\n",
    "grid_pts = generate_grid(opt_params)\n",
    "mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "dmc_pts = inverse_grid_transform(mapped_grid_pts, opt_params)\n",
    "dmc_matrix = to_dmc_matrix(dmc_pts, debug=True)\n",
    "print(\"DMC matrix:\")\n",
    "print(dmc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea99af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_DMC(matrix):\n",
    "    \"\"\"\n",
    "    Decodes the DMC matrix using pylibdmtx.\n",
    "\n",
    "    Args:\n",
    "        matrix: Numpy array representing the DMC matrix\n",
    "    \n",
    "    Returns:\n",
    "        Decoded string if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    # Converting binary matrix to uint8 image\n",
    "    image = np.zeros((matrix.shape[0], matrix.shape[1]), dtype=np.uint8)\n",
    "    image[matrix == 1] = 255\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Inverting the image for decoding\n",
    "    image = Image.eval(image, lambda x: 255 - x)\n",
    "\n",
    "    # Padding the image by 2 pixels to add margin larger than a DMC module (https://www.keyence.eu/ss/products/auto_id/codereader/basic_2d/datamatrix.jsp)\n",
    "    image = np.pad(np.array(image), ((2, 2), (2, 2)), mode='constant', constant_values=255)\n",
    "    image = Image.fromarray(image, 'L')\n",
    "\n",
    "    # Resizing to larger image for better decoding\n",
    "    image = image.resize((image.size[0] * 10, image.size[1] * 10), Image.NEAREST)\n",
    "\n",
    "    # display the image\n",
    "    # plt.imshow(image, cmap='gray')\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Decode using pylibdmtx\n",
    "    decoded = decode(image)\n",
    "    if decoded:\n",
    "        raw_bytes = decoded[0].data\n",
    "        try:\n",
    "            # Try to decode as UTF-8\n",
    "            decoded_str = raw_bytes.decode('utf-8')\n",
    "            return decoded_str\n",
    "        except UnicodeDecodeError:\n",
    "            # If decoding fails, return None\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d695c8c",
   "metadata": {},
   "source": [
    "# Example of Grid Fitting Followed by Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b12085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Estimate initial grid parameters ===\n",
    "init_params, init_pts = estimate_grid_params(nms_boxes,  reflectance)\n",
    "grid_pts = generate_grid(init_params)\n",
    "show_grid(img, grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2189851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optimize grid parameters & map ===\n",
    "opt_params, observed_pts, init_pts, opt_center, opt_spacing, opt_theta, opt_all = estimate_grid(init_params, init_pts, debug=True)\n",
    "grid_pts = generate_grid(opt_params)\n",
    "mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "show_grids(img, init_pts, opt_center, opt_spacing, opt_theta, opt_all, mapped_grid_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Convert to DMC matrix ===\n",
    "dmc_pts = inverse_grid_transform(mapped_grid_pts, opt_params)\n",
    "dmc_matrix = to_dmc_matrix(dmc_pts)\n",
    "display_DMC(dmc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decoding DMC ===\n",
    "decoded_data = decode_DMC(dmc_matrix)\n",
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff2487",
   "metadata": {},
   "source": [
    "# Full Decoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7262d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pipeline(image_path, yolo_model, unet_model, yolo_pad, tm_method=cv2.TM_CCOEFF_NORMED, match_thresh=0.9, nms_thresh=0.3, k_tm_templates=3, debug=False, rotation=None):\n",
    "    \"\"\"\n",
    "    Performs the entire decoding pipeline on the input image and template.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        yolo_model: YOLO model for object detection\n",
    "        unet_model: UNet model for template extraction\n",
    "        yolo_pad: Padding for YOLO detection\n",
    "        tm_method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        tmm_thresh_t: Threshold for initial template matching (default is 0.7)\n",
    "        tmm_thresh_ct: Threshold for cascade template matching (default is 0.8)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.3)\n",
    "        k_tm_templates: Number of templates to use for initial template matching (default is 3)\n",
    "        k_ctm_templates: Number of closest templates to use for cascade template matching (default is 3)\n",
    "        N_cascades: Number of cascades for template matching (default is 0)\n",
    "        N: Number of closest points to consider for cost function (default is 1)\n",
    "        alpha: Weighting factor for the cost function (default is 4.0)\n",
    "        debug: Flag for debugging (default is False)\n",
    "        rotation: Rotation angle in degrees (default is None, no rotation)\n",
    "    \n",
    "    Returns:\n",
    "        Decoded data from the DMC matrix or None if decoding fails.\n",
    "    \"\"\"\n",
    "    # === Load image ===\n",
    "    img = load_image_to_device(image_path)\n",
    "\n",
    "    # === Rotation ===\n",
    "    if rotation is not None:\n",
    "        _, _, h, w = img.shape\n",
    "        center = (w / 2, h / 2)\n",
    "\n",
    "        # rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(center, rotation, 1)\n",
    "\n",
    "        # getting size of new box to avoid cutting off corners\n",
    "        cos = np.abs(M[0, 0])\n",
    "        sin = np.abs(M[0, 1])\n",
    "        new_w = int(h * sin + w * cos)\n",
    "        new_h = int(h * cos + w * sin)\n",
    "\n",
    "        # adjusting the rotation matrix to take into account translation\n",
    "        M[0, 2] += new_w / 2 - center[0]\n",
    "        M[1, 2] += new_h / 2 - center[1]\n",
    "\n",
    "        # temporarily convert tensor to cv2 image\n",
    "        img = img.squeeze(0).permute(1, 2, 0).cpu().numpy() # (H, W, 3)\n",
    "\n",
    "        # rotating with new bounds\n",
    "        img = cv2.warpAffine(img, M, (new_w, new_h))\n",
    "\n",
    "        # converting back to tensor\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # === YOLO crop ===\n",
    "    img_yolo = yolo_detect_and_crop(img, yolo_model, yolo_pad, debug=debug)\n",
    "\n",
    "    # === UNet template ===\n",
    "    reflectance, illumination, unet_templates, template_bounds, heatmap_np = unet_get_template(img_yolo, unet_model, k_tm_templates, debug=debug)\n",
    "    if reflectance is None or illumination is None or unet_templates is None or len(template_bounds) == 0:\n",
    "        print(\"UNet failed to get templates, returning None...\")\n",
    "        return None\n",
    "    \n",
    "    # === Attempt decode with lower and lower thresholds until success or failure ===\n",
    "    while True:\n",
    "        # === Template matching ===\n",
    "        nms_boxes = cascade_template_matching(reflectance, template_bounds, method=tm_method, match_thresh=match_thresh, nms_thresh=nms_thresh, debug=debug)\n",
    "        if len(nms_boxes) <= 1:\n",
    "            print(\"Template matching found no (or only 1) matches, lowering match threshold...\")\n",
    "            match_thresh -= 0.025\n",
    "            continue\n",
    "        if len(nms_boxes) >= 256:\n",
    "            print(f\"Too many matches found ({len(nms_boxes)}), returning None...\")\n",
    "            return None\n",
    "\n",
    "        if debug:\n",
    "            if len(unet_templates) > 1:\n",
    "                display_yucheng_methods(nms_boxes, reflectance, img_yolo.squeeze(0).permute(1, 2, 0).cpu().numpy(), illumination, unet_templates[0], unet_templates[1])\n",
    "            else:\n",
    "                display_yucheng_methods(nms_boxes, reflectance, img_yolo.squeeze(0).permute(1, 2, 0).cpu().numpy(), illumination, unet_templates[0], unet_templates[0])\n",
    "\n",
    "        # === Estimate initial grid parameters ===\n",
    "        init_params, init_pts = estimate_grid_params(nms_boxes, reflectance, debug)\n",
    "\n",
    "        # === Optimizing Grid Parameters ===\n",
    "        if debug:\n",
    "            opt_params, observed_pts, init_pts, opt_center, opt_spacing, opt_theta, opt_all = estimate_grid(init_params, init_pts, debug)\n",
    "        else:\n",
    "            opt_params, observed_pts = estimate_grid(init_params, init_pts, debug)\n",
    "        grid_pts = generate_grid(opt_params)\n",
    "\n",
    "        # === Mapping observed points to grid points ===\n",
    "        mapped_grid_pts = map_observed_to_grid(observed_pts, grid_pts)\n",
    "        if debug:\n",
    "            show_grids(reflectance, init_pts, opt_center, opt_spacing, opt_theta, opt_all, mapped_grid_pts)\n",
    "\n",
    "        # === Convert to DMC matrix ===\n",
    "        dmc_pts = inverse_grid_transform(mapped_grid_pts, opt_params)\n",
    "        dmc_matrix = to_dmc_matrix(dmc_pts, debug=debug)\n",
    "        if dmc_matrix is None:\n",
    "            print(\"DMC matrix conversion failed, trying with lower match threshold...\")\n",
    "            match_thresh -= 0.025\n",
    "            continue\n",
    "\n",
    "        # === Decoding DMC ===\n",
    "        decoded_data = decode_DMC(dmc_matrix)\n",
    "\n",
    "        if type(decoded_data) is not str:\n",
    "            print(\"Decoding failed, trying with lower match threshold...\")\n",
    "            match_thresh -= 0.025\n",
    "\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"Decoded data: {decoded_data}\")\n",
    "\n",
    "            return decoded_data\n",
    "\n",
    "img_to_test = '../data/MAN/raw/train/1D1165212740006.jpeg'\n",
    "\n",
    "# === Load models ===\n",
    "yolo_pad = 0.01 # % to pad the yolo crop by\n",
    "yolo_model = load_yolo('../yolo/runs/obb/train7/weights/best.pt')\n",
    "unet_model = load_unet('../models/dot_detection/checkpoints/unet_best.pth')\n",
    "\n",
    "# === Template matching params ===\n",
    "tm_method = cv2.TM_CCOEFF_NORMED # method for template matching\n",
    "match_thresh = 0.95 # threshold for template matching (higher = more strict matching)\n",
    "nms_thresh = 0.2 # threshold for non-maximum suppression (higher = more overlap)\n",
    "k_tm_templates = 3 # number of UNet templates to get (higher = more templates, but slower)\n",
    "\n",
    "decoded_data = decode_pipeline(img_to_test, yolo_model, unet_model, yolo_pad, tm_method, match_thresh, nms_thresh, k_tm_templates, debug=True, rotation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923861a",
   "metadata": {},
   "source": [
    "# Evaluating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(image_paths, yolo_model, unet_model, yolo_pad, tm_method=cv2.TM_CCOEFF_NORMED, match_thresh=0.95, nms_thresh=0.2, k_tm_templates=3, img_idx=None, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluates the decoding pipeline on a list of images.\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of paths to input images\n",
    "        yolo_model: YOLO model for object detection\n",
    "        unet_model: UNet model for template extraction\n",
    "        yolo_pad: Padding for YOLO detection\n",
    "        tm_method: Method for template matching (default is cv2.TM_CCOEFF_NORMED)\n",
    "        match_thresh: Threshold for template matching (default is 0.95)\n",
    "        nms_thresh: Threshold for non-maximum suppression (default is 0.2)\n",
    "        k_tm_templates: Number of templates to use for initial template matching (default is 3)\n",
    "        debug: Flag for debugging (default is False)\n",
    "    \"\"\"\n",
    "    my_method = 0\n",
    "    pylibdmtx_method = 0\n",
    "    yolo_pylibdmtx_method = 0\n",
    "\n",
    "    # if img_idx:\n",
    "    #     image_paths = image_paths[img_idx-1:img_idx]\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # my method\n",
    "        decoded_data = decode_pipeline(image_path, yolo_model, unet_model, yolo_pad, tm_method, match_thresh, nms_thresh, k_tm_templates, debug)\n",
    "        if type(decoded_data) == str:\n",
    "            my_method += 1\n",
    "        \n",
    "        # pylibdmtx method\n",
    "        img = cv2.imread(image_path)\n",
    "        decoded_data = decode(img)\n",
    "        if decoded_data:\n",
    "            pylibdmtx_method += 1\n",
    "        \n",
    "        # yolo + pylibdmtx method\n",
    "        img = load_image_to_device(image_path)\n",
    "        img = yolo_detect_and_crop(img, yolo_model, yolo_pad, debug=debug)\n",
    "        img = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        decoded_data = decode(img)\n",
    "        if decoded_data:\n",
    "            yolo_pylibdmtx_method += 1\n",
    "\n",
    "    print(f\"Results for {len(image_paths)} images:\")\n",
    "    print(f\"My method: {my_method} ({my_method / len(image_paths) * 100:.2f}%)\")\n",
    "    print(f\"Pylibdmtx method: {pylibdmtx_method} ({pylibdmtx_method / len(image_paths) * 100:.2f}%)\")\n",
    "    print(f\"YOLO + Pylibdmtx method: {yolo_pylibdmtx_method} ({yolo_pylibdmtx_method / len(image_paths) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54c244",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc28e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/dot_detection/MAN/train-data/'\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
    "img_idx = 66\n",
    "\n",
    "evaluate_pipeline(image_paths, yolo_model, unet_model, yolo_pad, tm_method, match_thresh, nms_thresh, k_tm_templates, img_idx, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f672c58",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/dot_detection/MAN/val-data/'\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
    "img_idx = 21\n",
    "\n",
    "evaluate_pipeline(image_paths, yolo_model, unet_model, yolo_pad, tm_method, match_thresh, nms_thresh, k_tm_templates, img_idx, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da56b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4323d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/dot_detection/MAN/test-data/'\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
    "img_idx = 36\n",
    "\n",
    "evaluate_pipeline(image_paths, yolo_model, unet_model, yolo_pad, tm_method, match_thresh, nms_thresh, k_tm_templates, img_idx, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
